{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 2 - Probability distributions using PyTorch__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Recipe 2-1 : Sampling tensors](#Recipe-2-1-:-Sampling-tensors)\n",
    "1. [Recipe 2-2 : Variable tensors](#Recipe-2-2-:-Variable-tensors)\n",
    "1. [Recipe 2-3 : Basic statistics](#Recipe-2-3-:-Basic-statistics)\n",
    "1. [Recipe 2-4 : Gradient computation](#Recipe-2-4-:-Gradient-computation)\n",
    "1. [Recipe 2-5 : Tensor operations](#Recipe-2-5-:-Tensor-operations)\n",
    "1. [Recipe 2-6 : Tensor operations 2](#Recipe-2-6-:-Tensor-operations-2)\n",
    "1. [Recipe 2-7 : Distributions](#Recipe-2-7-:-Distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "import torch\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# magic functions\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe 2-1 : Sampling tensors\n",
    "\n",
    "__Problem__: There are many ways to initialize a tensor of weights in a neural network.\n",
    "\n",
    "__Solution__: Explore the various methods for initializing weights using several different distributions, including uniform, Bernoulli, multinomial and normal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Recipe-2-1-:-Sampling-tensors'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7893, 0.3216, 0.5247, 0.6688],\n",
       "        [0.8436, 0.4265, 0.9561, 0.0770],\n",
       "        [0.4108, 0.0014, 0.5414, 0.6419],\n",
       "        [0.2976, 0.7077, 0.4189, 0.0655]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tensor of values drawn from a uniform distribution between 0 and 1\n",
    "torch.Tensor(4, 4).uniform_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 0., 0., 1.],\n",
       "        [1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tensor of values drawn from a Bernoulli distribution\n",
    "torch.bernoulli(torch.Tensor(4, 4).uniform_(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tensor of values drawn from a multinomial distribution\n",
    "# returns index values. by default samples without replacement\n",
    "x = torch.Tensor([10, 10, 13, 10, 34, 45, 65, 67, 87, 89, 87, 34])\n",
    "torch.multinomial(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  9, 11,  5, 10])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tensor of values drawn from a multinomial distribution\n",
    "# returns index values. by default samples without replacement\n",
    "x = torch.Tensor([10, 10, 13, 10, 34, 45, 65, 67, 87, 89, 87, 34])\n",
    "torch.multinomial(x, 5, replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8246, 1.4849, 2.6099, 4.1361, 4.3711, 5.9630, 6.8620, 7.5366, 9.0137,\n",
       "        9.8490])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize tensor of values drawn from normal distribution\n",
    "# draw ten values from a normal distribution with increasing means\n",
    "# and decreasing standard deviations\n",
    "torch.normal(mean=torch.arange(1.0, 11.0), std=torch.arange(1, 0, -0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3209,  2.5122, -2.6993, -1.3289,  0.9503])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize tensor of values drawn from normal distribution\n",
    "# draw five values from a normal distribution with a static mean\n",
    "# and increasing standard deviations\n",
    "torch.normal(mean=0.5, std=torch.arange(1.0, 6.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3846])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize tensor of values drawn from normal distribution\n",
    "# draw one values from a normal distribution with a static mean\n",
    "# and a static standard deviation\n",
    "torch.normal(mean=0.5, std=torch.arange(0.1, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe 2-2 : Variable tensors\n",
    "\n",
    "__Problem__: What is a variable in PyTorch and how is it defined? What is a random variable in PyTorch?\n",
    "\n",
    "__Solution__: PyTorch represents algorithms as a computational graph. A variable is a representation of the tensor object, corresponding gradients, and a reference to the function from which it was created. A PyTorch variable is a node in a computational graph that stores data and gradients. For example, in simple linear regression, the graph includes one-dimensional tensors X, Y, W and alpha. X is multiplied by Y to create B, and B is added to alpha to create Y.\n",
    "\n",
    "In the following example, there are three variable objects representing the tensors x1, x2, and x3 with random points generated from a = 12 and b = 23. This is a simple graph that only involves multiplication and addition. PyTorch determines the partial derivative of the loss function with respect to the weights and biases in a neural network model using the Autograd module. Variables are specially designed to hold the changed values while running a backpropagation in a neural network model when the parameters of the model changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Recipe-2-1-:-Variable-tensors'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3317.7900, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch operations with variable\n",
    "from torch.autograd import Variable\n",
    "\n",
    "Variable(torch.ones(2, 2), requires_grad=True)\n",
    "\n",
    "a, b = 12, 23\n",
    "\n",
    "x1 = Variable(torch.randn(a, b), requires_grad=True)\n",
    "x2 = Variable(torch.randn(a, b), requires_grad=True)\n",
    "x3 = Variable(torch.randn(a, b), requires_grad=True)\n",
    "\n",
    "c = x1 * x2\n",
    "d = a + x3\n",
    "e = torch.sum(d)\n",
    "\n",
    "e.backward()\n",
    "\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe 2-3 : Basic statistics\n",
    "\n",
    "__Problem__: How do we calculate basic statistics such as mean, median, mode and so on?\n",
    "\n",
    "__Solution__: PyTorch has several built in operations that handle these calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Recipe-2-3-:-Basic-statistics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(45.9167)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the mean of a tensor\n",
    "torch.mean(\n",
    "    torch.tensor(\n",
    "        [10.0, 10.0, 13.0, 10.0, 34.0, 45.0, 65.0, 67.0, 87.0, 89.0, 87.0, 34.0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2273, -0.7638, -0.5213, -0.6272, -0.0245])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the mean of a tensor across columns\n",
    "d = torch.randn(4, 5)\n",
    "torch.mean(d, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5879, -0.3620,  0.3714, -0.3910])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the mean of a tensor across rows\n",
    "d = torch.randn(4, 5)\n",
    "torch.mean(d, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Median \n",
      "\n",
      "tensor(-0.5423)\n",
      "(tensor([ 0.5568, -0.7525, -0.1093, -1.6368, -0.9087]), tensor([1, 3, 3, 0, 0]))\n",
      "(tensor([-0.9087, -0.5524,  0.4179, -0.5423]), tensor([4, 1, 2, 4]))\n",
      "\n",
      "Mode \n",
      "\n",
      "(tensor([-1.6368, -1.3341,  0.0235, -2.2679]), tensor([3, 4, 0, 3]))\n",
      "(tensor([ 0.0235, -0.8017, -1.0633, -2.2679, -1.3341]), tensor([2, 0, 0, 3, 1]))\n",
      "(tensor([-1.6368, -1.3341,  0.0235, -2.2679]), tensor([3, 4, 0, 3]))\n",
      "\n",
      "Standard deviation \n",
      "\n",
      "tensor(1.0127)\n",
      "tensor([0.7901, 0.4708, 0.6921, 1.2584, 0.8070])\n",
      "tensor([1.1953, 0.8142, 0.2539, 1.4317])\n",
      "\n",
      "Variance \n",
      "\n",
      "tensor(1.0255)\n",
      "tensor([0.6242, 0.2217, 0.4790, 1.5835, 0.6513])\n",
      "tensor([1.4287, 0.6629, 0.0645, 2.0497])\n"
     ]
    }
   ],
   "source": [
    "# median, mode, variance and standard deviation are calculated the same way\n",
    "print(\"\\nMedian \\n\")\n",
    "print(torch.median(d))\n",
    "print(torch.median(d, dim=0))\n",
    "print(torch.median(d, dim=1))\n",
    "\n",
    "print(\"\\nMode \\n\")\n",
    "print(torch.mode(d))\n",
    "print(torch.mode(d, dim=0))\n",
    "print(torch.mode(d, dim=1))\n",
    "\n",
    "print(\"\\nStandard deviation \\n\")\n",
    "print(torch.std(d))\n",
    "print(torch.std(d, dim=0))\n",
    "print(torch.std(d, dim=1))\n",
    "\n",
    "print(\"\\nVariance \\n\")\n",
    "print(torch.var(d))\n",
    "print(torch.var(d, dim=0))\n",
    "print(torch.var(d, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe 2-4 : Gradient computation\n",
    "\n",
    "__Problem__: How do we compute basic gradients from the sample tensors using PyTorch.\n",
    "\n",
    "__Solution__: We will use a sample dataset datase0074. There are two values, x and y.With the initial weights given, we can get the gradients after each iteration. We will use two list, x_data and y_data. Computing the gradient of the two data lists requires computation of a loss function, a forward pass and  running everything in a loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Recipe-2-4-:-Gradient-computation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training): 4 4.0\n"
     ]
    }
   ],
   "source": [
    "# using forward pass\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "x_data = [11.0, 22.0, 33.0]\n",
    "y_data = [21.0, 14.0, 64.0]\n",
    "\n",
    "w = Variable(torch.Tensor([1.0]), requires_grad=True)\n",
    "\n",
    "# before training\n",
    "print(\"predict (before training): {} {}\".format(4, forward(4).data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad: 11.0 21.0 -220.0\n",
      "\tgrad: 22.0 14.0 2481.60009765625\n",
      "\tgrad: 33.0 64.0 -51303.6484375\n",
      "progress: 0 604238.8125\n",
      "\tgrad: 11.0 21.0 118461.7578125\n",
      "\tgrad: 22.0 14.0 -671630.6875\n",
      "\tgrad: 33.0 64.0 13114108.0\n",
      "progress: 1 39481139200.0\n",
      "\tgrad: 11.0 21.0 -30279010.0\n",
      "\tgrad: 22.0 14.0 171986000.0\n",
      "\tgrad: 33.0 64.0 -3358889472.0\n",
      "progress: 2 2590022582665216.0\n",
      "\tgrad: 11.0 21.0 7755301376.0\n",
      "\tgrad: 22.0 14.0 -44050112512.0\n",
      "\tgrad: 33.0 64.0 860298674176.0\n",
      "progress: 3 1.6990675778403933e+20\n",
      "\tgrad: 11.0 21.0 -1986333900800.0\n",
      "\tgrad: 22.0 14.0 11282376818688.0\n",
      "\tgrad: 33.0 64.0 -220344807849984.0\n",
      "progress: 4 1.114596779703609e+25\n",
      "\tgrad: 11.0 21.0 508751660449792.0\n",
      "\tgrad: 22.0 14.0 -2889709562888192.0\n",
      "\tgrad: 33.0 64.0 5.643602918322995e+16\n",
      "progress: 5 7.311812297356367e+29\n",
      "\tgrad: 11.0 21.0 -1.3030450598720307e+17\n",
      "\tgrad: 22.0 14.0 7.40129586448171e+17\n",
      "\tgrad: 33.0 64.0 -1.445473181358044e+19\n",
      "progress: 6 4.79658536356913e+34\n",
      "\tgrad: 11.0 21.0 3.3374367042303427e+19\n",
      "\tgrad: 22.0 14.0 -1.8956639512458114e+20\n",
      "\tgrad: 33.0 64.0 3.7022316429509803e+21\n",
      "progress: 7 inf\n",
      "\tgrad: 11.0 21.0 -8.548040947181921e+21\n",
      "\tgrad: 22.0 14.0 4.855287073351746e+22\n",
      "\tgrad: 33.0 64.0 -9.482375852864704e+23\n",
      "progress: 8 inf\n",
      "\tgrad: 11.0 21.0 2.1893751670486864e+24\n",
      "\tgrad: 22.0 14.0 -1.243565115636241e+25\n",
      "\tgrad: 33.0 64.0 2.4286824605446962e+26\n",
      "progress: 9 inf\n"
     ]
    }
   ],
   "source": [
    "# run training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad: {} {} {}\".format(x_val, y_val, w.grad.data[0]))\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "\n",
    "        # manually set the gradients to zero after updating weights\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print(\"progress: {} {}\".format(epoch, l.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (after training) 4 -9.268691075357862e+24\n"
     ]
    }
   ],
   "source": [
    "# after training\n",
    "print(\"predict (after training) {} {}\".format(4, forward(4).data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of 1 w.r.t Loss: -455.0\n",
      "gradient of 2 w.r.t Loss: -365.0\n",
      "gradient of 3 w.r.t Loss: -60.0\n",
      "gradient of 4 w.r.t Loss: -265.0\n"
     ]
    }
   ],
   "source": [
    "# compute the gradients from a loss function using the variable method on the tensor\n",
    "\n",
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "a = Variable(FloatTensor([5]))\n",
    "\n",
    "weights = [Variable(FloatTensor([i]), requires_grad=True) for i in (12, 53, 91, 73)]\n",
    "\n",
    "w1, w2, w3, w4 = weights\n",
    "\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "Loss = 10 - d\n",
    "Loss.backward()\n",
    "\n",
    "for index, weight in enumerate(weights, start=1):\n",
    "    gradient, *_ = weight.grad.data\n",
    "    print(\"gradient of {} w.r.t Loss: {}\".format(index, gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe 2-5 : Tensor operations\n",
    "__Problem__: how do we compute or perform operations based on variables such as matrix multiplication?\n",
    "\n",
    "__Solution__: Tensors are wrapped with the Variable, which has three properties: grad, volatile and gradient. So we can create a variable and extract the properties of the variable, and by using the mm module, we can perform matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Recipe-2-5-:-Tensor-operations'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "tensor([[ 2.8845, -2.5487, -3.4192,  0.1215],\n",
      "        [ 0.5551, -1.8283, -6.6794, -1.5815],\n",
      "        [ 5.1654, -3.0978, -0.3022,  2.0202],\n",
      "        [-1.3079,  4.3098, -1.1989,  0.6112]])\n"
     ]
    }
   ],
   "source": [
    "# create two variables and perform matri multiplication\n",
    "x = Variable(torch.Tensor(4, 4).uniform_(-4, 5))\n",
    "y = Variable(torch.Tensor(4, 4).uniform_(-3, 2))\n",
    "z = torch.mm(x, y)\n",
    "print(z.size())\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires gradient: False\n",
      "volatile: False\n",
      "gradient: None\n",
      "tensor([[ 2.8845, -2.5487, -3.4192,  0.1215],\n",
      "        [ 0.5551, -1.8283, -6.6794, -1.5815],\n",
      "        [ 5.1654, -3.0978, -0.3022,  2.0202],\n",
      "        [-1.3079,  4.3098, -1.1989,  0.6112]])\n"
     ]
    }
   ],
   "source": [
    "# print properties of variable\n",
    "print(\"requires gradient: {}\".format(z.requires_grad))\n",
    "print(\"volatile: {}\".format(z.volatile))\n",
    "print(\"gradient: {}\".format(z.grad))\n",
    "print(z.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe 2-6 : Tensor operations 2\n",
    "__Problem__: How do we perform operations based on variables such as matrix-vector operations, matrix-matri operations and vector-vector operations?\n",
    "\n",
    "__Solution__: Follow the rules of vector/matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Recipe-2-6-:-Tensor-operations-2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7786, 0.4222, 0.8917, 0.9283],\n",
       "        [0.3969, 0.6694, 0.4934, 0.5396],\n",
       "        [0.9355, 0.6115, 0.9139, 0.6781],\n",
       "        [0.4118, 0.9431, 0.1685, 0.1206]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define tensors\n",
    "mat1 = torch.FloatTensor(4, 4).uniform_(0, 1)\n",
    "mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7083, 0.8576, 0.1535, 0.5538],\n",
       "        [0.2005, 0.7325, 0.1566, 0.0832],\n",
       "        [0.4326, 0.9626, 0.3203, 0.0698],\n",
       "        [0.3634, 0.6819, 0.7408, 0.9494],\n",
       "        [0.6214, 0.7851, 0.4899, 0.3554]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define tensors\n",
    "mat2 = torch.FloatTensor(5, 4).uniform_(0, 1)\n",
    "mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4807, 0.9386, 0.6107, 0.6612])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define tensors\n",
    "vec1 = torch.FloatTensor(4).uniform_(0, 1)\n",
    "vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11.2786, 10.9222, 11.3917, 11.4283],\n",
       "        [10.8969, 11.1694, 10.9934, 11.0396],\n",
       "        [11.4355, 11.1115, 11.4139, 11.1781],\n",
       "        [10.9118, 11.4431, 10.6685, 10.6206]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scalar addition\n",
    "mat1 + 10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.0835e-01,  6.5757e-01, -4.6478e-02,  3.5376e-01],\n",
       "        [ 5.3887e-04,  5.3246e-01, -4.3382e-02, -1.1684e-01],\n",
       "        [ 2.3256e-01,  7.6260e-01,  1.2025e-01, -1.3018e-01],\n",
       "        [ 1.6341e-01,  4.8187e-01,  5.4082e-01,  7.4939e-01],\n",
       "        [ 4.2141e-01,  5.8513e-01,  2.8990e-01,  1.5540e-01]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scalar subtraction\n",
    "mat2 - 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2593, 1.3609, 1.5024, 1.5895],\n",
       "        [0.8777, 1.6080, 1.1041, 1.2008],\n",
       "        [1.4162, 1.5501, 1.5246, 1.3394],\n",
       "        [0.8926, 1.8817, 0.7792, 0.7818]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector and matrix addition\n",
    "mat1 + vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1891, 1.7962, 0.7642, 1.2150],\n",
       "        [0.6812, 1.6711, 0.7673, 0.7444],\n",
       "        [0.9133, 1.9012, 0.9309, 0.7310],\n",
       "        [0.8441, 1.6205, 1.3515, 1.6106],\n",
       "        [1.1021, 1.7238, 1.1006, 1.0166]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector and matrix addition\n",
    "mat2 + vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6062, 0.1783, 0.7951, 0.8617],\n",
       "        [0.1576, 0.4481, 0.2434, 0.2911],\n",
       "        [0.8751, 0.3739, 0.8352, 0.4599],\n",
       "        [0.1696, 0.8894, 0.0284, 0.0145]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "mat1 * mat1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe 2-7 : Distributions\n",
    "__Problem__: Understanding statistical distributions is essential for weight normalization / initialization and computation of gradients in neural network-based operations. How do we know which distrbution to use?\n",
    "\n",
    "__Solution__: Each distribution follows an established mathematical formula. We can utilize PyTorch's implementation of these distributions to explore how to apply them to different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Recipe-2-7-:-Distributions'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1.])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bernoulli distribution - binary sample where 1 with probability p and 0 with 1 - p\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "# creates a Bernoulli distribution paramaterized by probabilities\n",
    "# returns values 1 with probability p and 0 with probability 1 - p.\n",
    "dist = Bernoulli(torch.tensor([0.3, 0.6, 0.9]))\n",
    "dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5445])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beta distribution - a distribution of number random variables defined in the range of 0 to 1.\n",
    "# typically used for Bayesian inference analysis\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "dist = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n",
    "dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.,  19.,  81., 100.])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binomial distribution - applicable when the outcome is twofold and the experiment is repetitive.\n",
    "# the binomial distribution is used to model the number of successful events over many trials. it is a\n",
    "# discrete probability distribution, where the probability of success is defined as 1 and\n",
    "# the probability of failure is 0.\n",
    "from torch.distributions.binomial import Binomial\n",
    "\n",
    "dist = Binomial(100, torch.tensor([0, 0.2, 0.8, 1]))\n",
    "\n",
    "# 100 trials, returns count of successes given a tensor of probabilities\n",
    "dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# object distribution - can be thought of as a generalized Bernoulli distribution, which can be\n",
    "# expanded to > 2 outcomes\n",
    "from torch.distributions.object import object\n",
    "\n",
    "# pass in tensor of probabilities for n outcomes, adds up to 1.0\n",
    "dist = object(torch.tensor([0.20, 0.20, 0.20, 0.20, 0.20]))\n",
    "dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.6696])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# laplacian distribution - a number distribution function that is also known as the double exponential\n",
    "# distribution. often used in speech regonition systems to understand prior probabilities. also useful in\n",
    "# bayesian regression for deciding prior probabilities\n",
    "from torch.distributions.laplace import Laplace\n",
    "\n",
    "dist = Laplace(torch.tensor([10.0]), torch.tensor([0.990]))\n",
    "dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([94.8375])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal distribution - a distribution that is defined by the a mean and a standard deviation. If we know\n",
    "# both of those elements, we can estimate event probabilities.\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "dist = Normal(torch.tensor([100.0]), torch.tensor([10.0]))\n",
    "dist.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
