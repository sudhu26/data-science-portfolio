{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 2 - Training simple machine learning algorithms for classification__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Artificial neuron](#artificial-neuron)\n",
    "    1. [Intuition: perceptron](#intuition-perceptron)\n",
    "        1. [Decision function](#decision-function)\n",
    "        1. [Learning rule](#learning-rule)\n",
    "        1. [Homegrown implementation](#code-example-perceptron)\n",
    "1. [Adaptive linear neurons (Adaline)](#Adaptive-linear-neurons)\n",
    "    1. [Intuition: adaline](#intuition-adaline)\n",
    "        1. [The convergence of learning](#The-convergence-of-learning)\n",
    "        1. [Minimizing cost functions with gradient descent](#Minimizing-cost-functions-with-gradient-descent)\n",
    "        1. [Homegrown implementation](#code-example-adaline)\n",
    "    1. [Improve gradient descent through feature scaling](#Improve-gradient-descent-through-feature-scaling)\n",
    "        1. [Homegrown implementation](#code-example-standardization)\n",
    "    1. [Large-scale machine learning and stochastic gradient descent](#Large-scale-machine-learning-and-stochastic-gradient-descent)\n",
    "        1. [Homegrown implementation](#code-example-stochastic)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:03.021568Z",
     "start_time": "2019-09-07T03:25:01.607468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlmachine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9804d3355ef2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/home/prettierplot\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"/home/prettierplot\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmlmachine\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmlm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mprettierplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPrettierPlot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprettierplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlmachine'"
     ]
    }
   ],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.datasets import load_boston, load_wine, load_iris, load_breast_cancer, make_blobs, make_moons\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, IsolationForest\n",
    "from sklearn.feature_extraction.text import CounterVectorizer, TfidfTransformer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_selection import f_classif, f_regression, VarianceThreshold, SelectFromModel, SelectKBest\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, LinearRegression, LogisticRegression, SGDRegressor\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, explained_variance_score, mean_squared_log_error, mean_absolute_error, median_absolute_error, mean_squared_error, r2_score, confusion_matrix, roc_curve, accuracy_score, roc_auc_score, homogeneity_score, completeness_score, classification_report, silhouette_samples\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures, OrdinalEncoder, LabelEncoder, OneHotEncoder, KBinsDiscretizer, QuantileTransformer, PowerTransformer, MinMaxScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom extensions and settings\n",
    "sys.path.append(\"/home/mlmachine\") if \"/home/mlmachine\" not in sys.path else None\n",
    "sys.path.append(\"/home/prettierplot\") if \"/home/prettierplot\" not in sys.path else None\n",
    "\n",
    "import mlmachine as mlm\n",
    "from prettierplot.plotter import PrettierPlot\n",
    "import prettierplot.style as style\n",
    "\n",
    "# magic functions\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'artificial-neuron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition: perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'intuition-perceptron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Decision function \n",
    "\n",
    "A basic perceptron is a binary classifier that uses the decision function...\n",
    "\n",
    "$$\n",
    "\\phi(z) =\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1  & \\mbox{if } z >= \\theta \\\\\n",
    "        -1  & \\mbox{otherwise}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "...to designate an observation $x^i$ as belonging to the positive class '1' or the negative class '-1'. The input $z$ is the dot product of a weight vector  $\\mathbf{w}$ and an input vector $\\mathbf{x}$. Each value in vector $\\mathbf{x}$ represents a specific feature value of the observation $x^i$ and vector $\\mathbf{w}$ represents the weights associated with each feature.The equation for $z$ is...\n",
    "\n",
    "$$\n",
    "z = w_1x_1 + ... + w_nx_n = \\mathbf{w}^T\\mathbf{x}\n",
    "$$\n",
    "\n",
    "...where $n$ is the number of features/weights associated with $x^i$. If the value of $z$ for a particular observation $x^i$ is greater than or equal to a pre-defined theshold $\\theta$, class '1' is predicted, otherwise class '-1' is predicted.\n",
    "\n",
    "A small adjustment to the variabls considered simplifies our notation. If we move subtract the threshold $\\theta$ to move it to the left side side of the decision function...\n",
    "\n",
    "$$\n",
    "\\phi(z) =\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1  & \\mbox{if } z >= 0 \\\\\n",
    "        -1  & \\mbox{otherwise}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "...and add weight sub-zero $w_0 = -\\theta$ and $x_0 = 1$ to our dot product function $z$...\n",
    "\n",
    "\n",
    "$$\n",
    "z = w_0x_0 + w_1x_1 + ... + w_nx_n = \\mathbf{w}^T\\mathbf{x}\n",
    "$$\n",
    "\n",
    "...we can now use 0 as our threshold in the decision function. Our negative threshold, $w_0 = -\\theta$, is referred to as the bias unit.\n",
    "\n",
    "In machine learning parlance, this decision function is a step function that will 'squish' our input $x$ such that the output why is either 1 or -1. To associated this with neurons, and 1 or -1 indicates whether a neuron 'fires' or not based on its input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'decision-function'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rule\n",
    "\n",
    "The perceptron 'learns' by updating its weights after each epoch, or iteration. The learning rule updates each weight $w_j$ by...\n",
    "\n",
    "$$\n",
    "w_j := w_j + \\Delta w_j\n",
    "$$\n",
    "\n",
    "...where $\\Delta w_j$ is determined by...\n",
    "\n",
    "$$\n",
    "\\Delta w_j = \\eta(y^i - \\hat{y}^i)x_j^i\n",
    "$$\n",
    "\n",
    "In this equation, \n",
    "- $w_j$ is the initial weight for the $j$th of $n$ features\n",
    "- $\\eta$ is the learning rate (typically a value between 0.0 and 1.0)\n",
    "- $y^i$ is the true class label for observation $x^i$\n",
    "- $\\hat{y}^i$ is the class label estimated by the decision function $\\phi(z)$\n",
    "- $x_j^i$ is the value of the $j$th feature for the $i$th observation and acts as a multipicative factor\n",
    "\n",
    "If $\\phi(z)$ returns the correct class label, the weight remains unchanged because $\\Delta w_j$ evaluates to zero. If the true class is positive, but the prediction is negative, the weight is pushed in the direction of the positive class by making $x_j^i \\times w_j$ more positive by...\n",
    "\n",
    "$$\n",
    "\\Delta w_j = \\eta(1 - (-1))x_j^i = \\eta(2)x_j^i\n",
    "$$\n",
    "\n",
    "...and if the true class is negative, but the prediction is positive, the weight is pushed in the direction of the positive class by making $x_j^i \\times w_j$ more negative by...\n",
    "\n",
    "$$\n",
    "\\Delta w_j = \\eta(-1 - 1)x_j^i = \\eta(-2)x_j^i\n",
    "$$\n",
    "\n",
    "The weight update is proportional to the value of $x_j^i$\n",
    "\n",
    "One important thing to note is that the perceptron converges only when the two classes are linearly separable. Either a maximum number of epochs or a tolerable number of misclassifcations are required to prevent the perceptron from running indefinitely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'learning-rule'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homegrown implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'code-example-perceptron'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:03.034182Z",
     "start_time": "2019-09-07T03:25:03.023339Z"
    },
    "code_folding": [
     3,
     27,
     68,
     81
    ]
   },
   "outputs": [],
   "source": [
    "# perceptron algorithm\n",
    "class Perceptron(object):\n",
    "    \"\"\"\n",
    "    Info:\n",
    "        Description: \n",
    "            Perceptron classifier written from scratch.\n",
    "        Parameters:\n",
    "            eta : float\n",
    "                Learning rate\n",
    "            n_iter : int\n",
    "                Number of epochs to run learning step\n",
    "            random_state : int\n",
    "                Random state seed\n",
    "        Attributes:\n",
    "            w_: 1-d array\n",
    "                Weights after fitting\n",
    "            errors_: list\n",
    "                number of misclassifications in each epoch (iter)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Determines our model by finding the ideal weights. \n",
    "                Takes in an m row by n column matrix X and an m by 1 value vector y, \n",
    "                which includes the actual class labels. Initial weights are set to random \n",
    "                numbers drawn from a normal distribution with a standard deviation of 0.01. \n",
    "                The number of values in the weight vector is equal the number of columns \n",
    "                in X plus 1 (for the bias unit).\n",
    "\n",
    "                The fit method runs for a specified number of iterations. \n",
    "                Within each iteration, each sample in the training data X, along with its \n",
    "                corresponding label in vector y is evaluated. The weights are adjusted \n",
    "                after each training sample is evaluated.\n",
    "\n",
    "                The errors are also collected after each epoch so that we can \n",
    "                observe how the model performs during training.\n",
    "            Parameters:\n",
    "                X: Numpy matrix\n",
    "                    m x n matrix containing training data.\n",
    "                y: 1-d array\n",
    "                    Vector of target values. Contain m values.\n",
    "         \"\"\"\n",
    "\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors_ = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors_ += int(update != 0.0)\n",
    "            self.errors_.append(errors_)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Returns the dot product of a sample x^i from the training \n",
    "                data X and a weight vector containing weights w_1 ... w_n. \n",
    "                The weight w_0 is added on to the dot product.\n",
    "            Parameters:\n",
    "                X : Numpy matrix\n",
    "                    m x n matrix containing training data.\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Returns the predicted classes label. 1 if net_input() \n",
    "                returns a number >= 0, else returns -1. Used within \n",
    "                the fit method but can also be used to predict new, unseen data.\n",
    "            Parameters:\n",
    "                X : Numpy matrix\n",
    "                    m x n matrix containing training data.\n",
    "        \"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:03.051671Z",
     "start_time": "2019-09-07T03:25:03.036711Z"
    }
   },
   "outputs": [],
   "source": [
    "# import iris dataset\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(\n",
    "    np.c_[iris[\"data\"], iris[\"target\"]], columns=iris[\"feature_names\"] + [\"target\"]\n",
    ")\n",
    "\n",
    "# trim iris data set down to two classes and two features\n",
    "df = df.iloc[0:100, [0, 2, 4]]\n",
    "df[\"target\"] = np.where(df[\"target\"] == 0.0, -1, 1)\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:03.452234Z",
     "start_time": "2019-09-07T03:25:03.053713Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize petal and sepal length, differentiated by iris type\n",
    "p = PrettierPlot()\n",
    "\n",
    "ax = p.make_canvas(\n",
    "    title=\"Class comparison, select features\",\n",
    "    x_label=\"sepal length (cm)\",\n",
    "    y_label=\"petal length (cm)\",\n",
    "    y_shift=0.45,\n",
    ")\n",
    "\n",
    "p.scatter_2d_hue(\n",
    "    df=df,\n",
    "    x=\"sepal length (cm)\",\n",
    "    y=\"petal length (cm)\",\n",
    "    target=\"target\",\n",
    "    x_units=\"ff\",\n",
    "    y_units=\"ff\",\n",
    "    label=iris.target_names[:2],\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:03.479105Z",
     "start_time": "2019-09-07T03:25:03.453825Z"
    }
   },
   "outputs": [],
   "source": [
    "# create training data matrix and label vector\n",
    "# fit model using perceptron\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "ppn = Perceptron(eta=0.1, n_iter=10)\n",
    "ppn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:03.836684Z",
     "start_time": "2019-09-07T03:25:03.480747Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize errors vs. epochs\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas(\n",
    "    title=\"Errors after each epoch\", x_label=\"Epochs\", y_label=\"Errors\", y_shift=0.8\n",
    ")\n",
    "p.line(\n",
    "    x=np.arange(1, len(ppn.errors_) + 1),\n",
    "    y=ppn.errors_,\n",
    "    marker_on=True,\n",
    "    label=\"error\",\n",
    "    x_ticks=np.arange(1, 11, 1),\n",
    "    y_units=\"ff\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The perceptron converaged after 6 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:04.243756Z",
     "start_time": "2019-09-07T03:25:03.839331Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot decision region\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas(\n",
    "    title=\"Decision region\",\n",
    "    x_label=\"petal length\",\n",
    "    y_label=\"petal width\",\n",
    "    y_shift=0.68,\n",
    "    position=111,\n",
    ")\n",
    "p.decision_region(\n",
    "    x=df.iloc[:, :2].values,\n",
    "    y=df.iloc[:, -1].values,\n",
    "    classifier=ppn,\n",
    "    bbox=(1.1, 0.9),\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive linear neurons (Adaline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Adaptive-linear-neurons'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition: adaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'intuition-adaline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The-convergence-of-learning\n",
    "\n",
    "The adaptive linear neuron, or Adaline, is another type of singe-layer neural network. The key difference compared to the perceptron is that Adaline used a linear activation function rather than a step function. The Adaline activation function is:\n",
    "\n",
    "$$\n",
    "\\phi(\\textbf{w}^T\\textbf{x}) = \\textbf{w}^T\\textbf{x}\n",
    "$$\n",
    "\n",
    "A threshold function is still used to make the final prediction, which is similar to the perceptrons step function. The primary difference is that Adaline compared the true class label with the linear activation function's number valued output to computer the model error and update the weights accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-convergence-of-learning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing cost functions with gradient descent\n",
    "\n",
    "We use an objective function in the process of optimizing the model. This function is also referred to as a cost function, and we want to minimize the cost. With Adaline, we can use the Sum of Squared Errors (SSE) as our cost function\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2}\\sum_i\\big(y^{(i)} - \\phi(z^{(i)})\\big)^2\n",
    "$$\n",
    "\n",
    "This calculates the difference between the true class label and the Adaline activation function, squares the result, and divides it by two. This is repeated for each sample and summed.\n",
    "\n",
    "A benefit of using the SSE cost function is that it is differentiable, unlike to step function. This enables the use of gradient descent to minimize our cost function. Over each iteration, gradient descent adjusts the weights such that it takes a step in the opposite direction of the gradient.\n",
    "\n",
    "The weight update can be represented by the function:\n",
    "\n",
    "$$\n",
    "\\textbf{w} := \\textbf{w} + \\Delta\\textbf{w}\n",
    "$$\n",
    "\n",
    "Where the weight change $\\Delta\\textbf{w}$ is defined as the negative gradient multiplied by a chosen learning rate $\\eta$\n",
    "\n",
    "$$\n",
    "\\Delta\\textbf{w} = -\\eta\\triangledown J(\\textbf{w})\n",
    "$$\n",
    "\n",
    "To compute the gradient, we need the partial derivative of the cost function with respect to each weight $w_j$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j} = -\\sum_i\\big(y^{(i)} - \\phi(z^{(i)})\\big)x_j^{(i)}\n",
    "$$\n",
    "\n",
    "The function for updating weight $w_j$ can be written as:\n",
    "\n",
    "$$\n",
    "\\Delta w_j = -\\eta\\frac{\\partial J}{\\partial w_j} = \\eta\\sum_i\\big(y^{(i)} - \\phi(z^{(i)})\\big)x_j^{(i)}\n",
    "$$\n",
    "\n",
    "All weights are updated simultaneously, so the learning rule, again, is:\n",
    "\n",
    "$$\n",
    "\\textbf{w} := \\textbf{w} + \\Delta\\textbf{w}\n",
    "$$\n",
    "\n",
    "The weight update is claculated based on all samples, rather than one sample at a time in the case of the perceptron. This is often referred to as batch gradient decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Minimizing-cost-functions-with-gradient-descent'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homegrown implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'code-example-adaline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:04.258827Z",
     "start_time": "2019-09-07T03:25:04.247640Z"
    },
    "code_folding": [
     3,
     26,
     52,
     64,
     77
    ]
   },
   "outputs": [],
   "source": [
    "# adaline algorithm, full-batch variation\n",
    "class AdalineGD:\n",
    "    \"\"\"\n",
    "    Info:\n",
    "        Description:\n",
    "            ADaptive linear neuron classifier\n",
    "        Parameters:\n",
    "            eta : float, default=0.01\n",
    "                Learning rate\n",
    "            n_iter : int, default=50\n",
    "                Number of iterations to complete\n",
    "            random_state : int, default=1\n",
    "                Random state seed\n",
    "        Attributes:\n",
    "            w_ : 1-d array\n",
    "                Weight post-fitting\n",
    "            cost_ : list\n",
    "                Sum-of-squares cost function value in each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Fit training data\n",
    "            Parameters:\n",
    "                X : Array\n",
    "                    Training data\n",
    "                y : Array\n",
    "                    Labels for training data\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "\n",
    "        self.cost_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = y - output\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            cost = (errors ** 2).sum() / 2.0\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Return dot product of training data observations and weights\n",
    "                plus the bias weight\n",
    "            Parameters:\n",
    "                X : Array\n",
    "                    Training data\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Compute linear activation. This is an identity function\n",
    "                intended to illustrate how info flows through a single layer \n",
    "                in a neural network.\n",
    "            Parameters:\n",
    "                X : Array\n",
    "                    Training data\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Return class label\n",
    "            Parameters:\n",
    "                X : Array\n",
    "                    Training data\n",
    "        \"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:04.866721Z",
     "start_time": "2019-09-07T03:25:04.261285Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize errors vs. epoch using different learning rates\n",
    "p = PrettierPlot(chart_scale=12)\n",
    "\n",
    "ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)\n",
    "ax = p.make_canvas(\n",
    "    title=r\"Adaline - $\\eta$ = 0.01\",\n",
    "    x_label=\"Epochs\",\n",
    "    y_label=\"log(SSE)\",\n",
    "    y_shift=0.77,\n",
    "    position=121,\n",
    ")\n",
    "p.line(\n",
    "    x=np.arange(1, len(ada1.cost_) + 1),\n",
    "    y=np.log(ada1.cost_),\n",
    "    marker_on=True,\n",
    "    label=\"error\",\n",
    "    x_ticks=np.arange(1, 11, 1),\n",
    "    bbox=(0.4, 0.9),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)\n",
    "ax = p.make_canvas(\n",
    "    title=r\"Adaline - $\\eta$ = 0.0001\",\n",
    "    x_label=\"Epochs\",\n",
    "    y_label=\"SSE\",\n",
    "    y_shift=0.85,\n",
    "    position=122,\n",
    ")\n",
    "p.line(\n",
    "    x=np.arange(1, len(ada2.cost_) + 1),\n",
    "    y=ada2.cost_,\n",
    "    marker_on=True,\n",
    "    label=\"error\",\n",
    "    x_ticks=np.arange(1, 11, 1),\n",
    "    bbox=(0.9, 0.9),\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - On the left, the learning rate is too high. This is evident by the fact that the SSE (log) rises after each epoch. The optimization process is overshooting the minimum. On the right, the Adaline algorithm is converging, but doing so very slowly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve gradient descent through feature scaling\n",
    "\n",
    "Many machine learning algorithms require feature scaling of some type in order to function optimally. One type of scaling is called standardization, which gives our data the property of a standard normal distribution. This allows gradient descent to converge more quickly. When a dataset is standardized, each feature has a mean $\\mu$ of 0 and a standard deviation $\\sigma$ of 1. For example, to standardize the $j$th feature:\n",
    "\n",
    "$$\n",
    "\\textbf{x'}_j = \\frac{\\textbf{x}_j - \\mu_j}{\\sigma_j}\n",
    "$$\n",
    "\n",
    "Vector $\\textbf{x}_j$ consists of all the $j$th feature values for all training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Improve-gradient-descent-through-feature-scaling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homegrown implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'code-example-standardization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:04.872320Z",
     "start_time": "2019-09-07T03:25:04.868022Z"
    }
   },
   "outputs": [],
   "source": [
    "# manually apply standard scaling\n",
    "X_std = np.copy(X)\n",
    "X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\n",
    "X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:05.562714Z",
     "start_time": "2019-09-07T03:25:04.874306Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize full-batch gradient descent boundary and learning rate\n",
    "ada = AdalineGD(n_iter=15, eta=0.01)\n",
    "ada.fit(X_std, y)\n",
    "\n",
    "p = PrettierPlot(chart_scale=18)\n",
    "\n",
    "ax = p.make_canvas(\n",
    "    title=\"Decision region\",\n",
    "    x_label=\"petal length \\n[standardized]\",\n",
    "    y_label=\"petal width \\n[standardized]\",\n",
    "    y_shift=0.52,\n",
    "    position=121,\n",
    ")\n",
    "p.decision_region(x=X_std, y=y, classifier=ada, ax=ax, bbox=(1.25, 0.9))\n",
    "\n",
    "ax = p.make_canvas(\n",
    "    title=r\"Adaline - $\\eta$ = 0.01\",\n",
    "    x_label=\"Epochs\",\n",
    "    y_label=\"log(SSE)\",\n",
    "    y_shift=0.7,\n",
    "    position=122,\n",
    ")\n",
    "p.line(\n",
    "    x=np.arange(1, len(ada.cost_) + 1),\n",
    "    y=ada.cost_,\n",
    "    marker_on=True,\n",
    "    label=\"error\",\n",
    "    x_ticks=np.arange(1, 16, 1),\n",
    "    ax=ax,\n",
    ")\n",
    "plt.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The right graph shows a clean separation between the classes, and the left graph show a rapid learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large scale machine learning and stochastic gradient descent\n",
    "\n",
    "Batch gradient descent can become very computationally expensive with large data sets because every epoch involves an evaluation of the entire dataset. \n",
    "\n",
    "A less computationally expensive alternative is stochastic gradient descent. Rather than updating the weights based on the sum of accumulated errors over all samples $\\textbf{x}^{(i)}$:\n",
    "\n",
    "$$\n",
    "\\Delta w = -\\eta\\sum_i\\big(y^{(i)} - \\phi(z^{(i)})\\big)x^{(i)}\n",
    "$$\n",
    "\n",
    "The weights are updated incrementally for each training sample:\n",
    "\n",
    "$$\n",
    "\\eta\\big(y^{(i)} - \\phi(z^{(i)})\\big)x^{(i)}\n",
    "$$\n",
    "\n",
    "The learning path is typically much more erratic than the path forged by full-batch gradient descent, but still reaches convergence and does so much faster. It is important to present training data in random order, and we need to shuffle the training set for every epoch.\n",
    "\n",
    "The learning rate $\\eta$ is typically replace with an adaptive learning rate that decreases over time:\n",
    "\n",
    "$$\n",
    "\\frac{c_1}{[\\mbox{# of iterations}] + c_2}\n",
    "$$\n",
    "\n",
    "$c_1$ and $c_2$ are constants.\n",
    "\n",
    "__Online learning__\n",
    "\n",
    "Stochastic gradient descent is also useful for online learning, where the model is trained on the fly as new training data arrives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Large-scale-machine-learning-and-stochastic-gradient-descent'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homegrown implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'code-example-stochastic'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:05.581450Z",
     "start_time": "2019-09-07T03:25:05.564239Z"
    },
    "code_folding": [
     3,
     32,
     54,
     73,
     87,
     100,
     118,
     129,
     140
    ]
   },
   "outputs": [],
   "source": [
    "# homegrown Adaline algorithm, stochastic gradient descent\n",
    "class AdalineSGD:\n",
    "    \"\"\"\n",
    "    Info:\n",
    "        Description:\n",
    "            Adaptive linear neuron classifier using\n",
    "            stochastic gradient descent\n",
    "        Parameters:\n",
    "            eta : float, default=0.01\n",
    "                Learning rate\n",
    "            n_iter : int, default=50\n",
    "                Number of iterations to complete\n",
    "            shuffle : bool, default=true\n",
    "                Shuffles the training data after each epoch\n",
    "                to prevent cycles.\n",
    "            random_state : int, default=1\n",
    "                Random state seed\n",
    "        Attributes:\n",
    "            w_ : 1-d array\n",
    "                Weight post-fitting\n",
    "            cost_ : list\n",
    "                Sum-of-squares cost function value in each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.w_initialized = False\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Fit training data\n",
    "            Parameters:\n",
    "                X : Array\n",
    "                    Training data\n",
    "                y : Array\n",
    "                    Labels for training data\n",
    "        \"\"\"\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        self.cost_ = []\n",
    "        for i in range(self.n_iter):\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            cost = []\n",
    "            for xi, target in zip(X, y):\n",
    "                cost.append(self._update_weights(xi, target))\n",
    "            avg_cost = sum(cost) / len(y)\n",
    "            self.cost_.append(avg_cost)\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Fit training data without reinitializing the weights\n",
    "            Parameters:\n",
    "                X : Array\n",
    "                    Training data\n",
    "                y : Array\n",
    "                    Labels for training data\n",
    "        \"\"\"\n",
    "        if not self.w_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "        if y.ravel().shape[0] > 1:\n",
    "            for xi, target in zip(X, y):\n",
    "                self._update_weights(xi, target)\n",
    "        else:\n",
    "            self._update_weights(X, y)\n",
    "\n",
    "    def _shuffle(self, X, y):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Shuffle training data\n",
    "            Parameters:\n",
    "                X : Array\n",
    "                    Training data\n",
    "                y : Array\n",
    "                    Labels for training data\n",
    "        \"\"\"\n",
    "        r = self.rgen.permutation(len(y))\n",
    "        return X[r], y[r]\n",
    "\n",
    "    def _initialize_weights(self, m):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Initialize weights to small random numbers\n",
    "            Parameters:\n",
    "                m : int\n",
    "                    Number of columns/features in dataset\n",
    "        \"\"\"\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)\n",
    "        self.w_initialized = True\n",
    "\n",
    "    def _update_weights(self, xi, target):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Apply adaline learning rule to update weights\n",
    "            Parameters:\n",
    "                xi : Array\n",
    "                    Training sample\n",
    "                target : int\n",
    "                    Label for training sample\n",
    "        \"\"\"\n",
    "        output = self.activation(self.net_input(xi))\n",
    "        error = target - output\n",
    "        self.w_[1:] += self.eta * xi.dot(error)\n",
    "        self.w_[0] += self.eta * error\n",
    "        cost = (1 / 2) * error ** 2\n",
    "        return cost\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Calculate net input\n",
    "            Parameters:\n",
    "                X : Array\n",
    "                    Training data\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Compute linear activation\n",
    "            Parameters:\n",
    "                X : Array\n",
    "                    Training data\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Info:\n",
    "            Description:\n",
    "                Return class label after unit step\n",
    "            Parameters:\n",
    "                X : Array\n",
    "                    Training samples\n",
    "        \"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T03:25:06.273094Z",
     "start_time": "2019-09-07T03:25:05.583343Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize stochastic gradient descent boundary and learning rate\n",
    "ada_SGD = AdalineSGD(n_iter=15, eta=0.01, random_state=1)\n",
    "ada_SGD.fit(X_std, y)\n",
    "\n",
    "p = PrettierPlot()\n",
    "\n",
    "ax = p.make_canvas(\n",
    "    title=\"Decision region\",\n",
    "    x_label=\"petal length \\n[standardized]\",\n",
    "    y_label=\"petal width \\n[standardized]\",\n",
    "    y_shift=0.52,\n",
    "    xShift=0.00,\n",
    "    position=121,\n",
    ")\n",
    "p.decision_region(x=X_std, y=y, classifier=ada_SGD, ax=ax, bbox=(1.25, 0.9))\n",
    "\n",
    "ax = p.make_canvas(\n",
    "    title=r\"Adaline - $\\eta$ = 0.01\",\n",
    "    x_label=\"Epochs\",\n",
    "    y_label=\"SSE\",\n",
    "    y_shift=0.85,\n",
    "    xShift=0.00,\n",
    "    position=122,\n",
    ")\n",
    "p.line(\n",
    "    x=np.arange(1, len(ada_SGD.cost_) + 1),\n",
    "    y=np.array(ada_SGD.cost_),\n",
    "    marker_on=True,\n",
    "    label=\"error\",\n",
    "    y_units=\"fff\",\n",
    "    x_ticks=np.arange(1, 16, 1),\n",
    "    ax=ax,\n",
    ")\n",
    "plt.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The stochastic gradient descent variation of adaline also determines the linear boundary. The error improves rapidly after the first epoch, then makes gradual improvement after each successive iteration."
   ]
  }
 ],
 "metadata": {
  "author": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "columns": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
