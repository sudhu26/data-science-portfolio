{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 15 - Classifying Images with Deep Convolutional Neural Networks__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Building blocks of convolutional neural networks](#Building-blocks-of-convolutional-neural-networks)\n",
    "    1. [Understanding CNNs and learning feature hierarchies](#Understanding-CNNs-and-learning-feature-hierarchies)\n",
    "    1. [Performing discrete convolutions in one dimension](#Performing-discrete-convolutions-in-one-dimension)\n",
    "    1. [The effect of zero-padding in a convolution](#The-effect-of-zero-padding-in-a-convolution)\n",
    "    1. [Determining the size of the convolution output](#Determining-the-size-of-the-convolution-output)\n",
    "1. [Performing a discrete convolution in 2D](#Performing-a-discrete-convolution-in-2D)\n",
    "1. [Subsampling](#Subsampling)\n",
    "1. [Putting everything together to build a CNN](#Putting-everything-together-to-build-a-CNN)\n",
    "    1. [Working with multiple input or color channels](#Working-with-multiple-input-or-color-channels)\n",
    "    1. [Regularizing a neural network with dropout](#Regularizing-a-neural-network-with-dropout)\n",
    "1. [Implementing a deep convolutional neural network using TensorFlow](#Implementing-a-deep-convolutional-neural-network-using-TensorFlow)\n",
    "    1. [The multilayer CNN architecture](#The-multilayer-CNN-architecture)\n",
    "    1. [Loading and prepreocessing the data](#Loading-and-prepreocessing-the-data)\n",
    "1. [Implementing a CNN in the TensorFlow low-level API](#Implementing-a-CNN-in-the-TensorFlow-low-level-API)\n",
    "1. [Implementing a CNN in the TensorFlow Layers API](#Implementing-a-CNN-in-the-TensorFlow-Layers-API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T01:12:14.427884Z",
     "start_time": "2019-06-30T01:12:12.668268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from io import StringIO\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.utils as utils\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom extensions and settings\n",
    "sys.path.append(\"/home/mlmachine\") if \"/home/mlmachine\" not in sys.path else None\n",
    "sys.path.append(\"/home/prettierplot\") if \"/home/prettierplot\" not in sys.path else None\n",
    "\n",
    "import mlmachine as mlm\n",
    "from prettierplot.plotter import PrettierPlot\n",
    "import prettierplot.style as style\n",
    "\n",
    "# magic functions\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building blocks of convolutional neural networks\n",
    "\n",
    "Convolutional neural networks (CNNs) were inspired by how the visual cortex of the human brain functions when it is recognizing objects. Due to the high performance of CNNs for image classification, this approach has gained a lot of attention and this led to great improvements in machine learning and computer vision applications. Neural networks are able to automatically learn the features from raw data that are most useful for a particular task. This is why neural networks are often thought of as feature extraction engine: that is, the intial layer that immediately follow the input nodes are those that are used to extract low-level features.\n",
    "\n",
    "Multilayer neural networks, and in particular, deep convolutional neural networks, construct a feature hierarchy by combing low-levek featurs in layer-like fasion to form high-level features. In the context of images, low-level features like edges and blobs are extracted in the early layers, which are combined together to form high-level features that take more familiar shapes, such as buildings, cars or dogs. CNNs contruct feature maps from input images, where each element in the feature map comes from a local patch of pixels in the input images.\n",
    "\n",
    "A local patch of pixels is refered to as the local receptive field. A CNN's performance on image-related tasks is driven by two important concepts:\n",
    "\n",
    "1. Sparse connectivity - A single element in the feature map is connected to just a small patch of pixels, as opposed to the whole input image. The latter is true of perceptrons.\n",
    "2. Parameter sharing - The same weights are used for different patches of the input image.\n",
    "\n",
    "Because of the two concepts, the number of weights in the network drastically decrease, and there is also an improvement in the algorithm's ability to capture salient features. It makes intuitive sense that the nearby pixles are more relevant to each other than pixels that are far away from each other.\n",
    "\n",
    "CNNs are typically composed of several convolutional layers and subsampling/pooling layers that are followed by one or more full connected layers at the end. The fully connected layers are effectively a multilater perceptron, where every input unit $i$ is connected to every output unit $j$ with weight $w_{ij}$. One thing to note about pooling/subsampling layers is that these do not have any learnable parameters - there are no weights or bias units. Both convolutional and fully connected layers have weights and biases to be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Building-blocks-of-convolutional-neural-networks'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding CNNs and learning feature hierarchies\n",
    "\n",
    "Salient, or relevant, features are essential for high performing machine learning algorithms. Traditional machine learning algorithms rely on features determined by a domain expert, or by some computational feature extraction technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Understanding-CNNs-and-learning-feature-hierarchies'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing discrete convolutions in one dimension\n",
    "\n",
    "A discrete convolution (or just a convolution) is a fundamental concept of CNNs. As a basic example, we can look at a discrete convolution between two one-dimensional vectors $\\textbf{x}$ and $\\textbf{w}$. This is denoted by the formula: $\\textbf{y} = \\textbf{x} * \\textbf{w}$, where vector $\\textbf{x}$ is the input, or signal, and $\\textbf{w}$ is referred to as the filter, or kernel. A discrete convolution is mathematically defined as follows:\n",
    "\n",
    "$$\n",
    "\\textbf{y} = \\textbf{x} * \\textbf{w} \\rightarrow \\textbf{y}[i] = \\sum^\\infty_{k = -\\infty} \\textbf{X}[i - k]\\textbf{w}[k]\n",
    "$$\n",
    "\n",
    "The brackets [] are used to denote the indexing for vector elements. The index $i$ runs through each element of the output vector $\\textbf{y}$. The positive and negative infinity indexing for $\\textbf{x}$ needs a bit more explanation - a sum that runs through indices in such a range seems odd because, generally speaking, machine learning applications deal with finite feature vectors. As an example, if vector $\\textbf{x}$ has 10 features with indices 0,1,2,3,4,5,6,7,8,9, then the indices $-\\infty$ to -1 and 10 to $\\infty$ are out of bounds for $\\textbf{x}$. So in order to compute the summation shown in the predecing formula, it is assumed that $\\textbf{x}$ and $\\textbf{w}$ are filled with zeros. This results in an output vetor $\\textbf{y}$ that also has an infinite size with lots of zeros as well. Since this isn't useful in practice, $\\textbf{x}$ is padded only with a finite number of zeros. This process is called zero-padding, or just padding. The original vector $\\textbf{x}$, with padding $p$ = 2 zeroes, the vector changes from [3,2,1,7,1,2,5,4] to [0,0,3,2,1,7,1,2,5,4,0,0].\n",
    "\n",
    "Let's assume the original input $\\textbf{x}$ and $\\textbf{w}$ have n and m elements, respectively, where m <= n. So the padded vector $\\textbf{x}^p$ has the size $n + 2p$, and the practical formula for computing a discrete convolution will change to:\n",
    "\n",
    "$$\n",
    "\\textbf{y} = \\textbf{x} * \\textbf{w} \\rightarrow \\textbf{y}[i] = \\sum^{k=m-1}_{k = 0} \\textbf{x}^p[i + m - k]\\textbf{w}[k]\n",
    "$$\n",
    "\n",
    "\n",
    "This solves the infinite index issue. The second issue is indexing $\\textbf{x}$ with $i + m - k$. The problem is that $\\textbf{x}$ and $\\textbf{w}$ are indexed in different directions in this summation. For this reason, we flip one of the vectors, could be either one, after adding the padding. Then we can compute their dot product. So if we flip the filter $\\textbf{w}$ to get $\\textbf{w}^r$, then the dot product $\\textbf{x}[i:i+m] \\cdot \\textbf{w}^r$ is computed to get one element $\\textbf{y}[i]$, where $\\textbf{x}[i:i+m]$ is a patch of $\\textbf{x}$ with size $m$. We use $m$ to determine the size of the patches because the length of $\\textbf{w}$ = $m$ = 4, and this is the vector that will be used with $\\textbf{x}$ to determine a dot product. Therefore, the size of $\\textbf{x}$ must be the same as $\\textbf{w}$.\n",
    "\n",
    "This iterative approach is repeated in a way that mimics a window sliding across the image, which gets all of the output elements. A one-dimensional example would be:\n",
    "\n",
    "$$\n",
    "x = (3,2,1,7,1,2,5,4)\n",
    "\\\\\n",
    "w = \\bigg(\\frac{1}{2}, \\frac{3}{4}, 1, \\frac{1}{4}\\bigg)\n",
    "$$\n",
    "\n",
    "If we flip $\\textbf{w}$ we get:\n",
    "\n",
    "$$\n",
    "\\textbf{w}^r = \\bigg(\\frac{1}{4}, 1, \\frac{3}{4}, \\frac{1}{2}\\bigg)\n",
    "$$\n",
    "\n",
    "Now we need to calculate $\\textbf{y}$ by finding the dot product of three patches of $\\textbf{x}$ with $\\textbf{w}$, as determined by $\\textbf{x}[i:i+4]$:\n",
    "\n",
    "$$\n",
    "[3,2,1,7] \\cdot \\bigg[\\frac{1}{4}, 1, \\frac{3}{4}, \\frac{1}{2}\\bigg] = 7\n",
    "\\\\\n",
    "[1,7,1,2] \\cdot \\bigg[\\frac{1}{4}, 1, \\frac{3}{4}, \\frac{1}{2}\\bigg] = 9\n",
    "\\\\\n",
    "[1,2,5,4] \\cdot \\bigg[\\frac{1}{4}, 1, \\frac{3}{4}, \\frac{1}{2}\\bigg] = 8\n",
    "$$\n",
    "\n",
    "Which yield $\\textbf{y}$ = [7, 9, 8]\n",
    "\n",
    "We should note that the padding in this example is zero. Also the rotated filter $\\textbf{w}^r$ is shifted by two cells each time. This shift is a hyperparameter of a convolution called the stride $s$. In that example we used $s$ = 2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Performing-discrete-convolutions-in-one-dimension'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of zero-padding in a convolution\n",
    "\n",
    "As an example, say we have $n$ = 5 and $m$ = 3. If $p$ = 0, then $\\textbf{x}$[0] is only used in computing one output element $\\textbf{y}$[0], while $\\textbf{x}$[1] is used to compute two elements ($\\textbf{y}$[0] and $\\textbf{y}$[1]). In practice, this different treatment of elements in $\\textbf{x}$ can put more emphasis on the middle element $\\textbf{x}$[2] since this will show up in the most computations. If we choose $p$ = 2 instead to apply padding of two zeros on either end of $\\textbf{x}$, then each element in $\\textbf{x}$ will be involved in computing three elements of $\\textbf{y}$.\n",
    "\n",
    "The size of padding also affects the size out the output $\\textbf{y}$. There are three modes of padding that are commonly used\n",
    "\n",
    "-full: the padding parameter is set to $p$ = $m$ - 1. Full padding increases the dimensions of the output and is rarely used in CNN architecture\n",
    "-same: same padding is generally used when we want the size of the output $\\textbf{y}$ to be the same as the input $\\textbf{x}$. The padding parameter is computed according to the filter size.\n",
    "-valid: valid padding is the case where no padding is used.\n",
    "\n",
    "A 5 by 5 input pixel will create very different outputs depending on which padding type is used. When using a kernel size of 3 by 3 and a stride of 1, full padding creates a 7 by 7 output, same padding creates a 5 by 5 output, and valid padding creates a 2 by 2 output.\n",
    "\n",
    "Same padding is the most commonly used padding mode in CNNs. One advantage of this approach is that is preserves the height and width of the input images or tensors. On major disadvantage of valid padding versus full and same padding is that the volume of the tensors decreases substantially in networks with many layers, which leads to poor performance.\n",
    "\n",
    "It is recommended that spatial size is preserved in convolutional layers by using same padding, and spatial size then gets reduced in pooling layers instead.\n",
    "\n",
    "Full padding results in outputs larger than the inputs. It is usually used in signal processing applications where it is important to minimize boundary effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-effect-of-zero-padding-in-a-convolution'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the size of the convolution output\n",
    "\n",
    "The output size of a convolution is determined by the total number times we shift the filter $\\textbf{w}$ along the input vector. If the input vector is of size $n$ and the filter is of size $m$, then the size of the output resulting from $\\textbf{x}*\\textbf{w}$ with padding $p$ and stride $s$ is determined by:\n",
    "\n",
    "$$\n",
    "o = \\Bigl\\lfloor\\frac{n+2p-m}{s}\\Bigr\\rfloor + 1\n",
    "$$\n",
    "\n",
    "The 'floor' brackets $\\lfloor.\\rfloor$ return the largest interger that is equal to or smaller than the input. It rounds down to the nearest integer, unless the input is already an integer.\n",
    "\n",
    "If we have an input vector of size 10 with a convolution kernel of size 5, padding of 2, and stride of 1, then the output size equals:\n",
    "$$\n",
    "o = \\Bigl\\lfloor\\frac{10+2\\times2-5}{1}\\Bigr\\rfloor + 1 = 10\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Determining-the-size-of-the-convolution-output'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T01:12:16.855578Z",
     "start_time": "2019-06-30T01:12:16.846600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homegrown result: [ 5. 14. 16. 26. 24. 34. 19. 22.]\n",
      "numpy result: [ 5 14 16 26 24 34 19 22]\n"
     ]
    }
   ],
   "source": [
    "# simple, homegrown one-dimensional implentation for computing convolutions vs. numpy\n",
    "def conv1d(x, w, p=0, s=1):\n",
    "    w_rot = np.array(w[::-1])\n",
    "    x_padded = np.array(x)\n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape=p)\n",
    "        x_padded = np.concatenate([zero_pad, x_padded, zero_pad])\n",
    "    result = []\n",
    "    for i in range(0, int(len(x) / s), s):\n",
    "        result.append(np.sum(x_padded[i : i + w_rot.shape[0]] * w_rot))\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "x = [1, 3, 2, 4, 5, 6, 1, 3]\n",
    "w = [1, 0, 3, 1, 2]\n",
    "\n",
    "print(\"homegrown result: {}\".format(conv1d(x, w, p=2, s=1)))\n",
    "print(\"numpy result: {}\".format(np.convolve(x, w, mode=\"same\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing a discrete convolution in 2D\n",
    "\n",
    "All of the concepts learned above are extendible to two dimensions. When dealing with two-dimensional input, such as a matrix $\\textbf{X}_{n_1\\times n_2}$ and the filter matrix $\\textbf{W}_{m_1\\times m_2}$, where $m_1$ <= $n_1$ and $m_2$ <= $n_2$, the matrix $\\textbf{Y} = \\textbf{X} * \\textbf{W}$ results from the 2D convolution of $\\textbf{X}$ and $\\textbf{W}$. The formula for this is:\n",
    "\n",
    "$$\n",
    "\\textbf{Y} = \\textbf{X}*\\textbf{w} \\rightarrow \\textbf{Y}[i,j] = \\sum^\\infty_{k_1 = -\\infty}\\sum^\\infty_{k_2 = -\\infty}\\textbf{X}[i-k_1,j-k_2]\\textbf{W}[k_1,k_2]\n",
    "$$\n",
    "\n",
    "If we dropped one of the dimensions from this formula, the formula that would remain is exactly like the one-dimensional equation used above. All of the techniques, including padding, rotating the filter matrix, and stride use are applicable to two-dimensional convolutions. Here is an example where we start with an input matrix $\\textbf{X}_{4 \\times 4}$, a kernel matrix $\\textbf{W}_{3 \\times 3}$, padding $p$ = (1,1) and stride $s$ = (1,1). The padding results in $\\textbf{X}_{5 \\times 5}^{padded}$. With this stride, the resulting matrix $\\textbf{Y}$ will be 4 by 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Performing-a-discrete-convolution-in-2D'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T01:12:19.577999Z",
     "start_time": "2019-06-30T01:12:19.451589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homegrown result: [[11. 25. 32. 13.]\n",
      " [19. 25. 24. 13.]\n",
      " [13. 28. 25. 17.]\n",
      " [11. 17. 14.  9.]]\n",
      "scipy result: [[11 25 32 13]\n",
      " [19 25 24 13]\n",
      " [13 28 25 17]\n",
      " [11 17 14  9]]\n"
     ]
    }
   ],
   "source": [
    "# simple, homegrown two-dimensional implentation for computing convolutions vs. scipy\n",
    "def conv2d(X, W, p=(0, 0), s=(1, 1)):\n",
    "    W_rot = np.array(W)[::-1, ::-1]\n",
    "    X_orig = np.array(X)\n",
    "    n1 = X_orig.shape[0] + 2 * p[0]\n",
    "    n2 = X_orig.shape[1] + 2 * p[1]\n",
    "    X_padded = np.zeros(shape=(n1, n2))\n",
    "    X_padded[p[0] : p[0] + X_orig.shape[0], p[1] : p[1] + X_orig.shape[1]] = X_orig\n",
    "\n",
    "    result = []\n",
    "    for i in range(0, int((X_padded.shape[0] - W_rot.shape[0]) / s[0]) + 1, s[0]):\n",
    "        result.append([])\n",
    "        for j in range(0, int((X_padded.shape[1] - W_rot.shape[1]) / s[1]) + 1, s[1]):\n",
    "            X_sub = X_padded[i : i + W_rot.shape[0], j : j + W_rot.shape[1]]\n",
    "            result[-1].append(np.sum(X_sub * W_rot))\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "X = [[1, 3, 2, 4], [5, 6, 1, 3], [1, 2, 0, 2], [3, 4, 3, 2]]\n",
    "W = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\n",
    "\n",
    "print(\"homegrown result: {}\".format(conv2d(X, W, p=(1, 1), s=(1, 1))))\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "print(\"scipy result: {}\".format(scipy.signal.convolve2d(X, W, mode=\"same\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsampling\n",
    "\n",
    "Subsampling is generally applied in two forms of pooling operations in CNNs, max-pooling and mean-pooling (or average-pooling). The pooling layer is denoted by $\\textbf{P}_{n_1 \\times n_2}$. The subscript determines the size of the neighborhood, i.e. the number adjacent pixels in each dimension, in which the max or mean operation is performed. This neighborhood is referred to as pooling size.\n",
    "\n",
    "The advantage of pooling is that it introduces local invariance, meaning that small changes in the local neighborhood do not change the result of max-pooling. It helps generate features that are more robust to noise. Pooling also decreases the size of eatures, which leads to better computational efficiency. This may also reduce overfitting.\n",
    "\n",
    "Pooling is typically nonoverlapping. To accomplish this, the stride parameter and the pool size must be equal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Subsampling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together to build a CNN\n",
    "\n",
    "The commonalities between operations in traditional multilayer neural networks and CNNs are clear. With MLPs, we use matrix-vector multiplications to get net-inputs (pre-activations), as in $\\textbf{a} = \\textbf{W}\\textbf{x}+\\textbf{b}$. IF we were to use an MLP for image recognition, the column vector $\\textbf{x}$ contain the pixels, $\\textbf{W}$ is the weight matrix that connects the pixel inputs to each hidden unit. In a CNN, this operation is replaced by the convolution operation $\\textbf{A} = \\textbf{W}*\\textbf{X} + b$, where $\\textbf{X}$ is a matrix containing pixels in a height by width arrangement. \n",
    "\n",
    "In both arrangements, the calculated pre-activationsa re passed to an activation function to obtain the activation of a hidden unit $\\textbf{H} = \\phi(A)$, where $\\phi$ is the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Putting-everything-together-to-build-a-CNN'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with multiple input or color channels\n",
    "\n",
    "An input sample to a CNN layer may contain one or more two-dimensional arrays with dimensions $N_1 \\times N_2$, which describes the image height and width in pixels. These arrays are called channels. This representation requires us to use rank-3 tensors, or a three-dimensional array $\\textbf{X}_{N_1 \\times N_2 \\times C_{in}}$, where $C_{in}$ is the numper of input channels.\n",
    "\n",
    "For example, if an image is colored using an RGB color scheme, then $C_{in}$ = 3, for red, green and blue color channels in RGB). A CNN willj perform the convolution operation for each channel separately and then add the results together using matrix summation. The convolution associated with each channel $c$ has its own kernel matrix $\\textbf{W}[:,:,c]$. The total net-input/pre-activation result is computed by:\n",
    "\n",
    "$$\n",
    "\\mbox{Given a sample} \\ \\textbf{X}_{n_1 \\times n_2 \\times C_{in}}\n",
    "\\\\\n",
    "\\mbox{a kernel matrix} \\ \\textbf{W}_{m_1 \\times m_2 \\times C_{in}}\n",
    "\\\\\n",
    "\\mbox{and a bias value} \\ b \n",
    "\\\\\n",
    "...\n",
    "\\\\\n",
    "\\textbf{Y}^{Conv} = \\sum^{C_{in}}_{c=1}\\textbf{W}[:,:,c] * \\textbf{X}[:,:,c]\n",
    "\\\\\n",
    "\\mbox{pre-activation:} \\ \\textbf{A} = \\textbf{Y}^{Conv} + b\n",
    "\\\\\n",
    "\\mbox{feature map:} \\ \\textbf{H} = \\phi(\\textbf{A})\n",
    "$$\n",
    "\n",
    "The final result $\\textbf{H}$ is our feature map. CNNs typically have more than one feature map, which makes the kernel tensor four-dimensional: $m_1 \\times m_2 \\times C_{in} \\times C_{out}$, which captures the height by width, the number input channels, and the number of output feature maps. Updating the formula progression above:\n",
    "\n",
    "$$\n",
    "\\mbox{Given a sample} \\ \\textbf{X}_{n_1 \\times n_2 \\times C_{in}}\n",
    "\\\\\n",
    "\\mbox{a kernel matrix} \\ \\textbf{W}_{m_1 \\times m_2 \\times C_{in} \\times C_{out}}\n",
    "\\\\\n",
    "\\mbox{and a bias value} \\ b_{out}\n",
    "\\\\\n",
    "...\n",
    "\\\\\n",
    "\\textbf{Y}^{Conv}[:,:,k] = \\sum^{C_{in}}_{c=1}\\textbf{W}[:,:,c,k] * \\textbf{X}[:,:,c]\n",
    "\\\\\n",
    "\\mbox{pre-activation:} \\ \\textbf{A}[:,:,k] = \\textbf{Y}^{Conv}[:,:,k] + b[k]\n",
    "\\\\\n",
    "\\mbox{feature map:} \\ \\textbf{H}[:,:,k] = \\phi(\\textbf{A}[:,:,k])\n",
    "$$\n",
    "\n",
    "As an example, consider an image with three input channels. The kernel tensor is 4D. Each kernel matrix in the tensor is denoted as $m_1 \\times m_2$, and there are three matrixes, one for each input channel. There are five of these $m_1 \\times m_2 \\times 3$ kernels, which will give us five output feature maps. Lastly, there is apooling for subsampling the resulting feature maps.\n",
    "\n",
    "Through the convolution operation, the input $\\textbf{X}_{n_1 \\times n_2 \\times 3}$ and the 4D kernel tensor $\\textbf{W}_{m_1 \\times m_2 \\times 3 \\times 5}$ yield 5 output matrices $C_{out}$. These matrices are of size $n_1 \\times n_2 \\times 5$ if we use 'samee' zero-padding. The five output matrices are then put through a pooling layer, which yields 5 smaller matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Working-with-multiple-input-or-color-channels'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing a neural network with dropout\n",
    "\n",
    "Whether we're using a traditional, fully connected neural network or a CNN, determining the size of the network is a challenge. There are many parameters to consider and these must be tuned to achieve good performance. A network's capacity refers to the level of complexity of the function that it can learn. Small networks are likely to under fit, and very large networks can easily overfit.\n",
    "\n",
    "In practice, it is common to build a network with large capacity so that it does well on the training set, and then, to prevent overfitting, apply one or more regularization schemes to also achieve good generalization performance on new data. A popular regularization choice is L2 regularization. Another regularization technique that has increased in popularity is called dropout. This can be thoguht of as the consensus, or averaging, that occurs through ensembling. In ensemble learning, several models are trained independently. Then during the prediction phase, we use the consensus of all the models. In the context of neural networks, howevever, training several models and determining averages of multiple models would be too computationally expensive. Dropout is an efficient workaround that trains several models at once and then computes the average predictions during the prediction stage.\n",
    "\n",
    "Dropout is typically applied to hidden units of higher layers. During the neural network training phase, a fraction of the hidden units is randomly dropped during every iteration with probability $p_{drop}$ (or the keep probability of $p_{keep} = 1 - p_{drop}$. This is a user-defined parameter, and a common choice is $p$ = 0.5. This dropping of input neurons has the effect rescaling the remaing neurons such that they are rescaled to account for the missing/dropped neurons in higher layers. The effect of dropout is that it forces the network to learn a redundant reprsentation of the data. The network cannot rely on the activation of any one set of hidden units since units are randomly turned off during the training phase and is therefore required to come up with a more general and robust understanding of the patterns in the data.\n",
    "\n",
    "When the learned model is used for prediction, we use all of the neurons, i.e. $p_{drop} = 0$ when making predictions. To ensure that the overall activations are on the same scale during the training and predictions phases, the activations of the active neurons need to be scaled appropriately. If we used $p$ = 0.5, then the activation would need to be halved. In practice, it is inconvenient to scale activations when making predictions, libraries like TensorFlow scale the activations during training. So if we used $p$ = 0.5, then the activations would be doubled.\n",
    "\n",
    "In conclusion, drop relates to ensemble learning in that dropping different hidden neurons at random during each iteration effectively trains different models on different features, much like a random forst. Once all of the models are trained, we use a model with all hidden units in play, which allows us to use the average activation from all of the hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Regularizing-a-neural-network-with-dropout'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a deep convolutional neural network using TensorFlow\n",
    "\n",
    "In chapter 13, we used an MLP on the handwritten digit recognition problem using different TensorFlow APIs and achieved about 97 percent accuracy. This sounds pretty good, but in many applications, such as financial transactions involving checks, mistakes can be costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Implementing-a-deep-convolutional-neural-network-using-TensorFlow'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multilayer CNN architecture\n",
    "\n",
    "For the input, we will use 28 by 28 grayscale images, which means $C_{in}$ = 1. The input data will go through two convolutional layers that have a kernel size of 5 by 5. the first convolution has 32 output feature maps and the second has 64. Each convolution layer is followed by subsampling layer that performs max-pooling. The dimensions of the tensors in each layer are:\n",
    "\n",
    "$$\n",
    "Input: \\ [batchsize \\times 28 \\times 28 \\times 1]\n",
    "\\\\\n",
    "Conv1: \\ [batchsize \\times 24 \\times 24 \\times 32]\n",
    "\\\\\n",
    "Pool1: \\ [batchsize \\times 12 \\times 12 \\times 32]\n",
    "\\\\\n",
    "Conv2: \\ [batchsize \\times 8 \\times 8 \\times 64]\n",
    "\\\\\n",
    "Pool2: \\ [batchsize \\times 4 \\times 4 \\times 64]\n",
    "\\\\\n",
    "FullConnect1: \\ [batchsize \\times 1024]\n",
    "\\\\\n",
    "FullConnect2 and softmax: \\ [batchsize \\times 10]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-multilayer-CNN-architecture'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and prepreocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Loading-and-prepreocessing-the-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:25:47.794861Z",
     "start_time": "2019-06-30T02:25:39.366397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimensions: (42000, 785)\n",
      "Test data dimensions: (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Load data and print dimensions\n",
    "df_train = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-mnist//train.csv\", sep=\",\")\n",
    "df_test = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-mnist//test.csv\", sep=\",\")\n",
    "\n",
    "print(\"Training data dimensions: {}\".format(df_train.shape))\n",
    "print(\"Test data dimensions: {}\".format(df_test.shape))\n",
    "\n",
    "# separate\n",
    "df_train_label = df_train[\"label\"]\n",
    "df_train = df_train.drop(labels=\"label\", axis=1)\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    df_train, df_train_label, test_size=0.2\n",
    ")\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:25:56.726743Z",
     "start_time": "2019-06-30T02:25:56.044865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize samples digits\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X_train[y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap=\"Greys\")\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:26:12.655270Z",
     "start_time": "2019-06-30T02:26:12.648844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (33600, 784) (33600,)\n",
      "Validation: (4400, 784) (4400,)\n",
      "Test: (4000, 784) (4000,)\n"
     ]
    }
   ],
   "source": [
    "# train/validation data splita\n",
    "X_valid, y_valid = X_test[4000:, :], y_test[4000:]\n",
    "X_test, y_test = X_test[:4000, :], y_test[:4000]\n",
    "\n",
    "print(\"Training: {} {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Validation: {} {}\".format(X_valid.shape, y_valid.shape))\n",
    "print(\"Test: {} {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:26:20.022498Z",
     "start_time": "2019-06-30T02:26:20.016578Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom function for generating mini batches of data\n",
    "def batch_generator(X, y, batch_size=64, shuffle=False, random_seed=None):\n",
    "    idx = np.arange(y.shape[0])\n",
    "\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        rng.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X[i : i + batch_size, :], y[i : i + batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above will return a generator with a tuple for a match of samples, our X inputs and y labels. We also need to normalize the data by subtracting the means and dividing by the standard deviation as this will likely improve training performance and speed up convergence. \n",
    "\n",
    "To accomplish this, we compute the mean of each feature using the training data, and then calculate the standard deviation across all features. The reason for calculating the standard deviation this way, rather than dividing each feature by its own individual standard deviation, is that some features/pixels have a constant value of 255 across all images. This indicates no variation across all samples, and therefore, the standard deviation of those features will be zero, and this would cause a division by zero error. To do this, we can use np.std() without specifying an axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:26:24.429903Z",
     "start_time": "2019-06-30T02:26:24.023749Z"
    }
   },
   "outputs": [],
   "source": [
    "# standardize data\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_vals = np.std(X_train)\n",
    "\n",
    "X_trainCent = (X_train - mean_vals) / std_vals\n",
    "X_validCent = (X_valid - mean_vals) / std_vals\n",
    "X_testCent = (X_test - mean_vals) / std_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a CNN in the TensorFlow low-level API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Implementing-a-CNN-in-the-TensorFlow-low-level-API'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:26:30.078787Z",
     "start_time": "2019-06-30T02:26:28.426687Z"
    }
   },
   "outputs": [],
   "source": [
    "# wrapper function for performing convolution\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def conv_layer(\n",
    "    input_tensor,\n",
    "    name,\n",
    "    kernel_size,\n",
    "    n_output_channels,\n",
    "    padding_mode=\"SAME\",\n",
    "    strides=(1, 1, 1, 1),\n",
    "):\n",
    "    \"\"\"\n",
    "    input_tensor: tensor given as input into the convolutional layer\n",
    "    name: name of the layer, used to set scope\n",
    "    kernel_size: dimensions of the kernel tensor, provided as a tuple or list\n",
    "    n_output_channels: number of output feature maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        # get n_input_channels: input tensor shape: [batch size x width x height x channels_in]\n",
    "        input_shape = input_tensor.get_shape().as_list()\n",
    "        n_input_channels = input_shape[-1]\n",
    "\n",
    "        weights_shape = list(kernel_size) + [n_input_channels, n_output_channels]\n",
    "        weights = tf.get_variable(name=\"_weights\", shape=weights_shape)\n",
    "        print(weights)\n",
    "\n",
    "        biases = tf.get_variable(\n",
    "            name=\"_biases\", initializer=tf.zeros(shape=[n_output_channels])\n",
    "        )\n",
    "        print(biases)\n",
    "\n",
    "        conv = tf.nn.conv2d(\n",
    "            input=input_tensor, filter=weights, strides=strides, padding=padding_mode\n",
    "        )\n",
    "        print(conv)\n",
    "\n",
    "        conv = tf.nn.bias_add(conv, biases, name=\"bias_activation\")\n",
    "        print(conv)\n",
    "\n",
    "        conv = tf.nn.relu(conv, name=\"activation\")\n",
    "        print(conv)\n",
    "\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The weights are initialized using Xavier/Glorot intitialization by default, whereas the biases are initialized to the zero. The net pre-activations are passed to ReLU for activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:26:32.745739Z",
     "start_time": "2019-06-30T02:26:32.706161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "<tf.Variable 'convtest/_weights:0' shape=(3, 3, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'convtest/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"convtest/Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/bias_activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# test convolution wrapper function\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    conv_layer(x, name=\"convtest\", kernel_size=(3, 3), n_output_channels=32)\n",
    "del g, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:26:35.871606Z",
     "start_time": "2019-06-30T02:26:35.862508Z"
    }
   },
   "outputs": [],
   "source": [
    "# fully connected layer wrapper function\n",
    "def fc_layer(input_tensor, name, n_output_units, activation_fn=None):\n",
    "    \"\"\"\n",
    "    input_tensor: the input tensor\n",
    "    name: name of the layer, used to define scope\n",
    "    n_output_units: number of output units\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = input_tensor.get_shape().as_list()[1:]\n",
    "        n_input_units = np.prod(input_shape)\n",
    "        if len(input_shape) > 1:\n",
    "            input_tensor = tf.reshape(input_tensor, shape=(-1, n_input_units))\n",
    "\n",
    "        weights_shape = [n_input_units, n_output_units]\n",
    "        weights = tf.get_variable(name=\"_weights\", shape=weights_shape)\n",
    "        print(weights)\n",
    "\n",
    "        biases = tf.get_variable(\n",
    "            name=\"_biases\", initializer=tf.zeros(shape=[n_output_units])\n",
    "        )\n",
    "        print(biases)\n",
    "\n",
    "        layer = tf.matmul(input_tensor, weights)\n",
    "        print(layer)\n",
    "\n",
    "        layer = tf.nn.bias_add(layer, biases, name=\"net_pre-activation\")\n",
    "        print(layer)\n",
    "\n",
    "        if activation_fn is None:\n",
    "            return layer\n",
    "\n",
    "        layer = activation_fn(layer, name=\"activation\")\n",
    "        print(layer)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:26:42.017976Z",
     "start_time": "2019-06-30T02:26:41.994962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'fctest/_weights:0' shape=(784, 32) dtype=float32_ref>\n",
      "<tf.Variable 'fctest/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"fctest/MatMul:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/net_pre-activation:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/activation:0\", shape=(?, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# test full connected layer wrapper function\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    fc_layer(x, name=\"fctest\", n_output_units=32, activation_fn=tf.nn.relu)\n",
    "del g, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The way this wrapper function is utilized will be different for each of the two fully connected layer. the first fully connected layer gets its input right after a convolutional layer, so the input is still a four-dimensional tensor. The second fully connected layer requires that the input tensor be flattened, which we accomplish using the tf.reshape function. (my note - I wonder if this is reversed. I would think the 4D tensor approaching the first fully connected layer would need to be flattened, and this would be caught by the condtional evaluating the length of the input shape)\n",
    "\n",
    "> Also, the net pre-activations from the first fully connected layer are passed to the ReLU activation function, whereas the second corresponds to the logits, so we use a lienar activation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:26:45.692345Z",
     "start_time": "2019-06-30T02:26:45.672699Z"
    }
   },
   "outputs": [],
   "source": [
    "# build CNN\n",
    "def build_cnn():\n",
    "    # placeholders for X and y\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, 784], name=\"tf_x\")\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None], name=\"tf_y\")\n",
    "\n",
    "    # reshape x to a 4D tensor, [batch x width x height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1], name=\"tf_x_reshaped\")\n",
    "\n",
    "    # one-hot encoding\n",
    "    tf_y_onehot = tf.one_hot(\n",
    "        indices=tf_y, depth=10, dtype=tf.float32, name=\"tf_y_onehot\"\n",
    "    )\n",
    "\n",
    "    ## Build model\n",
    "    # 1st layer - conv 1\n",
    "    print(\"\\nBuilding 1st layer:\\n\")\n",
    "    h1 = conv_layer(\n",
    "        tf_x_image,\n",
    "        name=\"conv_1\",\n",
    "        kernel_size=(5, 5),\n",
    "        padding_mode=\"VALID\",\n",
    "        n_output_channels=32,\n",
    "    )\n",
    "\n",
    "    # max pooling\n",
    "    h1_pool = tf.nn.max_pool(\n",
    "        h1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\"\n",
    "    )\n",
    "\n",
    "    # 2nd layer -\n",
    "    print(\"\\nBuilding 2nd layer:\\n\")\n",
    "    h2 = conv_layer(\n",
    "        h1_pool,\n",
    "        name=\"conv_2\",\n",
    "        kernel_size=(5, 5),\n",
    "        padding_mode=\"VALID\",\n",
    "        n_output_channels=64,\n",
    "    )\n",
    "\n",
    "    # max pooling\n",
    "    h2_pool = tf.nn.max_pool(\n",
    "        h2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\"\n",
    "    )\n",
    "\n",
    "    # 3rd layer -\n",
    "    print(\"\\nBuilding 3rd layer:\\n\")\n",
    "    h3 = fc_layer(h2_pool, name=\"fc_3\", n_output_units=1024, activation_fn=tf.nn.relu)\n",
    "\n",
    "    # dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"fc_keep_prob\")\n",
    "    h3_drop = tf.nn.dropout(h3, keep_prob=keep_prob, name=\"dropout_layer\")\n",
    "\n",
    "    # 4th layer: fully connected (linear activation)\n",
    "    print(\"\\nBuilding 4th layer:\\n\")\n",
    "    h4 = fc_layer(h3_drop, name=\"fc_4\", n_output_units=10, activation_fn=None)\n",
    "\n",
    "    ## prediction\n",
    "    predictions = {\n",
    "        \"probabilities\": tf.nn.softmax(h4, name=\"probabilities\"),\n",
    "        \"labels\": tf.cast(tf.argmax(h4, axis=1), tf.int32, name=\"labels\"),\n",
    "    }\n",
    "\n",
    "    ## visualize with TensorBoard\n",
    "    # loss function an optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=h4, labels=tf_y_onehot),\n",
    "        name=\"cross_entropy_loss\",\n",
    "    )\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss, name=\"train_op\")\n",
    "\n",
    "    # computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(predictions[\"labels\"], tf_y, name=\"correct_preds\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:26:50.915351Z",
     "start_time": "2019-06-30T02:26:50.903863Z"
    }
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def save(saver, sess, epoch, path=\"./ch15_files/model\"):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    print(\"Saving model in {}\".format(path))\n",
    "    saver.save(sess, os.path.join(path, \"cnn-model.ckpt\"), global_step=epoch)\n",
    "\n",
    "\n",
    "def load(saver, sess, path, epoch):\n",
    "    print(\"Loading model from {}\".format(path))\n",
    "    saver.restore(sess, os.path.join(path, \"cnn-model.ckpt-{}\".format(epoch)))\n",
    "\n",
    "\n",
    "def train(\n",
    "    sess,\n",
    "    training_set,\n",
    "    validation_set=None,\n",
    "    initialize=True,\n",
    "    epochs=20,\n",
    "    shuffle=True,\n",
    "    dropout=0.5,\n",
    "    random_seed=None,\n",
    "):\n",
    "    X_data = np.array(training_set[0])\n",
    "    y_data = np.array(training_set[1])\n",
    "    training_loss = []\n",
    "\n",
    "    # initialize variables\n",
    "    if initialize:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(random_seed)  # for batch generator shuffling\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        batch_gen = batch_generator(X_data, y_data, shuffle=shuffle)\n",
    "        avg_loss = 0.0\n",
    "        for i, (batch_x, batch_y) in enumerate(batch_gen):\n",
    "            feed = {\"tf_x:0\": batch_x, \"tf_y:0\": batch_y, \"fc_keep_prob:0\": dropout}\n",
    "            loss, _ = sess.run([\"cross_entropy_loss:0\", \"train_op\"], feed_dict=feed)\n",
    "            avg_loss += loss\n",
    "\n",
    "        training_loss.append(avg_loss / (i + 1))\n",
    "        print(\n",
    "            \"Epoch {:2d} training average loss: {:.3f}\".format(epoch, avg_loss), end=\" \"\n",
    "        )\n",
    "\n",
    "        if validation_set is not None:\n",
    "            feed = {\n",
    "                \"tf_x:0\": validation_set[0],\n",
    "                \"tf_y:0\": validation_set[1],\n",
    "                \"fc_keep_prob:0\": 1.0,\n",
    "            }\n",
    "            valid_acc = sess.run(\"accuracy:0\", feed_dict=feed)\n",
    "            print(\"Validation accuracy: {:.3f}\".format(valid_acc))\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "\n",
    "def predict(sess, X_test, return_proba=False):\n",
    "    feed = {\"tf_x:0\": X_test, \"fc_keep_prob:0\": 1.0}\n",
    "    if return_proba:\n",
    "        return sess.run(\"probabilities:0\", feed_dict=feed)\n",
    "    else:\n",
    "        return sess.run(\"labels:0\", feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:26:55.489598Z",
     "start_time": "2019-06-30T02:26:54.919706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building 1st layer:\n",
      "\n",
      "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"conv_1/Conv2D:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/bias_activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "\n",
      "Building 2nd layer:\n",
      "\n",
      "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"conv_2/Conv2D:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/bias_activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "\n",
      "Building 3rd layer:\n",
      "\n",
      "<tf.Variable 'fc_3/_weights:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc_3/_biases:0' shape=(1024,) dtype=float32_ref>\n",
      "Tensor(\"fc_3/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-20-2a0a73b45522>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\n",
      "Building 4th layer:\n",
      "\n",
      "<tf.Variable 'fc_4/_weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'fc_4/_biases:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"fc_4/MatMul:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc_4/net_pre-activation:0\", shape=(?, 10), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-20-2a0a73b45522>:50: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build graph object, set graph-level random seed, construct model\n",
    "learning_rate = 1e-4\n",
    "random_seed = 123\n",
    "\n",
    "# create graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "\n",
    "    # build graph\n",
    "    build_cnn()\n",
    "\n",
    "    # saver\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the train the CNN model. First, we need to create a TensorFlow session to launch the graph, then we call the train funciton. We also need ot initialize all the variables in the network. This initialization is handled by the 'train' function define above and is handled by the 'initialize' parameter. If this is true, the tf.global_variables_initializer() is run. This step should be avoided if we want to train an existing model for additional epochs because we can restore a previously trained model and complete further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:39:30.204487Z",
     "start_time": "2019-06-30T02:26:58.803361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 training average loss: 240.226 Validation accuracy: 0.960\n",
      "Epoch  2 training average loss: 64.715 Validation accuracy: 0.979\n",
      "Epoch  3 training average loss: 42.250 Validation accuracy: 0.983\n",
      "Epoch  4 training average loss: 31.812 Validation accuracy: 0.985\n",
      "Epoch  5 training average loss: 26.158 Validation accuracy: 0.987\n",
      "Epoch  6 training average loss: 21.628 Validation accuracy: 0.989\n",
      "Epoch  7 training average loss: 18.379 Validation accuracy: 0.988\n",
      "Epoch  8 training average loss: 15.389 Validation accuracy: 0.991\n",
      "Epoch  9 training average loss: 13.421 Validation accuracy: 0.990\n",
      "Epoch 10 training average loss: 11.852 Validation accuracy: 0.991\n",
      "Epoch 11 training average loss: 10.336 Validation accuracy: 0.991\n",
      "Epoch 12 training average loss: 8.953 Validation accuracy: 0.990\n",
      "Epoch 13 training average loss: 7.837 Validation accuracy: 0.990\n",
      "Epoch 14 training average loss: 6.969 Validation accuracy: 0.991\n",
      "Epoch 15 training average loss: 6.638 Validation accuracy: 0.991\n",
      "Epoch 16 training average loss: 6.331 Validation accuracy: 0.991\n",
      "Epoch 17 training average loss: 4.683 Validation accuracy: 0.992\n",
      "Epoch 18 training average loss: 4.313 Validation accuracy: 0.992\n",
      "Epoch 19 training average loss: 4.071 Validation accuracy: 0.992\n",
      "Epoch 20 training average loss: 3.555 Validation accuracy: 0.990\n",
      "Saving model in ./ch15_files/model\n"
     ]
    }
   ],
   "source": [
    "# first execution\n",
    "with tf.Session(graph=g) as sess:\n",
    "    train(\n",
    "        sess,\n",
    "        training_set=(X_trainCent, y_train),\n",
    "        validation_set=(X_validCent, y_valid),\n",
    "        initialize=True,\n",
    "        random_seed=123,\n",
    "    )\n",
    "    save(saver, sess, epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:40:00.526747Z",
     "start_time": "2019-06-30T02:39:58.138711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building 1st layer:\n",
      "\n",
      "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"conv_1/Conv2D:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/bias_activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "\n",
      "Building 2nd layer:\n",
      "\n",
      "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"conv_2/Conv2D:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/bias_activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "\n",
      "Building 3rd layer:\n",
      "\n",
      "<tf.Variable 'fc_3/_weights:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc_3/_biases:0' shape=(1024,) dtype=float32_ref>\n",
      "Tensor(\"fc_3/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "\n",
      "Building 4th layer:\n",
      "\n",
      "<tf.Variable 'fc_4/_weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'fc_4/_biases:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"fc_4/MatMul:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc_4/net_pre-activation:0\", shape=(?, 10), dtype=float32)\n",
      "Loading model from ./ch15_files/model/\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./ch15_files/model/cnn-model.ckpt-20\n",
      "Test accuracy: 0.98725\n"
     ]
    }
   ],
   "source": [
    "# restore saved model by deleting existing graph g\n",
    "# then create a new graph g2, and reload the trained model\n",
    "# to perform predictions on the test set\n",
    "del g\n",
    "\n",
    "# create enw graph and build model\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    build_cnn()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "# create a new session and restore model\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    load(saver, sess, epoch=20, path=\"./ch15_files/model/\")\n",
    "    preds = predict(sess, X_testCent, return_proba=False)\n",
    "    print(\"Test accuracy: {}\".format(np.sum(preds == y_test) / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - Better prediction accuracy than what was achieved using the MLP in chapter 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:40:04.193365Z",
     "start_time": "2019-06-30T02:40:04.002854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./ch15_files/model/\n",
      "INFO:tensorflow:Restoring parameters from ./ch15_files/model/cnn-model.ckpt-20\n",
      "[9 9 6 3 7 6 7 9 6 2]\n",
      "[[0.         0.         0.         0.00000011 0.00011544 0.00000002\n",
      "  0.         0.0000005  0.         0.9998839 ]\n",
      " [0.         0.         0.         0.         0.00013212 0.\n",
      "  0.         0.00000411 0.00000001 0.99986374]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.00000009 0.         0.00000544 0.00000023 0.         0.\n",
      "  0.         0.99999416 0.         0.00000009]\n",
      " [0.00000001 0.00000088 0.         0.         0.00000023 0.00000001\n",
      "  0.9999988  0.         0.00000006 0.        ]\n",
      " [0.         0.00000023 0.00000381 0.00000002 0.         0.\n",
      "  0.         0.99999595 0.         0.        ]\n",
      " [0.00000002 0.00000534 0.         0.0000003  0.2033195  0.00000004\n",
      "  0.         0.00025084 0.00000025 0.7964237 ]\n",
      " [0.         0.         0.         0.         0.00000001 0.\n",
      "  1.         0.         0.         0.        ]\n",
      " [0.         0.0000003  0.9999875  0.00000004 0.         0.\n",
      "  0.         0.00001222 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# review predicted labels and probabilities on first 10 test samples\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    load(saver, sess, epoch=20, path=\"./ch15_files/model/\")\n",
    "    print(predict(sess, X_testCent[:10], return_proba=False))\n",
    "    print(predict(sess, X_testCent[:10], return_proba=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:55:41.245162Z",
     "start_time": "2019-06-30T02:40:06.347076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./ch15_files/model/\n",
      "INFO:tensorflow:Restoring parameters from ./ch15_files/model/cnn-model.ckpt-20\n",
      "Epoch  1 training average loss: 3.441 Validation accuracy: 0.991\n",
      "Epoch  2 training average loss: 3.910 Validation accuracy: 0.993\n",
      "Epoch  3 training average loss: 2.460 Validation accuracy: 0.991\n",
      "Epoch  4 training average loss: 2.985 Validation accuracy: 0.991\n",
      "Epoch  5 training average loss: 2.157 Validation accuracy: 0.992\n",
      "Epoch  6 training average loss: 2.296 Validation accuracy: 0.991\n",
      "Epoch  7 training average loss: 2.240 Validation accuracy: 0.992\n",
      "Epoch  8 training average loss: 2.184 Validation accuracy: 0.991\n",
      "Epoch  9 training average loss: 2.325 Validation accuracy: 0.992\n",
      "Epoch 10 training average loss: 1.636 Validation accuracy: 0.991\n",
      "Epoch 11 training average loss: 1.474 Validation accuracy: 0.993\n",
      "Epoch 12 training average loss: 1.466 Validation accuracy: 0.991\n",
      "Epoch 13 training average loss: 1.472 Validation accuracy: 0.992\n",
      "Epoch 14 training average loss: 1.407 Validation accuracy: 0.993\n",
      "Epoch 15 training average loss: 1.280 Validation accuracy: 0.992\n",
      "Epoch 16 training average loss: 1.538 Validation accuracy: 0.992\n",
      "Epoch 17 training average loss: 1.562 Validation accuracy: 0.992\n",
      "Epoch 18 training average loss: 1.392 Validation accuracy: 0.992\n",
      "Epoch 19 training average loss: 1.257 Validation accuracy: 0.992\n",
      "Epoch 20 training average loss: 1.192 Validation accuracy: 0.993\n",
      "Saving model in ./ch15_files/model/\n",
      "Test accuracy: 0.9895\n"
     ]
    }
   ],
   "source": [
    "# train the model another 20 epochs\n",
    "# be sure to set initialize = False\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    load(saver, sess, epoch=20, path=\"./ch15_files/model/\")\n",
    "    train(\n",
    "        sess,\n",
    "        training_set=(X_trainCent, y_train),\n",
    "        validation_set=(X_validCent, y_valid),\n",
    "        initialize=False,\n",
    "        epochs=20,\n",
    "        random_seed=123,\n",
    "    )\n",
    "    save(saver, sess, epoch=40, path=\"./ch15_files/model/\")\n",
    "\n",
    "    preds = predict(sess, X_testCent, return_proba=False)\n",
    "    print(\"Test accuracy: {}\".format(np.sum(preds == y_test) / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - Another 20 epochs shows a net decrease in training loss, steady performance on the validation data, and similar results on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a CNN in the TensorFlow Layers API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Implementing-a-CNN-in-the-TensorFlow-Layers-API'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:56:05.968139Z",
     "start_time": "2019-06-30T02:56:05.860950Z"
    }
   },
   "outputs": [],
   "source": [
    "# CNN custom class\n",
    "class ConvNN(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batchsize=64,\n",
    "        epochs=20,\n",
    "        learning_rate=1e-4,\n",
    "        dropout_rate=0.5,\n",
    "        shuffle=True,\n",
    "        random_seed=None,\n",
    "    ):\n",
    "        np.random.seed(random_seed)\n",
    "        self.batchsize = batchsize\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            tf.set_random_seed(random_seed)\n",
    "            self.build()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "        # create a session\n",
    "        self.sess = tf.Session(graph=g)\n",
    "\n",
    "    def build(self):\n",
    "        tf_x = tf.placeholder(tf.float32, shape=[None, 784], name=\"tf_x\")\n",
    "        tf_y = tf.placeholder(tf.int32, shape=[None], name=\"tf_y\")\n",
    "        is_train = tf.placeholder(tf.bool, shape=(), name=\"is_train\")\n",
    "\n",
    "        # reshape x to a 4D tensor, [batch x width x height x 1]\n",
    "        tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1], name=\"input_x_2dimages\")\n",
    "\n",
    "        # one hot encoding\n",
    "        tf_y_onehot = tf.one_hot(\n",
    "            indices=tf_y, depth=10, dtype=tf.float32, name=\"input_y_onehot\"\n",
    "        )\n",
    "\n",
    "        ## build model\n",
    "        # 1st layer - conv\n",
    "        h1 = tf.layers.conv2d(\n",
    "            tf_x_image, kernel_size=(5, 5), filters=32, activation=tf.nn.relu\n",
    "        )\n",
    "\n",
    "        # max pooling\n",
    "        h1_pool = tf.layers.max_pooling2d(h1, pool_size=(2, 2), strides=(2, 2))\n",
    "\n",
    "        # 2nd layer - conv\n",
    "        h2 = tf.layers.conv2d(\n",
    "            h1_pool, kernel_size=(5, 5), filters=64, activation=tf.nn.relu\n",
    "        )\n",
    "\n",
    "        # max pooling\n",
    "        h2_pool = tf.layers.max_pooling2d(h2, pool_size=(2, 2), strides=(2, 2))\n",
    "\n",
    "        # 3rd layer - fully connected\n",
    "        input_shape = h2_pool.get_shape().as_list()\n",
    "        n_input_units = np.prod(input_shape[1:])\n",
    "        h2_pool_flat = tf.reshape(h2_pool, shape=[-1, n_input_units])\n",
    "        h3 = tf.layers.dense(h2_pool_flat, 1024, activation=tf.nn.relu)\n",
    "\n",
    "        # dropout\n",
    "        h3_drop = tf.layers.dropout(h3, rate=self.dropout_rate, training=is_train)\n",
    "\n",
    "        # 4th layer = fully connected w/ linear activation\n",
    "        h4 = tf.layers.dense(h3_drop, 10, activation=None)\n",
    "\n",
    "        ## prediction\n",
    "        predictions = {\n",
    "            \"probabilities\": tf.nn.softmax(h4, name=\"probabilities\"),\n",
    "            \"labels\": tf.cast(tf.argmax(h4, axis=1), tf.int32, name=\"labels\"),\n",
    "        }\n",
    "\n",
    "        ## visualize with TensorBoard\n",
    "        # loss function an optimization\n",
    "        cross_entropy_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=h4, labels=tf_y_onehot),\n",
    "            name=\"cross_entropy_loss\",\n",
    "        )\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = optimizer.minimize(cross_entropy_loss, name=\"train_op\")\n",
    "\n",
    "        ## find accuracy\n",
    "        correct_predictions = tf.equal(\n",
    "            predictions[\"labels\"], tf_y, name=\"correct_preds\"\n",
    "        )\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32), name=\"accuracy\"\n",
    "        )\n",
    "\n",
    "    def save(self, epoch, path=\"./ch15_files/layers\"):\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "        print(\"Saving model in {}\".format(path))\n",
    "        saver.save(self.sess, os.path.join(path, \"cnn-model.ckpt\"), global_step=epoch)\n",
    "\n",
    "    def load(self, path, epoch):\n",
    "        print(\"Loading model from {}\".format(path))\n",
    "        saver.restore(self.sess, os.path.join(path, \"cnn-model.ckpt-{}\".format(epoch)))\n",
    "\n",
    "    def train(self, training_set, validation_set=None, initialize=True):\n",
    "        # initialize variables\n",
    "        if initialize:\n",
    "            self.sess.run(self.init_op)\n",
    "\n",
    "        self.train_cost_ = []\n",
    "        X_data = np.array(training_set[0])\n",
    "        y_data = np.array(training_set[1])\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            batch_gen = batch_generator(X_data, y_data, shuffle=self.shuffle)\n",
    "            avg_loss = 0.0\n",
    "            for i, (batch_x, batch_y) in enumerate(batch_gen):\n",
    "                feed = {\n",
    "                    \"tf_x:0\": batch_x,\n",
    "                    \"tf_y:0\": batch_y,\n",
    "                    \"is_train:0\": True,\n",
    "                }  # for dropout\n",
    "                loss, _ = self.sess.run(\n",
    "                    [\"cross_entropy_loss:0\", \"train_op\"], feed_dict=feed\n",
    "                )\n",
    "                avg_loss += loss\n",
    "\n",
    "            print(\n",
    "                \"Epoch {:2d} training average loss: {:.3f}\".format(epoch, avg_loss),\n",
    "                end=\" \",\n",
    "            )\n",
    "\n",
    "            if validation_set is not None:\n",
    "                feed = {\n",
    "                    \"tf_x:0\": validation_set[0],\n",
    "                    \"tf_y:0\": validation_set[1],\n",
    "                    \"is_train:0\": False,\n",
    "                }  # for dropout\n",
    "                valid_acc = self.sess.run(\"accuracy:0\", feed_dict=feed)\n",
    "                print(\"Validation accuracy: {:.3f}\".format(valid_acc))\n",
    "            else:\n",
    "                print()\n",
    "\n",
    "    def predict(self, X_test, return_proba=False):\n",
    "        feed = {\"tf_x:0\": X_test, \"is_train:0\": 1.0}  # dropout\n",
    "        if return_proba:\n",
    "            return self.sess.run(\"probabilities:0\", feed_dict=feed)\n",
    "        else:\n",
    "            return self.sess.run(\"labels:0\", feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T03:18:18.186359Z",
     "start_time": "2019-06-30T02:56:16.196083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-f726052c2677>:35: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-27-f726052c2677>:38: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-27-f726052c2677>:50: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-27-f726052c2677>:53: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "Epoch  1 training average loss: 224.376 Validation accuracy: 0.963\n",
      "Epoch  2 training average loss: 62.005 Validation accuracy: 0.980\n",
      "Epoch  3 training average loss: 41.396 Validation accuracy: 0.985\n",
      "Epoch  4 training average loss: 30.334 Validation accuracy: 0.986\n",
      "Epoch  5 training average loss: 26.541 Validation accuracy: 0.988\n",
      "Epoch  6 training average loss: 22.150 Validation accuracy: 0.989\n",
      "Epoch  7 training average loss: 18.923 Validation accuracy: 0.987\n",
      "Epoch  8 training average loss: 16.074 Validation accuracy: 0.988\n",
      "Epoch  9 training average loss: 13.766 Validation accuracy: 0.990\n",
      "Epoch 10 training average loss: 11.647 Validation accuracy: 0.989\n",
      "Epoch 11 training average loss: 10.130 Validation accuracy: 0.990\n",
      "Epoch 12 training average loss: 8.974 Validation accuracy: 0.991\n",
      "Epoch 13 training average loss: 7.964 Validation accuracy: 0.990\n",
      "Epoch 14 training average loss: 7.076 Validation accuracy: 0.991\n",
      "Epoch 15 training average loss: 6.212 Validation accuracy: 0.991\n",
      "Epoch 16 training average loss: 5.189 Validation accuracy: 0.992\n",
      "Epoch 17 training average loss: 5.127 Validation accuracy: 0.990\n",
      "Epoch 18 training average loss: 4.657 Validation accuracy: 0.990\n",
      "Epoch 19 training average loss: 3.759 Validation accuracy: 0.992\n",
      "Epoch 20 training average loss: 3.929 Validation accuracy: 0.991\n",
      "Saving model in ./ch15_files/layers\n"
     ]
    }
   ],
   "source": [
    "# create instance of ConvNN and train for 20 epochs\n",
    "cnn = ConvNN(random_seed=123)\n",
    "\n",
    "cnn.train(\n",
    "    training_set=(X_trainCent, y_train),\n",
    "    validation_set=(X_validCent, y_valid),\n",
    "    initialize=True,\n",
    ")\n",
    "cnn.save(epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T03:20:03.025304Z",
     "start_time": "2019-06-30T03:20:01.274649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./ch15_files/layers\n",
      "INFO:tensorflow:Restoring parameters from ./ch15_files/layers/cnn-model.ckpt-20\n",
      "[9 9 6 3 7 6 7 9 6 2]\n"
     ]
    }
   ],
   "source": [
    "# use trained model to perform predictions\n",
    "# del cnn\n",
    "cnn2 = ConvNN(random_seed=123)\n",
    "cnn2.load(epoch=20, path=\"./ch15_files/layers\")\n",
    "\n",
    "print(cnn2.predict(X_testCent[:10, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T03:20:06.280253Z",
     "start_time": "2019-06-30T03:20:04.202243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.98475\n"
     ]
    }
   ],
   "source": [
    "# measure accuracy on test set\n",
    "preds = cnn2.predict(X_testCent)\n",
    "print(\"Test accuracy: {}\".format(np.sum(y_test == preds) / len(y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
