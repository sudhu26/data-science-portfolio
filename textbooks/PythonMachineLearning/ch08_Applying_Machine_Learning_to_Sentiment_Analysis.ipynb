{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 8 - Applying Machine Learning to Sentiment Analysis__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Preparing the IMDb movie review data for text processing](#Preparing-the-IMDb-movie-review-data-for-text-processing)\n",
    "1. [Bag-of-words](#Bag-of-words)\n",
    "1. [Transforming words into feature vectors](#Transforming-words-into-feature-vectors)\n",
    "    1. [Assessing word relevancy via term frequency-inverse document frequency](#Assessing-word-relevancy-via-term-frequency-inverse-document-frequency)\n",
    "        1. [Manually calculate a word](#Manually-calculate-a-word)\n",
    "1. [Cleaning text data](#Cleaning-text-data)\n",
    "1. [Processing documents](#Processing-documents)\n",
    "1. [Training a logistic regression model for document classification](#Training-a-logistic-regression-model-for-document-classification)\n",
    "1. [Working with bigger data – online algorithms and out-of-core learning](#Working-with-bigger-data–online-algorithms-and-out-of-core-learning)\n",
    "    1. [Store learned model using pickle](#Store-learned-model-using-pickle)\n",
    "1. [Topic modeling with Latent Dirichlet Allocation](#Topic-modeling-with-Latent-Dirichlet-Allocation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:17:59.976270Z",
     "start_time": "2019-07-07T05:17:58.789717Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from io import StringIO\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.utils as utils\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom extensions and settings\n",
    "sys.path.append(\"/home/mlmachine\") if \"/home/mlmachine\" not in sys.path else None\n",
    "sys.path.append(\"/home/prettierplot\") if \"/home/prettierplot\" not in sys.path else None\n",
    "\n",
    "import mlmachine as mlm\n",
    "from prettierplot.plotter import PrettierPlot\n",
    "import prettierplot.style as style\n",
    "\n",
    "# magic functions\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the IMDb movie review data for text processing\n",
    "\n",
    "Sentiment analysis is a subdiscipline of NLP that is concerned with analyzing the polarity of documents. One particular task seeks to classify documents based on the expressed emotions of the authors regarding a topic.\n",
    "\n",
    "IMDb movies reviews have been gathered into a dataset consistening of 50,000 individual user critiques. Each review is labeled as positive or negative, where postitive means the movie received > 6 stars and negative means the movie received < 5 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Preparing-the-IMDb-movie-review-data-for-text-processing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:18:00.333596Z",
     "start_time": "2019-07-07T05:17:59.979482Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: Only need to run this to unpack the file\n",
    "# unzip tarfile, read into dataframe, and send to .csv\n",
    "import tarfile\n",
    "import pyprind\n",
    "\n",
    "# unzip tarfile\n",
    "with tarfile.open(\"aclImdb_v1.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "# read into dataframe\n",
    "basepath = \"/aclImdb\"\n",
    "labels = {\"pos\": 1, \"neg\": 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in (\"test\", \"train\"):\n",
    "    for l in (\"pos\", \"neg\"):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = [\"review\", \"sentiment\"]\n",
    "\n",
    "# send to .csv\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv(\"../../data/ImdbReviews.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words\n",
    "\n",
    "Bag-of-words is a method for represented text data in numerical feature vectors. This involves two key steps:\n",
    "\n",
    "1. Create a vocabulary of unique token (for example, words) from entire set of documents\n",
    "2. Construct a feature vector from each document that contains the counts of how often each word occurs in that specific document. These individual features vectors are typically very sparse because a single document will contains a small subset of the overall corpus vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Bag-of-words'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming words into feature vectors\n",
    "\n",
    "A set of text can be transformed into a numerical representation. CountVectorizer() is a tool that creates our bag-of-words, and this data can be reviewed in several different ways. \n",
    "\n",
    "The vocabulary of the data shows all of the unique words in the data set, along with the number of times each word appears in all documents. The bag of words is stored as a sparse matrix, and this can be converted to an array, which shows the raw term frequencies:\n",
    "\n",
    "$$\n",
    "tf(t,d)\n",
    "$$\n",
    "\n",
    "where the term frequency $tf$ is the number of times term $t$ occurs in document $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Transforming-words-into-feature-vectors'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:18:58.958292Z",
     "start_time": "2019-07-07T05:18:58.847586Z"
    }
   },
   "outputs": [],
   "source": [
    "# CountVectorizer() example\n",
    "count = feature_extraction.text.CountVectorizer()\n",
    "docs = np.array(\n",
    "    [\n",
    "        \"The sun is shining\",\n",
    "        \"The weather is sweet\",\n",
    "        \"The sun is shining, the weather is sweet, and one and one is two\",\n",
    "    ]\n",
    ")\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:18:58.965874Z",
     "start_time": "2019-07-07T05:18:58.961773Z"
    }
   },
   "outputs": [],
   "source": [
    "# print number count\n",
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:18:58.974589Z",
     "start_time": "2019-07-07T05:18:58.969465Z"
    }
   },
   "outputs": [],
   "source": [
    "# print raw term frequencies\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:18:58.996632Z",
     "start_time": "2019-07-07T05:18:58.978127Z"
    }
   },
   "outputs": [],
   "source": [
    "# store in data frame\n",
    "pd.DataFrame(bag.toarray(), columns=count.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing word relevancy via term frequency-inverse document frequency\n",
    "\n",
    "Words often occur across multiple documents in each class, and these typically dont contain useful information due to their pervasiveness. term frequency-inverse document frequency (TF-IDF) is a technique for downweighting frequently occuring words:\n",
    "\n",
    "$$\n",
    "\\mbox{tf-idf(t,d)} = tf(t,d) \\times (idf(t,d) + 1)\n",
    "$$\n",
    "\n",
    "$tf(t,d)$ is the term frequency described above, and $idf(t,d)$ is the inverse document frequency, calculated as follows:\n",
    "\n",
    "$$\n",
    "\\mbox{idf(t,d)} = log \\frac{1 + n_d}{1 + \\mbox{df(d,t)}}\n",
    "$$\n",
    "\n",
    "$n_d$ is the total number of documents, and $df(d,t)$ is the numnber of documents $d$ that contain term $t$. Taking the log of this ensures that low document frequencies are not given too much weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Assessing-word-relevancy-via-term-frequency-inverse-document-frequency'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:18:59.088649Z",
     "start_time": "2019-07-07T05:18:58.999744Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform TF-IDF transformation\n",
    "tfidf = feature_extraction.text.TfidfTransformer(\n",
    "    use_idf=True, norm=\"l2\", smooth_idf=True\n",
    ")\n",
    "bag = tfidf.fit_transform(count.fit_transform(docs))\n",
    "\n",
    "pd.DataFrame(bag.toarray(), columns=count.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually calculate a word\n",
    "\n",
    "The word 'is' has a term frequency in the third documents of 3 $(tf =3)$ and the document frequency is also 3 because it occurs in all three documents $(df = 3)$\n",
    "$$\n",
    "\\mbox{tf(\"is\", 3)} = 3\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mbox{idf(\"is\", 3)} = log \\frac{1 + 3}{1 + 3} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mbox{tf-idf(\"is\", 3)} = 3 \\times (0 + 1) = 3\n",
    "$$\n",
    "\n",
    "Repeating this for each word in document three gives $[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69, 1.29]$, which is clearly not equal to the values in the third row of the TF-IDF dataframe above. To get these values, L2-normalization needs to be applied:\n",
    "\n",
    "$$\n",
    "\\mbox{tf-idf(d3)}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69, 1.29]}{\\sqrt{[3.39^2 + 3.0^2 + 3.39^2 + 1.29^2 + 1.29^2 + 1.29^2 + 2.0^2 + 1.69^2 + 1.29]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]\n",
    "$$\n",
    "\n",
    "The second value 0.45 correspond to the the word \"is\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Manually-calculate-a-word'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Cleaning-text-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:19:00.335917Z",
     "start_time": "2019-07-07T05:18:59.091954Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and inspect data\n",
    "df = pd.read_csv(\"../../data/ImdbReviews.csv\")\n",
    "\n",
    "df.info()\n",
    "display(df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:19:00.363668Z",
     "start_time": "2019-07-07T05:19:00.343248Z"
    }
   },
   "outputs": [],
   "source": [
    "# last 50 chars from first doc\n",
    "df.loc[0, \"review\"][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - Based on this snippet, it's clear that some of the reviews contain HTML markup, punctuation and other non-letters. Punctuation may contain some useful information, but in this example all will be removed for simplicity. We will however leave it emoticons since those convey sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:19:00.459389Z",
     "start_time": "2019-07-07T05:19:00.375476Z"
    }
   },
   "outputs": [],
   "source": [
    "# regex parser that moves all emoticons to the end\n",
    "import re\n",
    "\n",
    "\n",
    "def textPreprocessor(text):\n",
    "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
    "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text.lower())\n",
    "    text = re.sub(\"[\\W]+\", \" \", text.lower()) + \" \".join(emoticons).replace(\"-\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - <[^>]*> removes the HTML markup. The remaing code removes all non-word characters and temporarily stores all emoticons. The emoticons are added to the end of the text string, and we also replace the nose character '-' from the emoticons to create consistency in the emoticons used. Adding the emoticons to end is sufficient because in this eample we will be using 1-gram tokens, therefore the order of words in the bag of words is not important. Lastly, all text is converted to lowercase, which is done for simplicity. In practice, capitalization for things such as proper nouns may carry some importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:19:00.567278Z",
     "start_time": "2019-07-07T05:19:00.468554Z"
    }
   },
   "outputs": [],
   "source": [
    "# test textProcessor\n",
    "textPreprocessor(\"</a>This :) is :( :-( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:19:06.740788Z",
     "start_time": "2019-07-07T05:19:00.623681Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply text processor to reviews\n",
    "df[\"review\"] = df[\"review\"].apply(textPreprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing documents \n",
    "\n",
    "__Tokenization__\n",
    "\n",
    "Tokenizing a document means splitting the text into individual elements by splitting the words from each other and removing white space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Processing-documents'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:19:06.752123Z",
     "start_time": "2019-07-07T05:19:06.744810Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize sample sentence\n",
    "sentence = \"runners like running and thus they run\"\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stemming__\n",
    "\n",
    "Another useful technique in the context of tokenization is word stemming, which is a process for transforming a word into its root form. There are many stemming algorithms, and the original is known as the Porter stemmer algorithm. NLTK includes a Python implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:19:08.696054Z",
     "start_time": "2019-07-07T05:19:06.755417Z"
    }
   },
   "outputs": [],
   "source": [
    "# stemming example\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenizerPorter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "\n",
    "tokenizerPorter(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stop-words__\n",
    "\n",
    "Stop-words are those that are very common, such as is, and, has and like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:19:10.599232Z",
     "start_time": "2019-07-07T05:19:08.699936Z"
    }
   },
   "outputs": [],
   "source": [
    "# stop word removal\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "[w for w in tokenizerPorter(sentence) if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a logistic regression model for document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-a-logistic-regression-model-for-document-classification'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T05:19:10.609046Z",
     "start_time": "2019-07-07T05:19:10.601704Z"
    }
   },
   "outputs": [],
   "source": [
    "# manual train/test split\n",
    "XTrain = df.loc[:25000, \"review\"].values\n",
    "yTrain = df.loc[:25000, \"sentiment\"].values\n",
    "XTest = df.loc[25000:, \"review\"].values\n",
    "yTest = df.loc[25000:, \"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.587797Z",
     "start_time": "2019-07-07T05:19:10.611487Z"
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDF grid search / logist regression pipeline\n",
    "tfidf = feature_extraction.text.TfidfVectorizer(\n",
    "    strip_accents=None, lowercase=False, preprocessor=None\n",
    ")\n",
    "paramGrid = [\n",
    "    {\n",
    "        \"vect__ngram_range\": [(1, 1)],\n",
    "        \"vect__stop_words\": [stop, None],\n",
    "        \"vect__tokenizer\": [tokenizer, tokenizerPorter],\n",
    "        \"logReg__penalty\": [\"l1\", \"l2\"],\n",
    "        \"logReg__C\": [1.0, 10.0, 100.0],\n",
    "    },\n",
    "    {\n",
    "        \"vect__ngram_range\": [(1, 1)],\n",
    "        \"vect__stop_words\": [stop, None],\n",
    "        \"vect__tokenizer\": [tokenizer, tokenizerPorter],\n",
    "        \"vect__use_idf\": [False],\n",
    "        \"vect__norm\": [None],\n",
    "        \"logReg__penalty\": [\"l1\", \"l2\"],\n",
    "        \"logReg__C\": [1.0, 10.0, 100.0],\n",
    "    },\n",
    "]\n",
    "logRegTfidf = pipeline.Pipeline(\n",
    "    [(\"vect\", tfidf), (\"logReg\", linear_model.LogisticRegression(random_state=0))]\n",
    ")\n",
    "gs = model_selection.GridSearchCV(\n",
    "    estimator=logRegTfidf,\n",
    "    param_grid=paramGrid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=1,\n",
    ")\n",
    "gs.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The first dictionary in paramGrid uses the default values for use_idf and norm, and the second dictionary forces the model to train based on the raw term frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.591825Z",
     "start_time": "2019-07-07T05:18:58.899Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate grid search CV results\n",
    "print(\"Best parameter set: {0}\".format(gs.best_params_))\n",
    "print(\"CV accuracy: {:.3f}\".format(gs.best_score_))\n",
    "print(\"Test accuracy: {:.3f}\".format(gs.score(XTest, yTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with bigger data – online algorithms and out-of-core learning\n",
    "\n",
    "The example above used only 50,000 reviews, and in real world applications the dataset can be much larger. A technique called out-of-core learning allows us to work with larger data sets, which can be helpful if we don't have access to advanced computing systems. sklearn's implementation of stochastic gradient descent SGDClassifier has a partial_fit function which enables streaming of subsets of documents directly from a local drive in order to train a model on mini-batches of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Working-with-bigger-data–online-algorithms-and-out-of-core-learning'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.595058Z",
     "start_time": "2019-07-07T05:18:58.902Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean up text and remove stop words\n",
    "def textProcessor(text):\n",
    "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
    "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text.lower())\n",
    "    text = re.sub(\"[\\W]+\", \" \", text.lower()) + \" \".join(emoticons).replace(\"-\", \"\")\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# generator function to read in and return on document at a time\n",
    "def streamDocs(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            # Slicing is very specific to how docs are stored\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "\n",
    "\n",
    "# return a particular number of documents from streamDocs\n",
    "def getMiniBatch(streamDocs, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(streamDocs)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - Neither CountVectorizer nor TfidfVectorizer can be used because each requires having the entire vocabulary in memory, which we won't have because we are implementating a mini-batch approach. Instead, sklearn has a process called HashingVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.597598Z",
     "start_time": "2019-07-07T05:18:58.906Z"
    }
   },
   "outputs": [],
   "source": [
    "# iterate over 45 batches of 1,000 documents\n",
    "vect = feature_extraction.text.HashingVectorizer(\n",
    "    decode_error=\"ignore\",\n",
    "    n_features=2 ** 21,\n",
    "    preprocessor=None,\n",
    "    tokenizer=textProcessor,\n",
    ")\n",
    "clf = linear_model.SGDClassifier(loss=\"log\", random_state=1, max_iter=1)\n",
    "docStream = streamDocs(path=\"../../data/ImdbReviews.csv\")\n",
    "\n",
    "import pyprind\n",
    "\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    XTrain, yTrain = getMiniBatch(docStream, size=1000)\n",
    "    if not XTrain:\n",
    "        break\n",
    "    XTrain = vect.transform(XTrain)\n",
    "    clf.partial_fit(XTrain, yTrain, classes=classes)\n",
    "    pbar.update()\n",
    "    print(\"Iteration accuracy: {0}\".format(clf.score(XTrain, yTrain)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - choosing a large number of features in HashingVectorizer prevents hash collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.600724Z",
     "start_time": "2019-07-07T05:18:58.910Z"
    }
   },
   "outputs": [],
   "source": [
    "# test model on remaining 5,000 documents\n",
    "XTest, yTest = getMiniBatch(docStream, size=5000)\n",
    "XTest = vect.transform(XTest)\n",
    "print(\"Test set accuracy: {0}\".format(clf.score(XTest, yTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test setaccuracy of the model trained in one batch is 0.899, and the accuracy of the model trained in mini batches is slightly lower at 0.867. The mini model trained in less than a minute, whereas the full batch model took over six hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.602848Z",
     "start_time": "2019-07-07T05:18:58.914Z"
    }
   },
   "outputs": [],
   "source": [
    "# update model one more time with 5,000 documents used as test set\n",
    "clf = clf.partial_fit(XTest, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store learned model using pickle\n",
    "\n",
    "Training a model can take awhile, and we lose it when the Python interpreter closes. Since we don't want to train a model every time we want to use it, we can use the pickle module to save the learned model. Pickle enables us to serialize and deserialize Python objects to compact bytecode so that we can save our classifier in its current state and then reload it later, even after the interpreter has been closed. With this pickle file in hand, we can classify new samples without needing the model to learn from the training data from scratch again. This will be used in Chapter 9 to build a Flask app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Store learned model using pickle'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.605308Z",
     "start_time": "2019-07-07T05:18:58.917Z"
    }
   },
   "outputs": [],
   "source": [
    "# use pickle to store model objects\n",
    "import pickle\n",
    "\n",
    "dest = os.path.join(\"movieClassifier\", \"pkl_objects\")\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "pickle.dump(stop, open(os.path.join(dest, \"stopwords.pkl\"), \"wb\"), protocol=4)\n",
    "pickle.dump(clf, open(os.path.join(dest, \"classifier.pkl\"), \"wb\"), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The dump method's first argument is the object we want to pickle, the second argument is name of the file we'll create in binary node per 'wb', and protocol specifies the latest/most efficient pickle protocol. We saved both the model and the stop words so that the NLTK stop word set doesn't have to be installed on the server where we will eventually deploy the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling with Latent Dirichlet Allocation\n",
    "\n",
    "Topic modeling is the task of assigned topics to unlabelled documents, which can be thought of as a form of unsupervised learning. For example, assigning newspaper article to a category when we don't know the specific theme would be topic modeling.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA, which has no relationship to linear discriminant analysis) is a popular technique for topic modeling. It is a generatie probabilistic model that attempts to fid groups of words that appear together across different documents. The frequently appearing words represent topics, under the assumption that each document is a mixture of different words. The input is a bag-of-words model. The output are two new matrices - a document to topic matrix and a word to topic matrix. LDA decomposes the bag-of-words input matrix in a way where multiplying the two output matrices together would reproduce the input matrix with the lowext possible error. The downside is that we need to define the number of topics beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Topic-modeling-with-Latent-Dirichlet-Allocation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.607388Z",
     "start_time": "2019-07-07T05:18:58.922Z"
    }
   },
   "outputs": [],
   "source": [
    "# create bog-of-words for LDA\n",
    "# use max_df parameter to exclude words that appear in >10% of the docs\n",
    "# use max_features to consider only the 5,000 most frequently occurring words\n",
    "count = feature_extraction.text.CountVectorizer(\n",
    "    stop_words=\"english\", max_df=0.1, max_features=5000\n",
    ")\n",
    "X = count.fit_transform(df[\"review\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.609653Z",
     "start_time": "2019-07-07T05:18:58.926Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform LDA on movie reviews\n",
    "lda = decomposition.LatentDirichletAllocation(\n",
    "    n_components=10, random_state=123, learning_method=\"batch\"\n",
    ")\n",
    "XTopics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The 'batch' learning method has the LDA estimator do its estimation on all training data which is slower than the alternative 'online', which is effectively does the same thing that the out-of-core workflow does above.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.611221Z",
     "start_time": "2019-07-07T05:18:58.930Z"
    }
   },
   "outputs": [],
   "source": [
    "# the LDA components_ attribute stores a matrix containing the word importance for the 10 topics\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.613144Z",
     "start_time": "2019-07-07T05:18:58.933Z"
    }
   },
   "outputs": [],
   "source": [
    "# print the top 5 most important words for each topic\n",
    "nTopWords = 5\n",
    "featureNames = count.get_feature_names()\n",
    "for topicIx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic {}: \".format(topicIx + 1))\n",
    "    print(\" \".join([featureNames[i] for i in topic.argsort()[: -nTopWords - 1 : -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - A few topics stand out as having a strong theme. Topic 10 seems to be about war movies, topic 8 - musicals, topic 4 - family movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T15:01:33.614738Z",
     "start_time": "2019-07-07T05:18:58.937Z"
    }
   },
   "outputs": [],
   "source": [
    "# print a few reviews for a category to evaluate theme\n",
    "war = XTopics[:, 9].argsort()[::-1]\n",
    "for iterIx, movieIx in enumerate(war[:3]):\n",
    "    print(\"\\nFamily movie review {0}: \".format(iterIx + 1))\n",
    "    print(df[\"review\"][movieIx][:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T14:26:20.593347Z",
     "start_time": "2018-11-14T14:26:19.884500Z"
    }
   },
   "source": [
    "> Remarks - The three reviews above mention things like 'civil war', 'cavalry', 'history'. These seems to be inline with the topic of war, battles and historical conflict."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
