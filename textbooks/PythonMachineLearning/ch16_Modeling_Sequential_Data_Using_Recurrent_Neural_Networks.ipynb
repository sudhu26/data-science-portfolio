{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 16 - Modeling Sequential Data Using Recurrent Neural Networks__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Introducing sequential data](#Introducing-sequential-data)\n",
    "    1. [Modeling sequential data – order matters](#Modeling-sequential-data–order-matters)\n",
    "    1. [Representing sequences](#Representing-sequences)\n",
    "    1. [The different categories of sequence modeling](#The-different-categories-of-sequence-modeling)\n",
    "1. [RNNs for modeling sequences](#RNNs-for-modeling-sequences)\n",
    "    1. [Understanding the structure and flow of an RNN](#Understanding-the-structure-and-flow-of-an-RNN)\n",
    "    1. [Computing activations in an RNN](#Computing-activations-in-an-RNN)\n",
    "    1. [The challenges of learning long-range interactions](#The-challenges-of-learning-long-range-interactions)\n",
    "1. [Implementing a multilayer RNN for sequence modeling in TensorFlow](#Implementing-a-multilayer-RNN-for-sequence-modeling-in-TensorFlow)\n",
    "    1. [Project one - performing sentiment analysis of IMDb movie reviews using multilayer RNNs](#performing-sentiment-analysis-of-IMDb-movie-reviews-using-multilayer-RNNs)\n",
    "        1. [Preparing the data](#Preparing-the-data)\n",
    "        1. [Embedding](#Embedding)\n",
    "        1. [Building an RNN model](#Building-an-RNN-model)\n",
    "            1. [The build method](#The-build-method)\n",
    "            1. [The train method](#The-train-method)\n",
    "            1. [The predict method](#The-predict-method)            \n",
    "        1. [Instantiating the SentimentRNN class](#Instantiating-the-SentimentRNN-class)\n",
    "        1. [Training and optimizing the sentiment analysis RNN model](#Training-and-optimizing-the-sentiment-analysis-RNN-model)\n",
    "    1. [Project two – implementing an RNN for character-level language modeling in TensorFlow](#Project-two–implementing-an-RNN-for-character-level-language-modeling-in-TensorFlow)\n",
    "        1. [Preparing the data](#Preparing-the-data2)\n",
    "        1. [Building a character-level RNN model](#Building-a-character-level-RNN-model)\n",
    "            1. [The constructor](#The-constructor)\n",
    "            1. [The build method](#The-build-method2)\n",
    "            1. [The train method](#The-train-method2)\n",
    "            1. [The sample method](#The-sample-method)\n",
    "        1. [Creating and training the CharRNN Model](#Creating-and-training-the-CharRNN-Model)\n",
    "        1. [The CharRNN model in the sampling mode](#The-CharRNN-model-in-the-sampling-mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:32:15.615662Z",
     "start_time": "2019-06-30T02:32:13.451176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from io import StringIO\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.utils as utils\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom extensions and settings\n",
    "sys.path.append(\"/home/mlmachine\") if \"/home/mlmachine\" not in sys.path else None\n",
    "sys.path.append(\"/home/prettierplot\") if \"/home/prettierplot\" not in sys.path else None\n",
    "\n",
    "import mlmachine as mlm\n",
    "from prettierplot.plotter import PrettierPlot\n",
    "import prettierplot.style as style\n",
    "\n",
    "# magic functions\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing sequential data\n",
    "\n",
    "This chapter explores the unique properties of sequences compared to other kinds of data. We will explore how we can represent sequential data and the various models for analyzing sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Introducing-sequential-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling sequential data – order matters\n",
    "\n",
    "One major unique aspect of sequential data is that the elements appear in a certain order and are not independent of each other. The contrasts with data and algorithms that we have dealth with up to this point, in that previous models assume that the data is independent and identically distributed (IID). But with sequential data, by definition, order matters. This is not necessarily a problem, and in fact, the order can yield meaningful information. We just need a different approach and different tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling-sequential-data–order-matters'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing sequences\n",
    "\n",
    "In this chaper, sequences will be represented as $\\big(x^1, x^2,...,x^T\\big)$, where the superscript indices indicate the order of the instances, and the length of the sequence is $T$. For example, in time-series data, sample $\\textbf{x}^T$ belongs to a particular time $t$. Further, if the data is labeled, the labels also follow a form where order matters: $\\big(y^1, y^2,...,y^T\\big)$.\n",
    "\n",
    "The MLP and CNN models built in the last few chapters are not capable of handling the order of the input simples. Recurrent neural networks (RNNs) are designed to model sequences that remember past information and process new events in light of that history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Representing sequences'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The different categories of sequence modeling\n",
    "\n",
    "Sequence modeling can be applied to, among other things, language translateion, image cpationing, and text generation. Sequential data comes in many forms, and the nature of the input and output data determines the type. If neither the input nor the output data is sequenced, then this is simply a standard dataset, any of the methods covered in previous chapter may be used (depending on the problem).\n",
    "\n",
    "If either the input or output data is sequenced, then it can be identified by one of these three categories:\n",
    "\n",
    "- Many-to-one: The input data is sequenced, but the output is a vector of a fixed size, not a sequence. For example, sentiment analysis takes text data as an input and outputs a class label.\n",
    "- One-to-many: The input data is in a standard format (not sequenced) but the output is a seuqnce. For example, in image captioning, the input is an image and the ouput is an English phrase.\n",
    "- Many-to-many: Both the input and output arrays are sequences. This category can be sub-divided into subcategories based on whether the input or output is synchronized or not. An example of synchronized many-to-many is video classification, where each from in a video is labeled. An example of delayed many-to-many is language translation, i.e. an English sentence is translated by a machine into its equivalent in German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-different-categories-of-sequence-modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs for modeling sequences\n",
    "\n",
    "This sections describes the foundations of RNNs, including typical structure, dat flow, neuron activation, and typical challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'RNNs-for-modeling-sequences'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the structure and flow of an RNN\n",
    "\n",
    "In a feedforward network, information flows from the input layer, to the hidden layer(s), then to the output layer. In an RNN, the hidden layer gets its input from both the input layer and the hidden layer from the previous time step. This flow of information in adjacent time steps in the hidden layer allows the network to use its 'memory of past events'. This can be envisioned as a loop, which in graph notation is referred to as a recurrent edge. This can be visualized as:\n",
    "\n",
    "$$\n",
    "\\textbf{x}^t \\rightarrow \\textbf{h}^t \\rightarrow \\textbf{y}^t \n",
    "$$\n",
    "\n",
    "where $\\textbf{x}^t$ is the input data at the $t$ point in the sequence, $\\textbf{h}^t$ is the hidden layer at point $t$, and $\\textbf{y}^t$ at point $t$. This can be unfolded to reveal how other data points observed at adjacent time steps are structured relative to point $t$:\n",
    "\n",
    "$$\n",
    "\\textbf{x}^{t-1} \\rightarrow \\textbf{h}^{t-1} \\rightarrow \\textbf{y}^{t-1}\n",
    "\\\\\n",
    "\\downarrow\n",
    "\\\\\n",
    "\\textbf{x}^t \\rightarrow \\textbf{h}^t \\rightarrow \\textbf{y}^t \n",
    "\\\\\n",
    "\\downarrow\n",
    "\\\\\n",
    "    \\textbf{x}^{t+1} \\rightarrow \\textbf{h}^{t+1} \\rightarrow \\textbf{y}^{t+1}\n",
    "$$\n",
    "\n",
    "RNNs can have multiple hidden layers as well.\n",
    "\n",
    "In a standard neural network, each hidden unit only receives one input - the net input associated with the input layer. RNNs, conversely, neurons in the hidden layer receive two distinct inputs - the net input from the input layer and the net input of the same hidden layer neuron from the previous time step $t-1$. At $t=0$, the first time step, the hidden units are initialized to zeros, or small random numbers. Then for $t>0$, the hidden units get input from the data point at the current time $\\textbf{x}^t$ and the previous values of the hidden units at $t-1$, $\\textbf{h}^{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Understanding-the-structure-and-flow-of-an-RNN'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing activations in an RNN\n",
    "\n",
    "Each directed edge (connection between boxes) of an RNN is associated with a weight matrix, and these weights do not depend on time $t$. These weights are shared across the time axis. The different weight matrices in a single layer RNN are:\n",
    "\n",
    "$$\n",
    "\\textbf{W}_{xh}: \\mbox{the weight matrix between the input} \\ \\textbf{x}^t \\mbox{and the hidden layer } \\ \\textbf{h}\n",
    "\\\\\n",
    "\\textbf{W}_{hh}: \\mbox{the weight matrix associated with the recurrent edge}\n",
    "\\\\\n",
    "\\textbf{W}_{hy}: \\mbox{the weight matrix between the hidden layer and the output layer}\n",
    "$$\n",
    "\n",
    "Again, these weight matrices apply to the current point in the sequence $t$, as well as to $t-1$ and $t+1$.\n",
    "\n",
    "The activations are computed similar to how this is handled in feed forward networks. For example, in the hidden layer, the net input $\\textbf{z}_h$ is computed through a linear combination determined by summing the multiplications of the weight matrices with the corresponding vectors, and adding the bias unit:\n",
    "\n",
    "$$\n",
    "\\textbf{z}_h^t = \\textbf{W}_{xh}\\textbf{x}^t + \\textbf{W}_{hh}\\textbf{h}^{t-1} + \\textbf{b}_h\n",
    "$$\n",
    "\n",
    "Then the activations of the hidden units at the time step $t$ are calculated using:\n",
    "\n",
    "$$\n",
    "\\textbf{h}^t = \\phi_h\\big(\\textbf{z}_h^t\\big) = \\phi_h\\big(\\textbf{W}_{xh}\\textbf{x}^t + \\textbf{W}_{hh}\\textbf{h}^{t-1} + \\textbf{b}_h\\big)\n",
    "$$\n",
    "\n",
    "where $\\phi_h(\\cdot)$ is the activation function.\n",
    "\n",
    "Once the activations of the hidden units at the current time step are calculated, the activations of the output units are calculated by:\n",
    "$$\n",
    "\\textbf{y}^t = \\phi_y\\big(\\textbf{W}_{hy}\\textbf{h}^t + \\textbf{b}_y\\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Computing-activations-in-an-RNN'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenges of learning long-range interactions\n",
    "\n",
    "Backpropagation through time (BPTT) is the process for optimizing the weights in an RNN. The basic idea is that the overall loss $L$ is the sum of all loss functions calculated at times $t$ = 1 to $t$ = $T$. \n",
    "\n",
    "$$\n",
    "L = \\sum^T_{t=1}L^t\n",
    "$$\n",
    "\n",
    "The loss at time 1:$t$ is dependent on the hidden units at all time steps that were evaluated before 1:$t$, so the gradient is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L^t}{\\partial\\textbf{W}_{hh}} = \\frac{\\partial L^t}{\\partial\\textbf{y}^{t}} \\times \\frac{\\partial \\textbf{y}^t}{\\partial\\textbf{h}^{t}} \\times \\Bigg(\\sum^t_{k=1}\\frac{\\partial \\textbf{h}^t}{\\partial\\textbf{h}^{k}} \\times \\frac{\\partial \\textbf{h}^k}{\\partial\\textbf{h}_{hh}}\\Bigg)\n",
    "$$\n",
    "\n",
    "In this formula $\\frac{\\partial \\textbf{h}^t}{\\partial\\textbf{h}^{k}}$ is computed as multiplication of adjacent time steps:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\textbf{h}^t}{\\partial\\textbf{h}^{k}} = \\prod^t_{i=k+1}\\frac{\\partial \\textbf{h}^i}{\\partial\\textbf{h}^{i-1}}\n",
    "$$\n",
    "\n",
    "Calculation of the term $\\frac{\\partial \\textbf{h}^t}{\\partial\\textbf{h}^{k}}$ introduces a few challenges. Namely, the so-called vanishing/exploding gradient. This term has $t-k$ multiplications, so multiply the $w$ weight a total of $t - k$ times results in a factor $w^{t-k}$. As a result, if $\\lvert w\\rvert$ < 1, this factor becomes very small when $t-k$ is large. On the other hand, if $\\lvert w\\rvert$ > 1, then $w^{t-k}$ becomes very large when $t-k$ is large. This means that we prefer $w$ to be equal to 1.\n",
    "\n",
    "There are two solutions to this problem:\n",
    "\n",
    "- Truncated backpropagation through time (TBPTT): clips the gradients above a given threshold. This solves exploding gradient issues, but the truncation limits the number of steps the gradient can effectively flow back and update weights properly\n",
    "- Long short-term memory (LSTM): introduced to overcome the vanishing gradient problem. More successful in modeling long-range sequences than TBPTT. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-challenges-of-learning-long-range-interactions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a multilayer RNN for sequence modeling in TensorFlow\n",
    "\n",
    "The rest of this notebook will explore RNN implementations to address two common tasks, sentiment analysis and language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Implementing-a-multilayer-RNN-for-sequence-modeling-in-TensorFlow'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project one – performing sentiment analysis of IMDb movie reviews using multilayer RNNs\n",
    "\n",
    "In chapter 8, we implemented a model to determine the sentiment of movie reiews on IMDb. This project will leverage an RNN model to do the same task. This is an example of a many-to-one problem, where we are given a document of text and need to return a single label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'performing-sentiment-analysis-of-IMDb-movie-reviews-using-multilayer-RNNs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "This dataset contains two columns, one with the movie reviews, and another with the sentiment label of 0 or 1. The text component of these movie reviews are sequences of words, so we want to build an RNN to process the words in sequence and then classify the entire sequence to the 0 or 1 class.\n",
    "\n",
    "To make this dataset ready for the neural network, it needs to be encoded into numeric values. First, we need to find the unique words in the entire dataset. This is not the same as preparing a bag-of-words model, as we are only interested in the set of unique words, and we don't need the counts necessarily. Second we create a mapping by way of a dictionary where we pair each unique word with a unique integer number. This will convert the entire text into a list of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Preparing-the-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:33:14.233557Z",
     "start_time": "2019-06-30T02:33:13.033954Z"
    }
   },
   "outputs": [],
   "source": [
    "# load ImdbReviews dataset\n",
    "import pyprind\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"s3://tdp-ml-datasets/misc/ImdbReviews.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:33:17.949111Z",
     "start_time": "2019-06-30T02:33:17.912476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review sampels\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:42:24.938978Z",
     "start_time": "2019-06-30T02:33:20.525007Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:08:58\n",
      "Map review to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:06\n"
     ]
    }
   ],
   "source": [
    "# deparate the words and count each words occurrence\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df[\"review\"]), title=\"Counting words occurrences\")\n",
    "\n",
    "for i, review in enumerate(df[\"review\"]):\n",
    "    text = \"\".join(\n",
    "        [c if c not in punctuation else \" \" + c + \" \" for c in review]\n",
    "    ).lower()\n",
    "    df.loc[i, \"review\"] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "\n",
    "# create a mapping of each unique word to an integer\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df[\"review\"]), title=\"Map review to ints\")\n",
    "\n",
    "for review in df[\"review\"]:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process effectively conerted sequences of words into sequences of integers, but these sequences have different lengths. For this dataset to be ready for an RNN, the sequences need to hae the same length. To accomplish this, we define a paramter called sequence_length that will be set to 200. Sequences that hae fewer than 200 words will be left-padded with zeros, while sequences longer than 200 words will be trimmed so that only the last 200 values will be used. This preprocessing step is implemented in two steps:\n",
    "\n",
    "1. Create a matrix of zeros, where each row corresponds to a sequence of size 200\n",
    "2. Fill the index of words in each sequence for the right-hand side of the matrix. If a sequence has a length of only 150, then the first 50 elements of the row would stay zero. It's worth noting that the value chosen for sequence_length is a hyperparameter than can be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:45:49.669655Z",
     "start_time": "2019-06-30T02:45:47.832318Z"
    }
   },
   "outputs": [],
   "source": [
    "# create matrix of same-length sequences\n",
    "sequence_length = 200\n",
    "\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row) :] = review_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:45:51.104668Z",
     "start_time": "2019-06-30T02:45:51.098321Z"
    }
   },
   "outputs": [],
   "source": [
    "# create training and test sets\n",
    "# note - the reviews were shuffled prior to being saved in a .csv\n",
    "\n",
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, \"sentiment\"].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, \"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T02:45:53.561108Z",
     "start_time": "2019-06-30T02:45:53.553913Z"
    }
   },
   "outputs": [],
   "source": [
    "# generator helper function for mini-batching\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x) // batch_size\n",
    "    x = x[: n_batches * batch_size]\n",
    "    if y is not None:\n",
    "        y = y[: n_batches * batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii : ii + batch_size], y[ii : ii + batch_size]\n",
    "        else:\n",
    "            yield x[ii : ii + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "The data prep stage above created same-length sequences where the elements are integers that correspond to the indices of unique words. Now we need to convert this to input features. The wrong way to do it would be to apply on-ehot encoding to convert indices into ectors of zeros and ones. Each word would be mapped to a vector with a size equal to the number of unique words in the dataset. This is less than ideal, because the number of unique words can rise to the tens of thousands. A model trained on features like this may suffer from the curse of dimensionality. Further, these features would be very sparse, since all values are zero except one.\n",
    "\n",
    "A better approach would be to map each word to a vector of fixed size with real-valued elements (not integers necessarily). With this approach, we can instead use finite-sized vector to represent and infinite number of real number. This is the idea behind the embedding, which is a feature-learning technique that can be utilized to automatically learn the salient features in the data. Given the value of a parameter unique_words, we can choose the size of the embedding vectors to be much smaller than the number of unique words in the corpus. The advantages of emebedding over one-hot encoding for these problems are:\n",
    "\n",
    "1. A reduction in dimensionality decreases the effect of the curse of dimensionality\n",
    "2. The extraction of salient features since the embedding layer in a neural network is trainable\n",
    "\n",
    "To create an embedding layer, we feed in tf_x as the input layer, which is comprised of vocabulary indices. We create a matrix of size $[n\\_words \\times embedding\\_size]$ as a tensor variable with randomly initialized values between [-1,1]. then we use tf.nn.embedding_lookup to loo up the row in the embedding matrix associated with each element of tf_x:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Embedding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an RNN model\n",
    "\n",
    "The SentimentRNN class that we will create has the following methods:\n",
    "\n",
    "- A constructor to set the model parameters and create a computation graph.\n",
    "- A build method that declares three placeholder for input data, input labels, and the kee-probability for the dropout process in the hidden layer. It also creates an embedding layer and creates embedded representations as input.\n",
    "- A train method that creates a session that launches a graph, iterates through mini-batches of data, run for a # of epochs, minimizing the cost along the way before saing the model\n",
    "- A predict method that creates a new session with the model as of the latest checkpoint saved at the end of the training process, and carries out the predictions on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Building-an-RNN-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T04:11:50.423052Z",
     "start_time": "2019-06-30T04:11:50.398090Z"
    }
   },
   "outputs": [],
   "source": [
    "# RNN custom class\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SentimentRNN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_words,\n",
    "        seq_len=200,\n",
    "        lstm_size=256,\n",
    "        num_layers=1,\n",
    "        batch_size=64,\n",
    "        learning_rate=0.0001,\n",
    "        embed_size=200,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_words - must be set equal to the number of unique words (+1, since we use \n",
    "                    zero to fill sequences with a size less than 200) and it's used\n",
    "                    to create the embedding layer, along with embed_size\n",
    "        embed_size - used with n_words to create the embedding layer\n",
    "        seq_len- must be set according to the length of the sequences that were created\n",
    "                    in the preprocessing steps above\n",
    "        lstm_size - a hyperparameter that determines the number of hidden units in each RNN layer\n",
    "        \"\"\"\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size  # number of hidden units\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def build(self):\n",
    "        tf_x = tf.placeholder(\n",
    "            tf.int32, shape=(self.batch_size, self.seq_len), name=\"tf_x\"\n",
    "        )\n",
    "        tf_y = tf.placeholder(tf.float32, shape=(self.batch_size), name=\"tf_y\")\n",
    "        tf_keepprob = tf.placeholder(tf.float32, name=\"tf_keepprob\")\n",
    "\n",
    "        # create embedding layer\n",
    "        embedding = tf.Variable(\n",
    "            tf.random_uniform((self.n_words, self.embed_size), minval=-1, maxval=1),\n",
    "            name=\"embedding\",\n",
    "        )\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name=\"embeded_x\")\n",
    "\n",
    "        # define LSTM cell and stack together\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [\n",
    "                tf.contrib.rnn.DropoutWrapper(\n",
    "                    tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                    output_keep_prob=tf_keepprob,\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # define the initial state\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print(\"  << initial state >>  \", self.initial_state)\n",
    "\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "            cells, embed_x, initial_state=self.initial_state\n",
    "        )\n",
    "\n",
    "        # lstm output shape = [batch_size x max_time x cells.output_size]\n",
    "        print(\"\\n  << lstm_output >>\", lstm_outputs)\n",
    "        print(\"\\n  << final state >>\", self.final_state)\n",
    "\n",
    "        logits = tf.layers.dense(\n",
    "            inputs=lstm_outputs[:, -1], units=1, activation=None, name=\"logits\"\n",
    "        )\n",
    "        logits = tf.squeeze(logits, name=\"logits_squeezed\")\n",
    "        print(\"\\n  << logits    >>\", logits)\n",
    "\n",
    "        y_proba = tf.nn.sigmoid(logits, name=\"probabilities\")\n",
    "        predictions = {\n",
    "            \"probabilities\": y_proba,\n",
    "            \"labels\": tf.cast(tf.round(y_proba), tf.int32, name=\"labels\"),\n",
    "        }\n",
    "        print(\"\\n  << predictions >>\", predictions)\n",
    "\n",
    "        # define cost function\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, logits=logits),\n",
    "            name=\"cost\",\n",
    "        )\n",
    "\n",
    "        # define optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name=\"train_op\")\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "\n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                    X_train, y_train, self.batch_size\n",
    "                ):\n",
    "                    feed = {\n",
    "                        \"tf_x:0\": batch_x,\n",
    "                        \"tf_y:0\": batch_y,\n",
    "                        \"tf_keepprob:0\": 0.5,\n",
    "                        self.initial_state: state,\n",
    "                    }\n",
    "                    loss, _, state = sess.run(\n",
    "                        [\"cost:0\", \"train_op\", self.final_state], feed_dict=feed\n",
    "                    )\n",
    "\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\n",
    "                            \"Epoch {}/{} Iteration {} | Train loss: {:.5f}\".format(\n",
    "                                epoch + 1, num_epochs, iteration, loss\n",
    "                            )\n",
    "                        )\n",
    "                    iteration += 1\n",
    "                if (epoch + 1) % 1 == 0:\n",
    "                    path = \"./ch16_files/model\"\n",
    "                    if not os.path.isdir(path):\n",
    "                        os.makedirs(path)\n",
    "                    self.saver.save(\n",
    "                        sess, \"./ch16_files/model/sentiment-{}.ckpt\".format(epoch)\n",
    "                    )\n",
    "\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint(\"./ch16_files/model/\"))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(\n",
    "                create_batch_generator(X_data, None, batch_size=self.batch_size), 1\n",
    "            ):\n",
    "                feed = {\n",
    "                    \"tf_x:0\": batch_x,\n",
    "                    \"tf_keepprob:0\": 1.0,\n",
    "                    self.initial_state: test_state,\n",
    "                }\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(\n",
    "                        [\"probabilities:0\", self.final_state], feed_dict=feed\n",
    "                    )\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        [\"labels:0\", self.final_state], feed_dict=feed\n",
    "                    )\n",
    "                preds.append(pred)\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The build method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-build-method'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the build method, we create three placeholder for the input, output, and dropout keep-probability. Then we add the embedding layer, which builds the embedded representation of the unique words. Next within the build method, we built the RNN network. This was done in three steps.\n",
    "\n",
    "1. Define multilayer RNN cells\n",
    "2. Define initial state of these cells\n",
    "3. Create and RNN specified by the RNN cells in their initial states\n",
    "\n",
    "These three steps are unpacked in further detail below:\n",
    "\n",
    "__Step 1: Define multilayer RNN cells__\n",
    "\n",
    "The first step is to define the multilayer RNN cells, which was accomplished using a TensorFlow wrapper ckass to define the LSTM cells - BasicLSTMCell. These can be stacked together to form a multilayer RNN using the MultiRNNCell wrapper class. The process of stacking RNN cells with a dropout stage has three nested steps. Described from the inside out:\n",
    "\n",
    "1. Create RNN cells using tf.contrib.rnn.BasicLSTMCell\n",
    "2. Apply dropout to the RNN cells using tf.contrib.rnn.DropoutWrapper\n",
    "3. Make a list of such cells according to the desired number of RNN layer and pass this list to tf.contrib.rnn.MultiRNNCell\n",
    "\n",
    "This process is completed using a list comprehension in the implementation above.\n",
    "\n",
    "__Step 2: defining the initiatl states for the RNN cells__\n",
    "\n",
    "In the architecture of LSTM cells, there are three types of inputs - input data $\\textbf{x}^t$, activations of hidden units from the previous time step $\\textbf{x}^{t-1}$, and the cell state of the previous time step $\\textbf{C}^{t-1}$.\n",
    "\n",
    "In the above implementation $\\textbf{x}^t$ is the embedded embed_x data tensor. We also need to specify the previous state of the cells. If we're starting a new input sequence, we initialize the cell state to a zero state, then for each seubsequent time step we need to store the updated state of the cells to use in the following time step. The initial state in the implementation above is set by calling cells.zero_state\n",
    "\n",
    "__Step 3: Creating the RNN using the RNN cells and their states__\n",
    "\n",
    "The third step of the RNN creation process used the tf.nn.dynamic_rnn function to pull all of the components together. This function pulls the embedded data, the RNN cells and their initial states, and creates a pipeline for them according to the unrolled architecture of the LSTM cells. It returns a tuple containing the activations of the RNN cells called outputs, as well as their final states in a variable called state. The output is a 3D tensor with the shape [batch size \\times num_steps \\times lstm_size]. We pass the variables outputs to a fully connected layer to get logits and then store the final state so that we can use this as the initial state for the next mini-batch of data. Lastly, once the components of the RNN are setup, the cost function and optimization method is defined in a fashion similar to other networks that we have implemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The train method\n",
    "\n",
    "The train method is very similar to other train functions implemented in previous chapter, except that there is an additional tensor called state that we need to feed into our network.\n",
    "\n",
    "In our implementation, at the beginning of each epoch we start from the zero states of the RNN cells as the current state. The process of running each mini-batch of data is performed by feeding the current state with the batch_x data and the corresponding labels in batch_y. After finishing the process for a mini-batch, we update the state to be the final state, which is returned by the tf.nn.dynamic_rnn function. This updated state will be used in the execution of the next mini-batch. This process is repeated for each mini-batch, and the current state is updated through the epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The train method'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The predict method\n",
    "\n",
    "The predict method is also setup to keep track of the current state, similar to train method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-predict-method'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the SentimentRNN class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Instantiating-the-SentimentRNN-class'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T04:11:56.418482Z",
     "start_time": "2019-06-30T04:11:55.408727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << initial state >>   (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << lstm_output >> Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      "  << final state >> (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << logits    >> Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "  << predictions >> {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "# run SentimentRNN\n",
    "n_words = max(list(word_to_int.values())) + 1\n",
    "rnn = SentimentRNN(\n",
    "    n_words=n_words,\n",
    "    seq_len=sequence_length,\n",
    "    embed_size=256,\n",
    "    lstm_size=128,\n",
    "    num_layers=1,\n",
    "    batch_size=100,\n",
    "    learning_rate=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - num_layers = 1 creates a single RNN layer, but we could set this higher to create a multilayer RNN. Given that we have a relatively small dataset, a multilayer model may tend to overfit the data, sp a single layer approach will likely generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and optimizing the sentiment analysis RNN model\n",
    "\n",
    "Train the model for 40 epochs using X_train and the labels in y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-and-optimizing-the-sentiment-analysis-RNN-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T05:20:52.854793Z",
     "start_time": "2019-06-30T04:12:08.658127Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Iteration 20 | Train loss: 0.68492\n",
      "Epoch 1/20 Iteration 40 | Train loss: 0.56067\n",
      "Epoch 1/20 Iteration 60 | Train loss: 0.66468\n",
      "Epoch 1/20 Iteration 80 | Train loss: 0.54809\n",
      "Epoch 1/20 Iteration 100 | Train loss: 0.55510\n",
      "Epoch 1/20 Iteration 120 | Train loss: 0.47294\n",
      "Epoch 1/20 Iteration 140 | Train loss: 0.51574\n",
      "Epoch 1/20 Iteration 160 | Train loss: 0.43877\n",
      "Epoch 1/20 Iteration 180 | Train loss: 0.46225\n",
      "Epoch 1/20 Iteration 200 | Train loss: 0.47922\n",
      "Epoch 1/20 Iteration 220 | Train loss: 0.47019\n",
      "Epoch 1/20 Iteration 240 | Train loss: 0.47956\n",
      "Epoch 2/20 Iteration 260 | Train loss: 0.45847\n",
      "Epoch 2/20 Iteration 280 | Train loss: 0.29903\n",
      "Epoch 2/20 Iteration 300 | Train loss: 0.39771\n",
      "Epoch 2/20 Iteration 320 | Train loss: 0.38088\n",
      "Epoch 2/20 Iteration 340 | Train loss: 0.32945\n",
      "Epoch 2/20 Iteration 360 | Train loss: 0.27774\n",
      "Epoch 2/20 Iteration 380 | Train loss: 0.36071\n",
      "Epoch 2/20 Iteration 400 | Train loss: 0.30949\n",
      "Epoch 2/20 Iteration 420 | Train loss: 0.31434\n",
      "Epoch 2/20 Iteration 440 | Train loss: 0.32033\n",
      "Epoch 2/20 Iteration 460 | Train loss: 0.45645\n",
      "Epoch 2/20 Iteration 480 | Train loss: 0.23019\n",
      "Epoch 2/20 Iteration 500 | Train loss: 0.30987\n",
      "Epoch 3/20 Iteration 520 | Train loss: 0.28442\n",
      "Epoch 3/20 Iteration 540 | Train loss: 0.22279\n",
      "Epoch 3/20 Iteration 560 | Train loss: 0.33980\n",
      "Epoch 3/20 Iteration 580 | Train loss: 0.20758\n",
      "Epoch 3/20 Iteration 600 | Train loss: 0.19748\n",
      "Epoch 3/20 Iteration 620 | Train loss: 0.20503\n",
      "Epoch 3/20 Iteration 640 | Train loss: 0.30889\n",
      "Epoch 3/20 Iteration 660 | Train loss: 0.19080\n",
      "Epoch 3/20 Iteration 680 | Train loss: 0.26983\n",
      "Epoch 3/20 Iteration 700 | Train loss: 0.22507\n",
      "Epoch 3/20 Iteration 720 | Train loss: 0.25520\n",
      "Epoch 3/20 Iteration 740 | Train loss: 0.29507\n",
      "Epoch 4/20 Iteration 760 | Train loss: 0.24888\n",
      "Epoch 4/20 Iteration 780 | Train loss: 0.24141\n",
      "Epoch 4/20 Iteration 800 | Train loss: 0.23896\n",
      "Epoch 4/20 Iteration 820 | Train loss: 0.37038\n",
      "Epoch 4/20 Iteration 840 | Train loss: 0.14814\n",
      "Epoch 4/20 Iteration 860 | Train loss: 0.10456\n",
      "Epoch 4/20 Iteration 880 | Train loss: 0.23802\n",
      "Epoch 4/20 Iteration 900 | Train loss: 0.26176\n",
      "Epoch 4/20 Iteration 920 | Train loss: 0.19879\n",
      "Epoch 4/20 Iteration 940 | Train loss: 0.23131\n",
      "Epoch 4/20 Iteration 960 | Train loss: 0.20040\n",
      "Epoch 4/20 Iteration 980 | Train loss: 0.19049\n",
      "Epoch 4/20 Iteration 1000 | Train loss: 0.27534\n",
      "Epoch 5/20 Iteration 1020 | Train loss: 0.12479\n",
      "Epoch 5/20 Iteration 1040 | Train loss: 0.18023\n",
      "Epoch 5/20 Iteration 1060 | Train loss: 0.23444\n",
      "Epoch 5/20 Iteration 1080 | Train loss: 0.12528\n",
      "Epoch 5/20 Iteration 1100 | Train loss: 0.06681\n",
      "Epoch 5/20 Iteration 1120 | Train loss: 0.16894\n",
      "Epoch 5/20 Iteration 1140 | Train loss: 0.08027\n",
      "Epoch 5/20 Iteration 1160 | Train loss: 0.05777\n",
      "Epoch 5/20 Iteration 1180 | Train loss: 0.20162\n",
      "Epoch 5/20 Iteration 1200 | Train loss: 0.11876\n",
      "Epoch 5/20 Iteration 1220 | Train loss: 0.25111\n",
      "Epoch 5/20 Iteration 1240 | Train loss: 0.23491\n",
      "Epoch 6/20 Iteration 1260 | Train loss: 0.17100\n",
      "Epoch 6/20 Iteration 1280 | Train loss: 0.06526\n",
      "Epoch 6/20 Iteration 1300 | Train loss: 0.11402\n",
      "Epoch 6/20 Iteration 1320 | Train loss: 0.11955\n",
      "Epoch 6/20 Iteration 1340 | Train loss: 0.08547\n",
      "Epoch 6/20 Iteration 1360 | Train loss: 0.07153\n",
      "Epoch 6/20 Iteration 1380 | Train loss: 0.12588\n",
      "Epoch 6/20 Iteration 1400 | Train loss: 0.09524\n",
      "Epoch 6/20 Iteration 1420 | Train loss: 0.17504\n",
      "Epoch 6/20 Iteration 1440 | Train loss: 0.14320\n",
      "Epoch 6/20 Iteration 1460 | Train loss: 0.22636\n",
      "Epoch 6/20 Iteration 1480 | Train loss: 0.17810\n",
      "Epoch 6/20 Iteration 1500 | Train loss: 0.18317\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch 7/20 Iteration 1520 | Train loss: 0.07803\n",
      "Epoch 7/20 Iteration 1540 | Train loss: 0.04042\n",
      "Epoch 7/20 Iteration 1560 | Train loss: 0.08368\n",
      "Epoch 7/20 Iteration 1580 | Train loss: 0.05353\n",
      "Epoch 7/20 Iteration 1600 | Train loss: 0.04468\n",
      "Epoch 7/20 Iteration 1620 | Train loss: 0.19199\n",
      "Epoch 7/20 Iteration 1640 | Train loss: 0.04612\n",
      "Epoch 7/20 Iteration 1660 | Train loss: 0.01531\n",
      "Epoch 7/20 Iteration 1680 | Train loss: 0.04095\n",
      "Epoch 7/20 Iteration 1700 | Train loss: 0.00679\n",
      "Epoch 7/20 Iteration 1720 | Train loss: 0.06438\n",
      "Epoch 7/20 Iteration 1740 | Train loss: 0.07103\n",
      "Epoch 8/20 Iteration 1760 | Train loss: 0.11657\n",
      "Epoch 8/20 Iteration 1780 | Train loss: 0.01507\n",
      "Epoch 8/20 Iteration 1800 | Train loss: 0.01715\n",
      "Epoch 8/20 Iteration 1820 | Train loss: 0.03098\n",
      "Epoch 8/20 Iteration 1840 | Train loss: 0.01332\n",
      "Epoch 8/20 Iteration 1860 | Train loss: 0.04982\n",
      "Epoch 8/20 Iteration 1880 | Train loss: 0.07760\n",
      "Epoch 8/20 Iteration 1900 | Train loss: 0.06697\n",
      "Epoch 8/20 Iteration 1920 | Train loss: 0.06544\n",
      "Epoch 8/20 Iteration 1940 | Train loss: 0.02316\n",
      "Epoch 8/20 Iteration 1960 | Train loss: 0.03924\n",
      "Epoch 8/20 Iteration 1980 | Train loss: 0.07217\n",
      "Epoch 8/20 Iteration 2000 | Train loss: 0.10950\n",
      "Epoch 9/20 Iteration 2020 | Train loss: 0.07391\n",
      "Epoch 9/20 Iteration 2040 | Train loss: 0.01165\n",
      "Epoch 9/20 Iteration 2060 | Train loss: 0.01229\n",
      "Epoch 9/20 Iteration 2080 | Train loss: 0.00722\n",
      "Epoch 9/20 Iteration 2100 | Train loss: 0.02202\n",
      "Epoch 9/20 Iteration 2120 | Train loss: 0.09644\n",
      "Epoch 9/20 Iteration 2140 | Train loss: 0.00953\n",
      "Epoch 9/20 Iteration 2160 | Train loss: 0.01241\n",
      "Epoch 9/20 Iteration 2180 | Train loss: 0.13190\n",
      "Epoch 9/20 Iteration 2200 | Train loss: 0.06310\n",
      "Epoch 9/20 Iteration 2220 | Train loss: 0.09454\n",
      "Epoch 9/20 Iteration 2240 | Train loss: 0.03723\n",
      "Epoch 10/20 Iteration 2260 | Train loss: 0.03110\n",
      "Epoch 10/20 Iteration 2280 | Train loss: 0.10659\n",
      "Epoch 10/20 Iteration 2300 | Train loss: 0.01519\n",
      "Epoch 10/20 Iteration 2320 | Train loss: 0.08259\n",
      "Epoch 10/20 Iteration 2340 | Train loss: 0.03604\n",
      "Epoch 10/20 Iteration 2360 | Train loss: 0.03528\n",
      "Epoch 10/20 Iteration 2380 | Train loss: 0.27636\n",
      "Epoch 10/20 Iteration 2400 | Train loss: 0.11426\n",
      "Epoch 10/20 Iteration 2420 | Train loss: 0.09820\n",
      "Epoch 10/20 Iteration 2440 | Train loss: 0.07624\n",
      "Epoch 10/20 Iteration 2460 | Train loss: 0.01876\n",
      "Epoch 10/20 Iteration 2480 | Train loss: 0.04931\n",
      "Epoch 10/20 Iteration 2500 | Train loss: 0.06474\n",
      "Epoch 11/20 Iteration 2520 | Train loss: 0.04912\n",
      "Epoch 11/20 Iteration 2540 | Train loss: 0.00981\n",
      "Epoch 11/20 Iteration 2560 | Train loss: 0.04066\n",
      "Epoch 11/20 Iteration 2580 | Train loss: 0.06115\n",
      "Epoch 11/20 Iteration 2600 | Train loss: 0.01944\n",
      "Epoch 11/20 Iteration 2620 | Train loss: 0.12009\n",
      "Epoch 11/20 Iteration 2640 | Train loss: 0.00707\n",
      "Epoch 11/20 Iteration 2660 | Train loss: 0.03881\n",
      "Epoch 11/20 Iteration 2680 | Train loss: 0.01678\n",
      "Epoch 11/20 Iteration 2700 | Train loss: 0.02482\n",
      "Epoch 11/20 Iteration 2720 | Train loss: 0.01305\n",
      "Epoch 11/20 Iteration 2740 | Train loss: 0.03769\n",
      "Epoch 12/20 Iteration 2760 | Train loss: 0.08621\n",
      "Epoch 12/20 Iteration 2780 | Train loss: 0.00782\n",
      "Epoch 12/20 Iteration 2800 | Train loss: 0.01474\n",
      "Epoch 12/20 Iteration 2820 | Train loss: 0.01415\n",
      "Epoch 12/20 Iteration 2840 | Train loss: 0.07909\n",
      "Epoch 12/20 Iteration 2860 | Train loss: 0.01737\n",
      "Epoch 12/20 Iteration 2880 | Train loss: 0.13464\n",
      "Epoch 12/20 Iteration 2900 | Train loss: 0.05951\n",
      "Epoch 12/20 Iteration 2920 | Train loss: 0.05185\n",
      "Epoch 12/20 Iteration 2940 | Train loss: 0.00806\n",
      "Epoch 12/20 Iteration 2960 | Train loss: 0.00445\n",
      "Epoch 12/20 Iteration 2980 | Train loss: 0.17341\n",
      "Epoch 12/20 Iteration 3000 | Train loss: 0.08256\n",
      "Epoch 13/20 Iteration 3020 | Train loss: 0.00973\n",
      "Epoch 13/20 Iteration 3040 | Train loss: 0.00451\n",
      "Epoch 13/20 Iteration 3060 | Train loss: 0.02394\n",
      "Epoch 13/20 Iteration 3080 | Train loss: 0.01047\n",
      "Epoch 13/20 Iteration 3100 | Train loss: 0.00403\n",
      "Epoch 13/20 Iteration 3120 | Train loss: 0.14009\n",
      "Epoch 13/20 Iteration 3140 | Train loss: 0.00329\n",
      "Epoch 13/20 Iteration 3160 | Train loss: 0.00402\n",
      "Epoch 13/20 Iteration 3180 | Train loss: 0.02482\n",
      "Epoch 13/20 Iteration 3200 | Train loss: 0.00189\n",
      "Epoch 13/20 Iteration 3220 | Train loss: 0.00078\n",
      "Epoch 13/20 Iteration 3240 | Train loss: 0.01141\n",
      "Epoch 14/20 Iteration 3260 | Train loss: 0.00445\n",
      "Epoch 14/20 Iteration 3280 | Train loss: 0.00051\n",
      "Epoch 14/20 Iteration 3300 | Train loss: 0.00238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Iteration 3320 | Train loss: 0.01868\n",
      "Epoch 14/20 Iteration 3340 | Train loss: 0.00616\n",
      "Epoch 14/20 Iteration 3360 | Train loss: 0.00598\n",
      "Epoch 14/20 Iteration 3380 | Train loss: 0.01195\n",
      "Epoch 14/20 Iteration 3400 | Train loss: 0.00470\n",
      "Epoch 14/20 Iteration 3420 | Train loss: 0.00747\n",
      "Epoch 14/20 Iteration 3440 | Train loss: 0.00253\n",
      "Epoch 14/20 Iteration 3460 | Train loss: 0.00123\n",
      "Epoch 14/20 Iteration 3480 | Train loss: 0.00040\n",
      "Epoch 14/20 Iteration 3500 | Train loss: 0.00175\n",
      "Epoch 15/20 Iteration 3520 | Train loss: 0.00573\n",
      "Epoch 15/20 Iteration 3540 | Train loss: 0.00096\n",
      "Epoch 15/20 Iteration 3560 | Train loss: 0.03162\n",
      "Epoch 15/20 Iteration 3580 | Train loss: 0.00238\n",
      "Epoch 15/20 Iteration 3600 | Train loss: 0.00211\n",
      "Epoch 15/20 Iteration 3620 | Train loss: 0.06452\n",
      "Epoch 15/20 Iteration 3640 | Train loss: 0.01990\n",
      "Epoch 15/20 Iteration 3660 | Train loss: 0.01134\n",
      "Epoch 15/20 Iteration 3680 | Train loss: 0.00149\n",
      "Epoch 15/20 Iteration 3700 | Train loss: 0.00680\n",
      "Epoch 15/20 Iteration 3720 | Train loss: 0.05542\n",
      "Epoch 15/20 Iteration 3740 | Train loss: 0.00566\n",
      "Epoch 16/20 Iteration 3760 | Train loss: 0.00247\n",
      "Epoch 16/20 Iteration 3780 | Train loss: 0.05163\n",
      "Epoch 16/20 Iteration 3800 | Train loss: 0.00325\n",
      "Epoch 16/20 Iteration 3820 | Train loss: 0.00084\n",
      "Epoch 16/20 Iteration 3840 | Train loss: 0.00417\n",
      "Epoch 16/20 Iteration 3860 | Train loss: 0.00066\n",
      "Epoch 16/20 Iteration 3880 | Train loss: 0.02315\n",
      "Epoch 16/20 Iteration 3900 | Train loss: 0.00341\n",
      "Epoch 16/20 Iteration 3920 | Train loss: 0.00279\n",
      "Epoch 16/20 Iteration 3940 | Train loss: 0.00267\n",
      "Epoch 16/20 Iteration 3960 | Train loss: 0.00046\n",
      "Epoch 16/20 Iteration 3980 | Train loss: 0.00129\n",
      "Epoch 16/20 Iteration 4000 | Train loss: 0.00097\n",
      "Epoch 17/20 Iteration 4020 | Train loss: 0.00340\n",
      "Epoch 17/20 Iteration 4040 | Train loss: 0.00048\n",
      "Epoch 17/20 Iteration 4060 | Train loss: 0.00062\n",
      "Epoch 17/20 Iteration 4080 | Train loss: 0.00092\n",
      "Epoch 17/20 Iteration 4100 | Train loss: 0.00082\n",
      "Epoch 17/20 Iteration 4120 | Train loss: 0.07497\n",
      "Epoch 17/20 Iteration 4140 | Train loss: 0.00133\n",
      "Epoch 17/20 Iteration 4160 | Train loss: 0.00035\n",
      "Epoch 17/20 Iteration 4180 | Train loss: 0.00031\n",
      "Epoch 17/20 Iteration 4200 | Train loss: 0.00150\n",
      "Epoch 17/20 Iteration 4220 | Train loss: 0.00501\n",
      "Epoch 17/20 Iteration 4240 | Train loss: 0.01195\n",
      "Epoch 18/20 Iteration 4260 | Train loss: 0.02179\n",
      "Epoch 18/20 Iteration 4280 | Train loss: 0.00105\n",
      "Epoch 18/20 Iteration 4300 | Train loss: 0.00226\n",
      "Epoch 18/20 Iteration 4320 | Train loss: 0.00267\n",
      "Epoch 18/20 Iteration 4340 | Train loss: 0.02678\n",
      "Epoch 18/20 Iteration 4360 | Train loss: 0.00131\n",
      "Epoch 18/20 Iteration 4380 | Train loss: 0.00894\n",
      "Epoch 18/20 Iteration 4400 | Train loss: 0.00385\n",
      "Epoch 18/20 Iteration 4420 | Train loss: 0.01122\n",
      "Epoch 18/20 Iteration 4440 | Train loss: 0.00089\n",
      "Epoch 18/20 Iteration 4460 | Train loss: 0.00885\n",
      "Epoch 18/20 Iteration 4480 | Train loss: 0.00483\n",
      "Epoch 18/20 Iteration 4500 | Train loss: 0.03906\n",
      "Epoch 19/20 Iteration 4520 | Train loss: 0.00494\n",
      "Epoch 19/20 Iteration 4540 | Train loss: 0.00067\n",
      "Epoch 19/20 Iteration 4560 | Train loss: 0.00071\n",
      "Epoch 19/20 Iteration 4580 | Train loss: 0.03237\n",
      "Epoch 19/20 Iteration 4600 | Train loss: 0.00181\n",
      "Epoch 19/20 Iteration 4620 | Train loss: 0.01844\n",
      "Epoch 19/20 Iteration 4640 | Train loss: 0.00526\n",
      "Epoch 19/20 Iteration 4660 | Train loss: 0.00462\n",
      "Epoch 19/20 Iteration 4680 | Train loss: 0.00457\n",
      "Epoch 19/20 Iteration 4700 | Train loss: 0.00241\n",
      "Epoch 19/20 Iteration 4720 | Train loss: 0.00042\n",
      "Epoch 19/20 Iteration 4740 | Train loss: 0.01258\n",
      "Epoch 20/20 Iteration 4760 | Train loss: 0.00498\n",
      "Epoch 20/20 Iteration 4780 | Train loss: 0.00034\n",
      "Epoch 20/20 Iteration 4800 | Train loss: 0.02209\n",
      "Epoch 20/20 Iteration 4820 | Train loss: 0.00074\n",
      "Epoch 20/20 Iteration 4840 | Train loss: 0.00069\n",
      "Epoch 20/20 Iteration 4860 | Train loss: 0.00094\n",
      "Epoch 20/20 Iteration 4880 | Train loss: 0.00258\n",
      "Epoch 20/20 Iteration 4900 | Train loss: 0.00503\n",
      "Epoch 20/20 Iteration 4920 | Train loss: 0.00157\n",
      "Epoch 20/20 Iteration 4940 | Train loss: 0.00094\n",
      "Epoch 20/20 Iteration 4960 | Train loss: 0.00153\n",
      "Epoch 20/20 Iteration 4980 | Train loss: 0.00398\n",
      "Epoch 20/20 Iteration 5000 | Train loss: 0.00336\n"
     ]
    }
   ],
   "source": [
    "# display results by epoch/iteration\n",
    "rnn.train(X_train, y_train, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T16:26:12.407345Z",
     "start_time": "2019-06-30T16:25:19.019566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ch16_files/model/sentiment-19.ckpt\n",
      "test accuracy: 0.838\n"
     ]
    }
   ],
   "source": [
    "# create predictions and calculate accuracy\n",
    "preds = rnn.predict(X_test)\n",
    "yTrue = y_test[: len(preds)]\n",
    "print(\"test accuracy: {:.3f}\".format(np.sum(preds == yTrue) / len(yTrue)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - This result is comparable to what was achieved in chapter 8, mainly due to the small size of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T16:27:37.579158Z",
     "start_time": "2019-06-30T16:26:43.902908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ch16_files/model/sentiment-19.ckpt\n"
     ]
    }
   ],
   "source": [
    "# calculate probabilities\n",
    "proba = rnn.predict(X_test, return_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T16:27:37.589518Z",
     "start_time": "2019-06-30T16:27:37.582345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000027, 0.99999976, 0.8358301 , 0.00001538, 0.00009781],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print subset of probabilities\n",
    "proba[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can be optimized further by changing the hyperparameters, such as lstm_size, seq_len, and embed_size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project two – implementing an RNN for character-level language modeling in TensorFlow\n",
    "\n",
    "The input for this model will be a text document, and the goal is to develop a model that can generate new text that is similar to the input document. Examples of an input could be a book or a computer program written in a certain language.\n",
    "\n",
    "This involves character-level language modeling, where the input is brokem down into a sequence of characters that are fed into the network one character at a time. The netwrok processes each new character in conjunction with its memory of the previously seem characters to predict the next character. A very simple example looks something like this:\n",
    "\n",
    "$$\n",
    "\\mbox{Input data: \"Hello!\"}\n",
    "\\\\\n",
    "\\mbox{Input sequence} \\ \\mbox{| Prediction}\n",
    "\\\\\n",
    "H \\  \\rightarrow \\ e\\\\\n",
    "e \\  \\rightarrow \\ l\\\\\n",
    "l \\  \\rightarrow \\ l\\\\\n",
    "l \\  \\rightarrow \\ o\\\\\n",
    "o \\  \\rightarrow \\ !\\\\\n",
    "! \\  \\rightarrow \\ \\mbox{end}\\\\\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "This implementation has a data prep stage, RNN build stage, and a prediction stage where the model predicts the next character and sampling to generate new text.\n",
    "\n",
    "Just as the sentiment analysis RNN has a tendency to develop an exploding gradient problem that needs to be addressed, this model will also employ a gradient clipping technique to avoid this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Project-two–implementing-an-RNN-for-character-level-language-modeling-in-TensorFlow'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "We will be using 'The Tragedie of Hamlet' by Willian Shakespeare, which can be retrived online in plain text. Just as we mapped unique words to unique integers with the IMDb movies reviews, we will be mapping unique characters to unique integers. We will create a dictionary that maps characters to integers, and another dictionary that mirrors the first by mapping integer to characters. We want the training data array x and the training data array y to have the same shape, where the number of rows is equal to the batch size and the number of columns is the number of batches $\\times$ the number of steps\n",
    "\n",
    "Then we need to reshape the data into mini-batches of sequences. Since the goal is to predict the next character based of the sequence of characters seen up to that point. Therefore, we need to shift the input data and output of the neural network by one character. Next, the $\\textbf{x}$ and $\\textbf{y}$ arrays need to be split into mini-batches where each row is a sequence with a length equal to the number of steps. This is a way of breaking a long sequence of text into several smaller segments. Each mini-batch contains segment of all of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Preparing-the-data2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T16:29:00.150345Z",
     "start_time": "2019-06-30T16:29:00.110766Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(\"s3://tdp-ml-datasets/misc/pg2265.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "text = text[15858:]\n",
    "chars = set(text)\n",
    "char2int = {ch: i for i, ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T16:29:02.480128Z",
     "start_time": "2019-06-30T16:29:02.474254Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom function for preparing data\n",
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    total_batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence) / total_batch_length)\n",
    "    if num_batches * total_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "\n",
    "    # truncate sequence at the end to remove remaining characters that do not make a full batch\n",
    "    x = sequence[0 : num_batches * total_batch_length]\n",
    "    y = sequence[1 : num_batches * total_batch_length + 1]\n",
    "\n",
    "    # split x and y into a list of batches of sequences\n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "\n",
    "    # stack the batches together: shape = [batch _size x total_batch_length]\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T16:29:04.597422Z",
     "start_time": "2019-06-30T16:29:04.590359Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom function for generating sample batches\n",
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, total_batch_length = data_x.shape\n",
    "    num_batches = int(total_batch_length / num_steps)\n",
    "    for b in range(num_batches):\n",
    "        yield (\n",
    "            data_x[:, b * num_steps : (b + 1) * num_steps],\n",
    "            data_y[:, b * num_steps : (b + 1) * num_steps],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a character-level RNN model\n",
    "\n",
    "CharRNN is a class that will construct a graph to predict the next character in a sequence after observing a given sequence. This can be thought of as choosing a class, where the number of classes is the total number of unique characters in the text corpus. CharRNN has four mathods\n",
    "\n",
    "- Constructor: setup learning parameters, create graph, and call build method to construct the graph in sampling mode or training mode\n",
    "- Build: define placeholders for feeding in data, construct RNN using LSTM cells, as well as define network output, cost function and optimizer\n",
    "- train: iterate through mini batches and train for a certain number of epochs.\n",
    "- sample: start from a given string, calculate the probabilities of teh next character, and choose a character randomly according to the probabilities. This process is repeated and samples characters will be concatenated together to form a string. Once the string reaches a specified length, it return the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Building-a-character-level-RNN-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T16:29:15.378293Z",
     "start_time": "2019-06-30T16:29:15.373357Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom function for calculating most top character\n",
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    p = np.squeeze(probas)\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "    return ch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T16:34:22.863131Z",
     "start_time": "2019-06-30T16:34:22.835020Z"
    }
   },
   "outputs": [],
   "source": [
    "# character-level RNN custom class\n",
    "class CharRNN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        batch_size=64,\n",
    "        num_steps=100,\n",
    "        lstm_size=128,\n",
    "        num_layers=1,\n",
    "        learning_rate=0.001,\n",
    "        keep_prob=0.5,\n",
    "        grad_clip=5,\n",
    "        sampling=False,\n",
    "    ):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "\n",
    "        tf_x = tf.placeholder(tf.int32, shape=[batch_size, num_steps], name=\"tf_x\")\n",
    "        tf_y = tf.placeholder(tf.int32, shape=[batch_size, num_steps], name=\"tf_y\")\n",
    "        tf_keepprob = tf.placeholder(tf.float32, name=\"tf_keepprob\")\n",
    "\n",
    "        # one -hot encoding:\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "\n",
    "        # built multilayer RNN cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [\n",
    "                tf.contrib.rnn.DropoutWrapper(\n",
    "                    tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                    output_keep_prob=tf_keepprob,\n",
    "                )\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # define the initial state\n",
    "        self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # run each sequence step through RNN\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "            cells, x_onehot, initial_state=self.initial_state\n",
    "        )\n",
    "        print(\"  << lstm_outputs >>  \", lstm_outputs)\n",
    "\n",
    "        seq_output_reshaped = tf.reshape(\n",
    "            lstm_outputs, shape=[-1, self.lstm_size], name=\"seq_output_reshaped\"\n",
    "        )\n",
    "\n",
    "        logits = tf.layers.dense(\n",
    "            inputs=seq_output_reshaped,\n",
    "            units=self.num_classes,\n",
    "            activation=None,\n",
    "            name=\"logits\",\n",
    "        )\n",
    "        proba = tf.nn.softmax(logits, name=\"probabilities\")\n",
    "        y_reshaped = tf.reshape(\n",
    "            y_onehot, shape=[-1, self.num_classes], name=\"y_reshaped\"\n",
    "        )\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped),\n",
    "            name=\"cost\",\n",
    "        )\n",
    "\n",
    "        # gradient clipping\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), self.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), name=\"train_op\")\n",
    "\n",
    "    def train(self, train_x, train_y, num_epochs, ckpt_dir=\"./ch16_files/model2/\"):\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "\n",
    "            n_batches = int(train_x.shape[1] / self.num_steps)\n",
    "            iterations = n_batches * num_epochs\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "\n",
    "                # train network\n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "\n",
    "                # mini-batch generator\n",
    "                bgen = create_batch_generator(train_x, train_y, self.num_steps)\n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch * n_batches + b\n",
    "                    feed = {\n",
    "                        \"tf_x:0\": batch_x,\n",
    "                        \"tf_y:0\": batch_y,\n",
    "                        \"tf_keepprob:0\": self.keep_prob,\n",
    "                        self.initial_state: new_state,\n",
    "                    }\n",
    "                    batch_cost, _, new_state = sess.run(\n",
    "                        [\"cost:0\", \"train_op\", self.final_state], feed_dict=feed\n",
    "                    )\n",
    "                    if iteration % 100 == 0:\n",
    "                        print(\n",
    "                            \"Epoch {}/{} Iteration {} | Training loss: {:.5f}\".format(\n",
    "                                epoch + 1, num_epochs, iteration, batch_cost\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                # save trained model\n",
    "                self.saver.save(sess, os.path.join(ckpt_dir, \"language_modeling.ckpt\"))\n",
    "\n",
    "    def sample(self, output_length, ckpt_dir, starter_seq=\"The \"):\n",
    "        observed_seq = [ch for ch in starter_seq]\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\n",
    "\n",
    "            # 1: run the model using starter sequence\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0, 0] = char2int[ch]\n",
    "            feed = {\"tf_x:0\": x, \"tf_keepprob:0\": 1.0, self.initial_state: new_state}\n",
    "            proba, new_state = sess.run(\n",
    "                [\"probabilities:0\", self.final_state], feed_dict=feed\n",
    "            )\n",
    "\n",
    "            ch_id = get_top_char(proba, len(chars))\n",
    "            observed_seq.append(int2char[ch_id])\n",
    "\n",
    "            # 2: run the model using the updated observed_seq\n",
    "            for i in range(output_length):\n",
    "                x[0, 0] = ch_id\n",
    "                feed = {\n",
    "                    \"tf_x:0\": x,\n",
    "                    \"tf_keepprob:0\": 1.0,\n",
    "                    self.initial_state: new_state,\n",
    "                }\n",
    "                proba, new_state = sess.run(\n",
    "                    [\"probabilities:0\", self.final_state], feed_dict=feed\n",
    "                )\n",
    "\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "        return \"\".join(observed_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The-constructor\n",
    "\n",
    "Unlike the sentiment analysis computation graph, where we used the same graph for both training and prediction modes, this model will have different graphs for the training and sampling modes.\n",
    "\n",
    "To handle this, we add a boolean argument to determine the mode.\n",
    "\n",
    "We also add an argument called grad_clip, which is used for clipping gradients to avoid exploding gradient issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-constructor'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The build method\n",
    "\n",
    "The build function first defines two local variables, batch_size and num_steps, based on the mode:\n",
    "\n",
    "$$\n",
    "\\mbox{in sampling mode} =\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        \\mbox{batch_size} \\ = 1  \\\\\n",
    "        \\mbox{num_Steps} \\ = 1\n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\\\\n",
    "\\mbox{in training mode} =\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        \\mbox{batch_size} \\ = self.batch\\_size  \\\\\n",
    "        \\mbox{num_Steps} \\ = self.num\\_steps\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Rather than using an embedding layer to efficiently create a salient representation of unique words in the data, we will just use a one-hot encoding scheme for both $x$ and $y$ with 'depth = num_classes', where 'num_classes' is indeed the total number of characters in the corpus.\n",
    "\n",
    "The process of building the multilayer RNN component is exactly the same as in the sentiment analysis representation, except that 'outputs' from 'tf.nn.dynamic_rnn' is a 3D tensor with the shape [batch_size \\times num_steps \\times lstm_size]. Then this tensor is reshaped into a 2D tensor with the 'batch_size*num_steps, lstm_steps' shape, which is passed into the fully connected layer 'tf.layers.dense' to get the logits (net input). Lastly, the probabilities for the next batch of characters are obtained and the cost function is defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The build method2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The train method\n",
    "\n",
    "This method is very similar to the 'train' method implemented in the sentiment analysis RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-train-method2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The sample method\n",
    "\n",
    "This is similar to the predict method implemented in the sentiment analysis RNN, with the key difference being that we calculate the probabilities for the next character from an input sequence 'observed_seq'. Then these probabilties are passed to a function  'get_top_char', which randomly selects one character based on the probabilities.\n",
    "\n",
    "The first observed sequence, starts with 'starter_seq', and then when new characters are sampled according to their predicted probabilties, they are appended to the observed sequence, and this newly updated sequence is used for predicting the next character.\n",
    "\n",
    "The 'sample' method calls the 'get_top_char' function to choose a character ID randomly ('ch_id') according to the returned probabilities. 'get_top_char' sorts the probabilities, then the 'top_n' probabilities are passed to 'numpy.random.choice' to randomly select one out of these top probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-sample-method'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and training the CharRNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-and-training-the-CharRNN-Model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T16:37:34.884890Z",
     "start_time": "2019-06-30T16:34:25.487770Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << lstm_outputs >>   Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n",
      "Epoch 4/50 Iteration 100 | Training loss: 3.14284\n",
      "Epoch 8/50 Iteration 200 | Training loss: 2.79166\n",
      "Epoch 12/50 Iteration 300 | Training loss: 2.50157\n",
      "Epoch 16/50 Iteration 400 | Training loss: 2.35567\n",
      "Epoch 20/50 Iteration 500 | Training loss: 2.27658\n",
      "Epoch 24/50 Iteration 600 | Training loss: 2.23098\n",
      "Epoch 28/50 Iteration 700 | Training loss: 2.19604\n",
      "Epoch 32/50 Iteration 800 | Training loss: 2.15389\n",
      "Epoch 36/50 Iteration 900 | Training loss: 2.12686\n",
      "Epoch 40/50 Iteration 1000 | Training loss: 2.09561\n",
      "Epoch 44/50 Iteration 1100 | Training loss: 2.06910\n",
      "Epoch 48/50 Iteration 1200 | Training loss: 2.04369\n"
     ]
    }
   ],
   "source": [
    "# train character RNN\n",
    "batch_size = 64\n",
    "num_steps = 100\n",
    "train_x, train_y = reshape_data(text_ints, batch_size, num_steps)\n",
    "\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "rnn.train(train_x, train_y, num_epochs=50, ckpt_dir=\"./ch16_files/model2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CharRNN model in the sampling mode\n",
    "\n",
    "Create a new instance in sampling mode and generate a sequnce of 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-CharRNN-model-in-the-sampling-mode'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T16:38:49.488865Z",
     "start_time": "2019-06-30T16:38:48.210739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << lstm_outputs >>   Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 128), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./ch16_files/model2/language_modeling.ckpt\n",
      "The way selle the that\n",
      "\n",
      "   Ham. I mo the whes thit wind and this thes at are bothare.\n",
      "Whind atendest te tho desther the serint ang ther ang to bothes,\n",
      "But is a the the thene soull my tho hee and is me in a theare, in wis thath in the toong thee aue authere,\n",
      "Iles on hat tare to tor are me auline, wall\n",
      " a to me that thes seare, thee sille the this dond,\n",
      "And mo the merere ant ond ind there and this dond,\n",
      "Whan sis moue to the wislles to day, be thare, ar hath my here,\n",
      "Whore we hourd ond morertens mingre \n"
     ]
    }
   ],
   "source": [
    "# generate sample text\n",
    "del rnn\n",
    "\n",
    "np.random.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "print(rnn.sample(ckpt_dir=\"./ch16_files/model2/\", output_length=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are clearly some English words within this block of text. To further strengthen the model, additional epochs with additional data are needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
