{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  __Chapter 6 - Text classification__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Text classification](#Text-classification)\n",
    "    1. [Basic modeling](#Basic-modeling)\n",
    "        1. [Sampling](#Sampling)\n",
    "        1. [Naive Bayes](#Naive-Bayes)\n",
    "1. [Text clustering](#Text-clustering)\n",
    "    1. [K-means](#K-means)\n",
    "1. [Topic modeling in text](#Topic-modeling-in-text)\n",
    "1. [](#)\n",
    "1. [](#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# Modeling extensions\n",
    "import nltk\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Text-classification'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = text.decode(\"utf8\")\n",
    "\n",
    "    # tokenize into words\n",
    "    tokens = [\n",
    "        word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)\n",
    "    ]\n",
    "\n",
    "    # remove stopwords\n",
    "    stop = stopwords.words(\"english\")\n",
    "    tokens = [tokens for token in tokens if token not in stop]\n",
    "\n",
    "    # remove words < 3 letters\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "    # lower capitalization\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # lemmetize\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens - [lmtzr.lemmatize(word) for word in tokens]\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SMSSpamCollection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-88d01cd41545>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msmsdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SMSSpamCollection'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SMSSpamCollection'"
     ]
    }
   ],
   "source": [
    "#\n",
    "smsdata = open(\"SMSSpamCollection\")\n",
    "smsdata_data = []\n",
    "sms_labels = []\n",
    "csv_reader = csv.reader(sms, delimiter=\"\\t\")\n",
    "for line in csv_reader:\n",
    "    # add sms ID\n",
    "    sms_labels.append(line[0])\n",
    "\n",
    "    # call preprocessing function\n",
    "    sms_data.append(preprocess(line[1]))\n",
    "sms.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Basic-modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "Split data into train data and test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Sampling'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "trainset_size = int(round(len(sms_data) * 0.70))\n",
    "X_train = np.array([\"\".join(el) for el in sms_data[0:trainset_size]])\n",
    "y_train = np.array([el for el in sms_data[0:trainset_size]])\n",
    "X_test = np.array([\"\".join(el) for el in sms_data[trainset_size + 1 : len(sms_data)]])\n",
    "y_test = np.array([el for el in sms_data[trainset_size + 1 : len(sms_data)]])\n",
    "\n",
    "print(X_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create term-document matrix using bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sms_exp = []\n",
    "for line in sms_list:\n",
    "    sms_exp.append(preprocessing(line[1]))\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "xExp = vectorizer.fit_transform(sms_exp)\n",
    "print(\"||\".join(vectorizer.get_feature_names()))\n",
    "print(xExp.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilize TF-IDF to downscale weights for words that occur in many documents\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=\"english\",\n",
    "    strip_accents=\"unicode\",\n",
    "    norm=\"l2\",\n",
    ")\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes, in a basic sense, assigns a class label to a sample based on the conditional probability class given the sample's feature values. The 'naive' aspect of the algorithm stems from the fact that the model assumes all features are independent from each other, which is particularly counter-intuitive in the context of text analysis. Despite this, Naive Bayes typically performs very well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Naive-Bayes'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model and make predictions\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_nb_predicted = clf.predict(X_test)\n",
    "print(y_nb_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate results\n",
    "print(\"\\nConfusion matrix\\n\")\n",
    "cm = confusion_matrix(y_test, yPred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification report\\n\")\n",
    "cm = classification_report(y_test, y_nb_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe features that contribute to the positive and negative predictions\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "coefs = clf.coef_\n",
    "intercept = clf.intercept_\n",
    "\n",
    "coefs_with_fns = sorted(zip(clf.coef_[0], features_names))\n",
    "n = 10\n",
    "\n",
    "top = zip(coefs_with_fns[:n], coefs_with_fns[: -(n + 1) : -1])\n",
    "for ((coef_1, fn_2), (coef_2, fn_2)) in top:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text clustering\n",
    "\n",
    "Text clustering is often used to group together a large corpus of documents in an unsupervised fashion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Text-clustering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'K-means'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "true_k = 5\n",
    "km = KMeans(n_clusters=true_k, init=\"k-means++\", max_iter=100, n_init=1)\n",
    "kmini = MiniBatchKMeans(\n",
    "    n_clusters=true_k,\n",
    "    init=\"k-means++\",\n",
    "    max_iter=100,\n",
    "    n_init=1,\n",
    "    init_size=1000,\n",
    "    batch_Size=1000,\n",
    "    verbose=opts.verbose,\n",
    ")\n",
    "\n",
    "km_model = km.fit(X_train)\n",
    "kmini_model = kmini.fit(X_train)\n",
    "\n",
    "print(\"K-means clustering (full dataset)\")\n",
    "clustering = collection.defaultdict(list)\n",
    "for idx, label in enumerate(km_model.labels_):\n",
    "    clustering[label].append(idx)\n",
    "\n",
    "print(\"K-means clustering (mini batch)\")\n",
    "clustering = collection.defaultdict(list)\n",
    "for idx, label in enumerate(kmini_model.labels_):\n",
    "    clustering[label].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling in text\n",
    "\n",
    "In many industries, there are large unlabeled text documents datasets. Topic modeling is one approach toward addressing the need to categorize the documents. Latent Dirichlet allocation (LDA) and Latent semantic indexing (LSI) are two techniques often used to determine the topic of a given document. A Python library called gensim implements these algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Topic-modeling-in-text'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter\n",
    "import re\n",
    "\n",
    "# read documents and remove stop words\n",
    "document = [document for document in sms_data]\n",
    "stoplist = stopwords.words(\"english\")\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to bag of words and TF-IDF\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## given a number of topics, the models use all documents in the corpus to build identify topics\n",
    "# LSI model\n",
    "lis = models.LisaModel(corpus_tfidf, id2word=dictionary, num_topics=100)\n",
    "lsi.print_topics(20)\n",
    "\n",
    "# LDA model\n",
    "n_topics = 5\n",
    "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=n_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
