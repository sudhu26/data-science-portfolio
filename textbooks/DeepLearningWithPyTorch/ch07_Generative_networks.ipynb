{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 7 - Generative Networks__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Neural style transfer](#Neural-style-transfer)\n",
    "    1. [Loading the data](#Loading-the-data)\n",
    "    1. [Creating the VGG model](#Creating-the-VGG-model)\n",
    "    1. [Content loss](#Content-loss)\n",
    "    1. [Style loss](#Style-loss)\n",
    "    1. [Extracting the losses](#Extracting-the-losses)\n",
    "    1. [Creating loss function for each layers](#Creating-loss-function-for-each-layers)\n",
    "    1. [Creating the optimizer](#Creating-the-optimizer)\n",
    "    1. [Training](#Training)\n",
    "1. [Generative adversarial networks](#Generative-adversarial-networks)\n",
    "    1. [Deep convolutional GAN](#Deep-convolutional-GAN)\n",
    "        1. [Defining the generator network](#Defining-the-generator-network)\n",
    "        1. [Defining the discriminator network](#Defining-the-discriminator-network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# pytorch tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural style transfer\n",
    "\n",
    "Given a content image and a style image, generate a new image that combines the content of the content image and the style of the style image.\n",
    "\n",
    "The style of an image is captured across multiple layers in a CNN by a technique called gram matrix. This calculates the correlation between the features maps captures across each layer. Similarly styled images have similar values for a gram matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Neural-style-transfer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Loading-the-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the image size\n",
    "imsize = 512\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# convert image for training with VGG model\n",
    "prep = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(imsize),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x[torch.LongTensor([2, 1, 0])]),  # turn to BGR\n",
    "        transforms.Normalize(\n",
    "            mean=[0.40760392, 0.45795686, 0.48501961], std=[1, 1, 1]\n",
    "        ),  # subtract imagenet mean\n",
    "        transforms.Lambda(lambda x: x.mul_(255)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# convert the generated image back to a format that can be visualized\n",
    "postpa = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda x: x.mul_(1.0 / 255)),\n",
    "        transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], std=[1, 1, 1]),\n",
    "        transforms.Lambda(lambda x: x[torch.LongTensor([2, 1, 0])]),  # turn to RGB\n",
    "    ]\n",
    ")\n",
    "postpb = transforms.Compose([transform.ToPILImage()])\n",
    "\n",
    "# ensure data in the image does not cross the permissible range of values\n",
    "def postp(tensor):\n",
    "    t = postpa(tensor)\n",
    "    t[t > 1] = 1\n",
    "    t[t < 0] = 0\n",
    "    img = postpb(t)\n",
    "    return img\n",
    "\n",
    "\n",
    "# ease data loading\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(prep(image))\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load style and conversion image\n",
    "style_img = image_loader(\"images/vangogh_starry_night.jpg\")\n",
    "convert_img = image_loader(\"images/tuebinge_neckarfront.jpg\")\n",
    "\n",
    "opt_img = Variable(content_img.data.clone(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the VGG model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-the-VGG-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a VGG model, grabbing only the convolution block (features) and freeze the parameters\n",
    "vgg = vgg19(pretrained=True).features\n",
    "\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Content-loss'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "target_layer = dummy_fn(content_img)\n",
    "noise_layer = dummy_fn(noise_img)\n",
    "criterion = nn.MSELoss()\n",
    "content_loss = criterion(target_layer, noise_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style loss\n",
    "\n",
    "Style loss is the MSE of the gram matrix generated for each feature map. Envision a feature map with dimensions representing bacth_size by color channels and values (which in this example is itself a 3 by 3 window). To calculate the gram matrix, the 9 values in each channel are flattened into a 9 value vector, and then the correlation coefficient is calculated by multiplying the flattened vector by its transpose.\n",
    "\n",
    "The class below is written in a way so that it can be used like another PyTorch layer.  First, the batch, channel, height and width are maintained, and then the features are reshaped such that the batch and channel dimensions remain intact, and the values are flattened along the height and width dimension. The gram matrix is calculated using the PyTorch batch matrix multiplication function torch.bmm(), which will multiply the flattened values with its transposed vector. The final step normalizes the values of the gram matrix by dividing it by the number of elements. Without this, a feature map with an especially high number of values would tend to dominate the score.\n",
    "\n",
    "The second class below calculates the style loss, which is also implemented as a PyTorch layer. It calculates the MSE between the input image gram matrix and the style image gram matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Style-loss'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat gram matrix class\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b, c, h, w = input.size()\n",
    "        features = input.view(b, c, h * w)\n",
    "        gram_matrix = torch.bmm(features, features.tranpose(1, 2))\n",
    "        gram_matrix.div_(h * w)\n",
    "        return gram_matrix\n",
    "\n",
    "\n",
    "# create style loss class\n",
    "class StyleLoss(nn.Module):\n",
    "    def forward(self, inputs, targets):\n",
    "        out = nn.MSELoss()(GramMatrix()(inputs), targets)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the losses\n",
    "\n",
    "Just as activations can be extracted from convolution layers use the register_forward_hook(), we can extract losses of different convolutional layers required to calculate style loss and content loss. The key difference is that rather than extracting from a single layer, we need to extract outputs from several layers.\n",
    "\n",
    "In the class below, the init method takes in the model on which we will call register_forward_hook() as well as the layer ID number for layers from which we will extract the outputs. The init method's for loop iterates through the layer IDs and registers the forward hook required the pull outputs.\n",
    "\n",
    "hook_fn is passed to register_forward_hook() and is called by PyTorch after the current layer is registered. Inside the function, the output is captured and stored in the features array.\n",
    "\n",
    "Lastly, the remove function is called to clear the outputs captured, otherwise this process may result in memory issues.\n",
    "\n",
    "The extract_layers function extracts the outputs for the style and content images. Inside this funciton, we call LayerActivations and pass in the model and the layer numbers. We follow this by ensuring we have an empty list. The an image is passed through the model, and we will review the outputs generated in the features array \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Extracting-the-losses'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create layer activations class for capturing loss at various modelslayer\n",
    "class LayerActivations:\n",
    "    features = []\n",
    "\n",
    "    def __init__(self, model, layer_nums):\n",
    "        self.hooks = []\n",
    "        for layer_num in layer_nums:\n",
    "            self.hooks.append(model[layer_num].register_forward_hook(self.hook_fn))\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features.append(output)\n",
    "\n",
    "    def remove(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "\n",
    "# function for extracting outputs form the images\n",
    "def extract_layers(layers, img, model=None):\n",
    "    la = LayerActivations(model, layers)\n",
    "    la.features = []\n",
    "    out = model(img)\n",
    "    la.remove()\n",
    "    return la.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the outputs needs to be detached from the graphs that created them\n",
    "content_targets = extract_layers(content_layers, content_img, model = vgg)\n",
    "style_targets = extract_layers(style_layers, style_img, model = vgg)\n",
    "\n",
    "content_targets [t.detach() for t in content_targets]\n",
    "style_targets = [GramMatrix(t).detach() for t in style_targets]\n",
    "\n",
    "# add all targets into one list\n",
    "target = style_targets + content_targets\n",
    "\n",
    "# specify layers to be extracted\n",
    "style_layers = [1, 6, 11, 20, 25]\n",
    "content_layers = [21]\n",
    "loss_layers = style_layers + content_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimizer needs a single scalar to minimize, so the losses frome each layer are summed\n",
    "style_weights = [1e3 / n ** 2 for n in [64, 128, 256, 512, 512]]\n",
    "content_weights = [1e0]\n",
    "weights = style_weights + content_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review layers selected\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating loss function for each layers\n",
    "\n",
    "We need to create the loss layers for the separate style losses and content losses. The variable loss_fns is a list containing several style loss objects and content loss objects that are based on the lengths of the arrays created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-loss-function-for-each-layers'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create style loss and content loss objects\n",
    "loss_fns = [StyleLoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the optimizer\n",
    "\n",
    "An optimizer typically receives the parameters of the model, but in this case we are using VGG models as feature extracts, and therefore cannot pass the VGG parameters. Instead, we will provide the parameters of the opt_img variable. These are the parameters that will be optimized to make the image have the required content and style.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-the-optimizer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer object\n",
    "optimizer = optim.LBFGS([opt_img])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This training method will calculate loss for multiple layers. Each time the optimizer is called, it will chang the input image so that the content and style gets nearer to the target's content and style.\n",
    "\n",
    "In the function below, each iteration involves calcualting the output from different layers of the VGG model using extract_layers. The only values that change here are the values of the style image (opt_img). Once the outputs are calculated, we calcualte the losses by iterations through the outputs and passing them to the associated loss functions along with the targets. The losses are summed and the backward function is called.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train process\n",
    "max_iter = 500\n",
    "show_iter = 50\n",
    "n_iter = [0]\n",
    "\n",
    "while n_iter[0] <= max_iter:\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = extract_layers(loss, opt_img, model=vgg)\n",
    "        layer_losses = [\n",
    "            weights[a] * loss_fns[a](A, targets[a]) for a, A in enumerate(out)\n",
    "        ]\n",
    "        loss = sum(layer_losses)\n",
    "        loss.backward()\n",
    "        n_iter[0] += 1\n",
    "\n",
    "        print(loss)\n",
    "\n",
    "        if n_iter[0] % show_iter == (show_iter - 1):\n",
    "            print(\"Iteration: {}, loss: {}\".format(n_iter[0] + 1, loss.data[0]))\n",
    "\n",
    "    return loss\n",
    "\n",
    "optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative adversarial networks\n",
    "\n",
    "In a general sense, GANs address the problem of unsupervise learning by training two deep neural networks - one is called the generator and the other is the discriminator. The networks compete with each other and through that competition, both become better at the tasks they perform.\n",
    "\n",
    "The generator network can also be thought of as the counterfeiter, and the discriminator is the police. The counterfeiter shows the police fake money, and the police identifies it as fake and explains to the counterfeiter why it's fake. With that information in hand, the counterfeiter makes more fake money based on the police feedback. The police again find it to be fake and explains why. This process repeats until the police is unable to recognize the money as fake. The end result is a generator that creates fake images which are quite similar to the real images, and a classifier that is good at identifying a fake image from a real image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Generative-adversarial-networks'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep convolutional GAN\n",
    "\n",
    "Some of the key components of a DCGAN include:\n",
    "\n",
    "- A generator network, which maps a vector of some fixed dimension to images of a certain shape. Our shape will be 3 by 64 by 64\n",
    "\n",
    "- a discriminaotr network, which takes an input image either created by the generator or from the actual dataset, and maps a score estimating if an input image is real or fake\n",
    "\n",
    "- Loss functions for the generator and discriminator\n",
    "\n",
    "- An optimizer\n",
    "\n",
    "- A training pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Deep-convolutional-GAN'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the generator network\n",
    "\n",
    "The generator receives a random vector of a fixed dimension as an input and applies a process of transposed convolutions, batch normalization and ReLU activations. The results in an image with the required size.\n",
    "\n",
    "In the implementation below, the model takes an input of tensor size nz and then passes it on to a transposed convolution which maps the input to the image size that it needs to generate. The forward function moves the input through the sequential module and returns the output.\n",
    "\n",
    "The last layer is a tanh layer, which limits the ranage of values that the network can generate.\n",
    "\n",
    "The model is initialized with weights defined in the paper from which this chapter draws its inspiration, but the weight can also be randomly initialized. In this example, the weight function is passed to the generator object. The weights are intitialized different in the convolution and BatchNorm layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Defining-the-generator-network'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that sets weight in accordance with the reference paper\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal(0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "# class defining discriminator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution step\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True)\n",
    "            # state size = (ngf * 8) by 4 by 4\n",
    "            ,\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True)\n",
    "            # state size = (ngf * 4) by 8 by 8\n",
    "            ,\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True)\n",
    "            # state size = (ngf * 2) by 16 by 16\n",
    "            ,\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True)\n",
    "            # state size = (ngf) by 32 by 32\n",
    "            ,\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(True)\n",
    "            # state size = (nc) by 64 by 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator()\n",
    "netG.apply(weights_init)\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the discriminator network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Defining-the-discriminator-network'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class defining discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution step\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True)\n",
    "            # state size = (ngf * 8) by 4 by 4\n",
    "            ,\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True)\n",
    "            # state size = (ngf * 4) by 8 by 8\n",
    "            ,\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True)\n",
    "            # state size = (ngf * 2) by 16 by 16\n",
    "            ,\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True)\n",
    "            # state size = (ngf) by 32 by 32\n",
    "            ,\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(True)\n",
    "            # state size = (nc) by 64 by 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator()\n",
    "netG.apply(weights_init)\n",
    "print(netG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
