{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 6 - Deep learning with sequence data and text__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Word embedding](#Word-embedding)\n",
    "    1. [Training word embedding by building a sentiment classifier](#Training-word-embedding-by-building-a-sentiment-classifier)\n",
    "    1. [torchtext.datasets](#torchtextdatasets)\n",
    "    1. [Building vocabulary](#Building-vocabulary)\n",
    "    1. [Generate batches of vectors](#Generate-batches-of-vectors)\n",
    "1. [Creating a network model with embedding](#Creating-a-network-model-with-embedding)\n",
    "    1. [Training the model](#Training-the-model)\n",
    "    1. [Using pretrained word embeddings](#Using-pretrained-word-embeddings)\n",
    "    1. [Loading the embeddings in the model](#Loading-the-embeddings-in-the-model)\n",
    "    1. [Freeze the embedding layer weights](#Freeze-the-embedding-layer-weights)\n",
    "1. [Recursive neural networks](#Recursive-neural-networks)\n",
    "    1. [Understanding how RNN works with an example ](#Understanding-how-RNN-works-with-an-example)\n",
    "1. [LSTM](#LSTM)\n",
    "    1. [Preparing the data](#Preparing-the-data)\n",
    "    1. [Creating batches](#Creating-batches)\n",
    "    1. [Creating the network](#Creating-the-network)\n",
    "    1. [Training the model](#Training-the-model2)\n",
    "1. [Convolutional network on sequence data](#Convolutional-network-on-sequence-data)\n",
    "    1. [Creating the network](#Creating-the-network2)\n",
    "    1. [Training the model](#Training-the-model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# pytorch tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Word embedding\n",
    "\n",
    "Word embedding is a popular way of representing text data in problems that are solved by deep learning algorithms. This technique provides a dense representation of a word filled with floats. The vector dimension varies based on the vocabulary size. It is common to use a word emebedding of dimension size 50, 100, 256, 300 and occassionally 1,000. This size is a hyperparameter.\n",
    "\n",
    "Contrasting this with on-hot encoding, if we have a vocabulary of 20,000 words, then we end up with 20,000 x 20,000 numbers, the vast majority of which will be zero. This same vocabulary can be represented as a word emebedding of size 20,000 x (dimension size).\n",
    "\n",
    "One method for creating word embeddings is to start with dense vectors of random numbers for each token, then train a model (such as a document classifier or sentiment classifier). The floating point numbers in the vectors, which collectively represent the tokens, are adjusted in a way such that semantically 'close' words will have similar represented.\n",
    "\n",
    "Word embeddings may not be feasible if there isn't enough data. In these case, embeddings trained by some other machine learning algorithm can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Word-embedding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Training word embedding by building a sentiment classifier\n",
    "\n",
    "Using a dataset called IMDB (which contains movie reviews), we will build a sentiment classifier. In the processing training the model, we will also train word embedding for the words in the IMDB dataset. This will be done using a library called torchtext.\n",
    "\n",
    "The torchtext.data module has a class called Field, which defines how the data needs to be read and tokenized. Below, we define two Field objects, one for the text itself and a second for the labels. The Field constructor also accepts a tokenize argument, which by default use the str.split function. We can override this by passing in a tokenizer of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Training-word-embedding-by-building-a-sentiment-classifier'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "from torchtext import data\n",
    "\n",
    "text = data.Field(lower=True, batch_first=True, fix_length=20)\n",
    "label = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## torchtext.datasets\n",
    "\n",
    "torchtext.datasets provides wrappers for several different datasets, such as IMDB. This utility abstracts away the process of downloading, tokenizing and splitting the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'torchtextdatasets'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# download IMDB\n",
    "train, test = datasets.IMDB.splits(text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "print(\"train.fields\", train.fields)\n",
    "\n",
    "# results\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Building vocabulary\n",
    "\n",
    "We can use the build_vocab method to take in an object from which we will build a vocabulary. Below, we pass in the train object, and using the dim argument, initialize vectors with pretrained mebeddings of dimension 300. The max_size instance limits the number of words in the vocabulary, and min_freq removes any word which has not occurred more than 10 times.\n",
    "\n",
    "Once the vocabulary is built we can obtain different values such as frequency, word index and the vector representation of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Building-vocabulary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# build the vocabulary\n",
    "text.build_vocab(train, vectors=GloVe(name=\"6B\", dim=300), max_size=10000, min_freq=10)\n",
    "label.build(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print word frequencies\n",
    "print(text.vocab.freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print word vectors, which displays the 300 dimension vector for each word\n",
    "print(text.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print word and their indexes\n",
    "print(text.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Generate batches of vectors\n",
    "\n",
    "BucketIterator is a tools that helps to batch the text and replace the words with the index number of the individual words. The following code creates iterators that generate batches for the train and test objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Generate-batches-of-vectors'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "train_iter, test_iter = data.BuckerIterator.splits(\n",
    "    (train, test), batch_size=18, device=-1, shuffle=True\n",
    ")\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "print(batch.text)\n",
    "\n",
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a network model with embedding\n",
    "\n",
    "In this section we will create word embeddings in our network architecture, and then train the entire model to predict the sentiment of each review. Once training is complete, we will have a sentiment classifier model as well as the word embeddings for the IMDB dataset.\n",
    "\n",
    "In the following code, the init function initializes an object of the nn.Embedding class, which takes two arguments. emb_size is the size of the vocabulary, and hiddensize1 is the dimension we want to create for each word. We will set the vocabulary size at 10,000 and the embedding size (hidden_state1) at size 10. As a side note, small embeddings are great for speed, but production systems typically use much large embeddings. The last item in the init function is a linear layer that maps the word embeddings to the sentiment decision category: positive, negative, unknown.\n",
    "\n",
    "The forward function determines how th einput is processed. When the batch size is 32 and the sentences have a max length of 20 words, so the inputs will have a shape of 32 by 20. The first embedding layer acts as a lookup table which replaces each word with the corresponding embedding vector. When the embedding dimension size is 10, the output becomes 32 by 20 by 10 after each word is replaced with the corresponding embedding. The view() function flattens the result from the embedding layer. The first argument given to view() will keep the dimensions intact. Since we're not interesting in combining data from the different batches, we have the view() preserve the first dimension and flatten the rest of the value in the tensor. Following view(), the tensor shape is now 32 by 200. Lastly, a dense layer maps the flattened embeddings to the output categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-a-network-model-with-embedding'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a network architecture to predic sentiment using word embeddings\n",
    "class EmbNet(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size1, hidden_size2=400):\n",
    "        seuper().__init__()\n",
    "        self.embedding = nn.Embedding(emb_size, hidden_size1)\n",
    "        self.fc = nn.Linear(hidden_size2, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x).view(x.size(0), -1)\n",
    "        out = self.fc(embeds)\n",
    "        return F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-the-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training the model\n",
    "def fit(epoch, model, data_loader, phase=\"training\", volatile=False):\n",
    "    if phase == \"training\":\n",
    "        model.train()\n",
    "    if phase == \"validation\":\n",
    "        model.eval()\n",
    "        volatile = True\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        text, target = batch.text, batch.label\n",
    "        if torch.cuda.is_available():\n",
    "            text, target = text.cuda(), target.cuda()\n",
    "\n",
    "        if phase == \"training\":\n",
    "            optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        running_loss += F.nll_loss(output, target, reduction=\"sum\").data.item()\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == \"training\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100.0 * running_correct / len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"{0} loss is {1} and {0} accuracy is {2}/{3} {4}\".format(\n",
    "            phase, loss, running_correct, len(data_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model for 20 epochs\n",
    "model = EmbNet()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "\n",
    "# batch iterator by default does not stop generating batches, so the repeat variable\n",
    "# object need to be set to False. Otherwise the training process will run indefinitely.\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False\n",
    "\n",
    "# 10 epochs gives a validation accuracy of around 70%\n",
    "for epoch in range(1, 10):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, model, train_iter, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, model, test_iter, phase=\"validation\"\n",
    "    )\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pretrained word embeddings\n",
    "\n",
    "Pretrained word embeddings can be particularly helpful when working within a specific domain, such as medicine. There are pretrained embeddings that have been trained on massive corpuses, such as Wikipedia, Google News and Twitter tweets. We can use torchtext to easily access these resources. This process works similar to transfer learning in the context image classification.\n",
    "\n",
    "- Download the embeddings\n",
    "- Load the embeddings into the model\n",
    "- Freeze the embedding layer weights\n",
    "\n",
    "torchtext provides three class, GloVe, FastText and CharNGram in the vocab module, which facilitates the downloading of the embeddings, and then maps them to our vocabulary.\n",
    "\n",
    "The vectors argument denote which embedding class to used and the name and dim arguments determine which embeddings to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Using-pretrained-word-embeddings'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The build_vocab method of the Field object take an argument specifying the embedding\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "TEXT.build_vocab(train, vectors=GloVe(name=\"6B\", dim=300), max_size=1000, min_freq=10)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the embeddings from the vocab object\n",
    "TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the embeddings in the model\n",
    "\n",
    "The vectors variable returns a torch tensor with the shape of vocab_size by dimensions containing the pretrained embeddings. We need to store the embeddings in our embedding layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Loading-the-embeddings-in-the-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store embeddings as the weight in the embedding layer by accessing the weights of the embedding layer\n",
    "# mdel represents the model object, embedding represents the embedding layer\n",
    "\n",
    "model.embedding.weight.data = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings architecture\n",
    "class EmbNet(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size1, hiddensize2=400):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(emb_size, hidden_size1)\n",
    "        self.fc1 = nn.Linear(hiddensize2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x).view(x.size(0), -1)\n",
    "        out = self.fc1(embeds)\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "\n",
    "model = EmbNet(len(TEXT.vocab.stoi), 300, 12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze the embedding layer weights\n",
    "\n",
    "Freezing the embedding layer is a two step process:\n",
    "\n",
    "1. Set requires_grad to False\n",
    "2. Prevent the embedding laye rparameters to the optimizer.\n",
    "\n",
    "Up to this point, this architecture doesn't take advantage of the sequential nature of text data. The following sections will explore RNN and Conv1D, which do take advantage of text data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Freeze-the-embedding-layer-weights'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn of gradients and create optimizer object\n",
    "model.embedding.weight.requires_grad = False\n",
    "optimizer = optim.SGD([param for param in model.parameters() if param.requires_grad = True], lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive neural networks\n",
    "\n",
    "Feedforward networks are designed to look at all features at once and map them to the output. RNNs, by contrast, evaluate elements on at a time, retaining information evaluated up to that point in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Recursive-neural-networks'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding how RNN works with an example\n",
    "\n",
    "Let's use this string of text as an example: \"the action scenes were top notch in this movie.\"\n",
    "\n",
    "We start by passing the word 'the' into the emodel, and the model generates two different things:\n",
    "\n",
    "- State vector - This is passed to the model when it processes the next word in the input string.\n",
    "- Output vector - The output of the model is reviewed once the last item of the sequence is evaluated.\n",
    "\n",
    "In other words, the RNN recurseively passes the State vector to itself as it moves from item to item in the data sequence.\n",
    "\n",
    "In the implementation below, the init function initializes two linear layers, one for calculating the output and another for calculating the state/hidden vector. The forward function combines the input and hidden vectors, and passes it through the two linear layers, which generates an output and state/hidden vector. The log_softmax function is applied in the output layer. The initHidden function is a helper funciton that creates hidden vectors with no state needed when calling the RNN the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Understanding-how-RNN-works-with-an-example'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 'hidden' variable represents the state vector\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "for i in range(len(thor_review)):\n",
    "    output, hidden = rnn(thor_review[i], hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN implementation\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "The vanilla implementation of an RNN above is rarely used in practice due to issues with vanishing gradients and gradient explosion. Instead, LSTM or GRU are used to address these issues that arise when dealing with large sequences of data. Generally speaking, LSTMs and other variants of RNN more successfully capture meaning in long sequences of data by addining different neural networks inside the LSTM which decide data gets remembered and which date is forgotten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'LSTM'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "RNN networks expect data to be in the form Sequence_length, batch_size and features. In the preparation step below, batch_first needs to be set to False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Preparing the data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "TEXT = data.Field(lower=True, fix_length=200, batch_first=False)\n",
    "LABEL = data.Field(sequential=False)\n",
    "train, test = IMDB.splits(TEXT, LABEL)\n",
    "TEXT.build_vocab(train, vectors=GloVe(name=\"6B\", dim=300), max_size=10000, min_frew=10)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating batches\n",
    "\n",
    "BuckerIterator is used for creating batches, and the size of the batches is equal to the seuqnce length and batches. In this case, the size will be 200 by 32, where 200 is the sequence length and 32 is the batch size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-batches'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch iterator object\n",
    "train_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, test), batch_size=32, device=-1\n",
    ")\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network\n",
    "\n",
    "In the implementation below, the init method creates an embedding layer with the size of n_vocab by hidden_size. It also creates LSTM and linear layer. The last layer is a LogSoftmax layer that converts results from the linear layer to probabilities. \n",
    "\n",
    "The forward function receives and input dataset of size 200 by 32, which gets passed through the embedding layer. Each token in the batch gets replaced by embeddings and the size transforms to 200 by 32 by 100. The dimension with size 100 represents the embeddings. The LSTM layer takes the output of the embedding layer along with two hidden variables. The hidden variables are of the same type as the embeddings output and are of the size num_layers by batch_size by hidden_size. The LSTM layer process the data in a sequence and generates an output of shape sequence_length by batch_size by hiden_size, where each sequence index represents the output of that sequence. In this implementation we are only interested in the output of the last sequence, which has the shape batch_size by hidden_dim, and this is passed on to a linear layer, where it is mapped to the output categories. The droppout layer is includes to fend off overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-the-network'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN for IMDB dataset\n",
    "class IMDBRnn(nn.Module):\n",
    "    def __init__(self, vocab, hidden_size, n_cat, bs=1, n1=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bs = bs\n",
    "        self.n1 = n1\n",
    "        self.e = nn.Embedding(n_vocab, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, n1)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_cat)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        bs = inp.size()[1]\n",
    "        if bs != self.bs:\n",
    "            self.bs = bs\n",
    "        e_out = self.e(inp)\n",
    "        h0 = c0 = Variable(\n",
    "            e_out.data.new(*(self.n1, self.bs, self.hidden_size)).zero_()\n",
    "        )\n",
    "        rnn_o, _ = self.rnn(e_out, (h0, c0))\n",
    "        rnn_o = rnn_o[-1]\n",
    "        fc = F.dropout(self.fc2(rnn_o), p=0.8)\n",
    "        return self.softmax(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-the-model2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = IMDBRnn(n_vocab, n_hidden, bs=32)\n",
    "model = model.cuda()\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# model fit function\n",
    "def fit(epoch, model, data_loader, phase=\"training\", volatile=False):\n",
    "    if phase == \"training\":\n",
    "        model.train()\n",
    "    if phase == \"validation\":\n",
    "        model.eval()\n",
    "        volatile = True\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        text, target = batch.text, batch.label\n",
    "        if is_cuda:\n",
    "            text, target = text.cuda(), target.cuda()\n",
    "\n",
    "        if phase == \"training\":\n",
    "            optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        running_loss += F.nll_loss(output, target, size_average=False).data.item()\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == \"training\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss, accuracy = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100.0 * running_correct / len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"{0} loss is {1} and {0} accuracy is {2}/{3} {4}\".format(\n",
    "            phase, loss, running_correct, len(data_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "# execute training loop\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 5):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, model, train_iter, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, model, test_iter, phase=\"validation\"\n",
    "    )\n",
    "    train.losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional network on sequence data\n",
    "\n",
    "Just as CNNS can be used in computer vision problems for images, convolutions can also be helpful in model sequential data. One-dimensional convolutions sometimes perform better than RNNs and are computationally cheaper.\n",
    "\n",
    "The convolution operation shares similarities with the technique's application to images. There is a kernel with weights of a set length that slides along the sequence of data, returning an abstraction that results from the vector multiplication. The original sequence can be padded just as we did with images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Convolutional-network-on-sequence-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network\n",
    "\n",
    "In the implementation below, we replace the LSTM layer with a Conv1d layer and an AdaptiveAvgPool1d layer. The convolution layer accepts the sequence length as its input size, and the output size as the hidden layer size, and the kernel size defaults tp 3. AdaptiveAvgPool1d is used to ensure that the input into the linear layer is of a fixed size. AdaptiveAvgPool1d takes an input of any size and generates an output of a given size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-the-network2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB CNN\n",
    "class IMDBCnn(nn.Module):\n",
    "    def __init__(self, vocab, hidden_size, n_cat, bs=1, kernel_size=3, max_len=200):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bs = bs\n",
    "\n",
    "        self.e = nn.Embedding(n_vocab, hidden_size)\n",
    "        self.cnn = nn.Conv1d(max_len, hidden_size, kernel_size)\n",
    "        self.avg = nn.AdaptiveAvgPool1d(10)\n",
    "        self.fc = nn.Linear(1000, n_cat)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        bs = inp.size()[0]\n",
    "        if bs != self.bs:\n",
    "            self.bs = bs\n",
    "        e_out = self.e(inp)\n",
    "        cnn_o = self.cnn(e_out)\n",
    "        cnn_avg = self.avg(cnn_o)\n",
    "        cnn_avg = cnn_avg.view(self.bs, -1)\n",
    "        fc = F.dropout(self.fc(cnn_avg), p=0.5)\n",
    "        return self.softmax(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-the-model3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = IMDBCnn(n_vocab, n_hidden, bs=32)\n",
    "model = model.cuda()\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# model fit function\n",
    "def fit(epoch, model, data_loader, phase=\"training\", volatile=False):\n",
    "    if phase == \"training\":\n",
    "        model.train()\n",
    "    if phase == \"validation\":\n",
    "        model.eval()\n",
    "        volatile = True\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        text, target = batch.text, batch.label\n",
    "        if is_cuda:\n",
    "            text, target = text.cuda(), target.cuda()\n",
    "\n",
    "        if phase == \"training\":\n",
    "            optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        running_loss += F.nll_loss(output, target, size_average=False).data.item()\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == \"training\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss, accuracy = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100.0 * running_correct / len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"{0} loss is {1} and {0} accuracy is {2}/{3} {4}\".format(\n",
    "            phase, loss, running_correct, len(data_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "# execute training loop\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 5):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, model, train_iter, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, model, test_iter, phase=\"validation\"\n",
    "    )\n",
    "    train.losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
