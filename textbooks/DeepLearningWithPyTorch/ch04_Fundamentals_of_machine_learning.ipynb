{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 4 - Fundamentals of machine learning__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Supervised learning](#Supervised-learning)\n",
    "1. [Unsupervised learning](#Unsupervised-learning)\n",
    "1. [Reinforcement learning](#Reinforcement-learning)\n",
    "1. [Overfitting](#Overfitting)\n",
    "    1. [Reducing the size of the network](#Reducing-the-size-of-the-network)\n",
    "    1. [Applying weight regularization](#Applying-weight-regularization)\n",
    "    1. [Dropout](#Dropout)\n",
    "1. [Learning rate picking strategies](#Learning-rate-picking-strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T12:27:05.403250Z",
     "start_time": "2019-03-25T12:27:02.647480Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# pytorch tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "\n",
    "PyTorch can be applied to supervised learning problems just like popular machine learning libraries such as scikit-learn. Here is a brief list of problem types and generic examples:\n",
    "\n",
    "-__Classification__ - classifying dogs and cats based on images.\n",
    "\n",
    "-__Regression__ - predicting stock prices. \n",
    "\n",
    "-__Image segmentation__ - pixel-classification. as a specific example, it is important for a self-driving car to identify what each pixel belongs to so that it can identify where various objects exist/begin/end, such as other cars, pedestrians, trees, and so on.\n",
    "\n",
    "-__Speech recognition__ - Alexa, Siri, OK Google.\n",
    "\n",
    "-__Language translation__ - translating speech form one language to another language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Supervised-learning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "When there are no labels in the data, the task is considered to be unsuperivsed. two commonly-used techniques are:\n",
    "\n",
    "- __Clustering__ - groups similar data points / observations together\n",
    "- __Dimensionality reduction__ - reduces the number of dimensions so that we can evaluate / visualize high-dimensional data to discover any hidden patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Unsupervised-learning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "An application of reinforcement learning that received mainstream news coverage recently is Google's DeepMind project called AlphaGo that defeated the Go world champion. This topic will not be covered in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Reinforcement-learning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "Overfitting occurs when a model performs well on a training dataset but generalizes poorly to unseen data. Neural networks can be adjusted in several differt ways to attmpt addressing overfitting issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Overfitting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the size of the network\n",
    "\n",
    "Removing certain layers from an unnecessarily deep network can prevent the network from memorizing the training data. Training data memorization typically results in poor performance on validation/unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Reducing-the-size-of-the-network'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T12:27:10.933494Z",
     "start_time": "2019-03-25T12:27:10.921839Z"
    }
   },
   "outputs": [],
   "source": [
    "# excessively deep network\n",
    "class Architecture1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Architecture1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# same network as above, but one ReLU layer and one linear layer has been removed, which may be less likely to overfit\n",
    "class Architecture2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Architecture2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying weight regularization\n",
    "\n",
    "Regularization reduces the complexity of model by penalizing large weights. PyTorch provides an easy way to utilize l2/ridge regularization through the weight_decay parameter in the optimizer. by default weight_decay is set to 0.\n",
    "\n",
    "```python\n",
    "model = Architecture1(10, 20, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4, weight_decay = le-5)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Applying-weight-regularization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T12:28:12.229639Z",
     "start_time": "2019-03-25T12:28:12.220344Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate model and create optimizer\n",
    "model = Architecture1(10, 20, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout is a oommonly used and powerful regularization in deep learning. It is a technique applied to intermediate layers in the model during the training phase. A threshold value is set (between 0 and 1), and then dropout randomly masks/zeros-out a proportion of weights equal to that threshold. A higher dropout threshold applies heavier regularization. Common value are between 0.2 and 0.5. Dropout is only applied during training, and during the testing phase values are scaled down by a factor equal to the dropout.\n",
    "\n",
    "Here is a snippet illustrating how PyTorch implements dropout as an additional layer:\n",
    "\n",
    "```python\n",
    "nn.dropout(x, training = True)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Dropout'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate picking strategies\n",
    "\n",
    "Tuning the learning rate is something that can be pursued using the torch.optim.lr_scheduler package.\n",
    "\n",
    "__StepLR__ - This scheduer takes two paramters: step size, which denotes the number of epochs where the learning rate needs to change, and gamma, which decides how much the learning rates changes. For example, for a learning of 0.01, with a step size of 10 and a gamme of 0.1, then for the first 10 epochs the learning rates changes to 0.001, and by the last of learning rate has changed to 0.0001. Her followis an example framework:\n",
    "\n",
    "```python\n",
    "scheduler = StepLR(optimizer, step_size = 30, gamma = 0.1)\n",
    "for epoch in range(100)\n",
    "    scheduler.step()\n",
    "    train(...)\n",
    "    validate(...)\n",
    "```\n",
    "\n",
    "- __MultiStepLR__ - This works similarly to StepLR, except for the fact that the steps are not at regular intervals. Steps are given as lists. If the list given is [10, 15, 30], this will control when the learning rate changes. The learning rate is multiplied by its gamma value. Here is an example.\n",
    "\n",
    "```python\n",
    "scheduler = MultiStepLR(optimizer, milestones = [10, 15, 30], gamma = 0.1)\n",
    "for epoch in range(1000):\n",
    "    scheduler.step()\n",
    "    train(...)\n",
    "    validate(...)\n",
    "```\n",
    "\n",
    "__ExponentialLR__ - This sets the learning rate to a multiple of the learning rate with gamma values for each epoch.\n",
    "\n",
    "__ReduceLROnPlateau__ - This is a commonly used learning rate tuning strategy where the learning rate changes when a particular metric, such as training loos, validation loss or accuracy begins to stagnate. It is a common practice to reduce the learning rate by 2 to 1- times its original value. An example:\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "for epoch in range(10:\n",
    "    train(...)\n",
    "    val_loss = validate(...)\n",
    "    scheduler.step(val_loss)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Learning-rate-picking-strategies'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
