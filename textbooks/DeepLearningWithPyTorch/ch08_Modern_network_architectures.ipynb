{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  __Chapter 8 - Modern network architectures__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Modern network architectures](#Modern-network-architectures)\n",
    "    1. [ResNet](#ResNet)\n",
    "        1. [Creating PyTorch datasets](#Creating-PyTorch-datasets)\n",
    "        1. [Creating loaders for training and validation](#Creating-loaders-for-training-and-validation)\n",
    "        1. [Creating a ResNet model](#Creating-a-ResNet-model)\n",
    "        1. [Extracting convolutional features](#Extracting-convolutional-features)\n",
    "        1. [Creating a custom PyTorch dataset class for the pre-convoluted features and loader](#Creating-a-custom-PyTorch-dataset-class-for-the-pre-convoluted-features-and-loader)\n",
    "        1. [Training and validating the model](#Training-and-validating-the-model)\n",
    "    1. [Inception](#Inception)\n",
    "        1. [Creating an Inception model](#Creating-an-Inception-model)\n",
    "        1. [Extracting convolutional features using register_forward_hook](#Extracting-convolutional-features-using-register_forward_hook)\n",
    "        1. [Creating a new dataset for the convoluted features](#Creating-a-new-dataset-for-the-convoluted-features)\n",
    "        1. [Creating a fully connected model](#Creating-a-fully-connected-model)\n",
    "        1. [Training and validating the model](#Training-and-validating-the-model2)\n",
    "    1. [DenseNet](#DenseNet)\n",
    "        1. [Creating a DenseNet model](#Creating-a-DenseNet-model)\n",
    "        1. [Extracting DenseNet features](#Extracting-DenseNet-features)\n",
    "        1. [Creating a dataset and loaders](#Creating-a-dataset-and-loaders)\n",
    "    1. [Model ensembling](#Model-ensembling)\n",
    "        1. [Creating models](#Creating-models)\n",
    "        1. [Extracting the image features](#Extracting-the-image-features) \n",
    "        1. [Creating a custom dataset along with data loaders](#Creating-a-custom-dataset-along-with-data-loaders)\n",
    "        1. [Creating an ensembling model](#Creating-an-ensembling-model) \n",
    "        1. [Training and validating the ensemble model](#Training-and-validating-the-ensemble-model)        \n",
    "        1. [](#)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# pytorch tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern network architecture\n",
    "\n",
    "Adding layers to the model can add to its predictive abilities, but also introduces the possiblility of other problem, such as vanishing/exploding gradients. Modern architectures try to solve these problem by introducing different techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modern-network-architecture'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "\n",
    "ResNet approaches these issues by enabling layers in the network to fit to the residuals. In a typical network, we fit a model to find a function that maps the input $x$ to its output $H(x)$ by stacking different layers. ResNet, instead of trying to learn a mapping from $x$ to $H(x)$, tries to learn the difference between the two (aka the residual). To calculate $H(x)$, we add the residual to the input. If the residual is $F(x) = H(x) - x$, then we don't need to learn $H(x)$ directly. Instead, we try to learn $F(x) + x$.\n",
    "\n",
    "Each ResNet block is comprised of several layers an da shortcut connection that adds the input of the block to the output of the block. The addition operation is performed element-wise, so the inputs and outputs need to be the same size. If the objects are not the same size naturally then we can use padding.\n",
    "\n",
    "In the example below, the init method initializes all of the different layers, and the forward method is very similar to implementation seen so far, except that the input is being adding back to the layer's output before returning it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'ResNet'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet block demonstration\n",
    "class ResNetBasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.atchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.atchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residuals = x\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(self.bn1(out), inplace=True)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating PyTorch datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-PyTorch-datasets'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets from respective image folders\n",
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = ImageFolder(\"../../kaggleDogsVsCats/data/train\", transforms=data_transform)\n",
    "val_data = ImageFolder(\"../../kaggleDogsVsCats/data/valid\", transforms=data_transform)\n",
    "classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating loaders for training and validation\n",
    "\n",
    "The exact sequence of the data need to be maintained in order to facilitate calculating the pre-convoluted features. If the data gets shuffled, then the labels are not maintained. Therefore, it is important to ensure that the shuffle argument is set to False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-loaders-for-training-and-validation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "train_loader = DataLoader(train_dset, batch_size=32, shuffle=False, num_workers=3)\n",
    "val_loader = DataLoader(val_dset, batch_size=32, shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a ResNet model\n",
    "\n",
    "The nn.Sequential instance enables the rapid creation of a model using a set of PyTorch layers. It is important to set requires_grad to False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-a-ResNet-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup ResNet 34 model\n",
    "resnet_model = models.resnet34(pretrained=True)\n",
    "\n",
    "# set device\n",
    "if is_cuda:\n",
    "    resnet_model = resnet_model.cuda()\n",
    "\n",
    "# discard the last linear layer\n",
    "resnet_model = nn.Sequential(*list(resnet_model.children())[:-1])\n",
    "\n",
    "# turn of gradients\n",
    "for p in resnet_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting convolutional features\n",
    "\n",
    "Calculating the pre-convouted features can save substantial time in the model training stage. This avoids having to calcualte the features in every iteration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Extracting-convolutional-features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the training data labels\n",
    "trn_labels = []\n",
    "\n",
    "# store the pre-convoluted features of the training data\n",
    "trn_featuers = []\n",
    "\n",
    "# iterate through training data, store the calculated featuers and the lables\n",
    "for d, la in train_loader:\n",
    "    o = m(Variable(d.cuda()))\n",
    "    o = o.view(o.size(0), -1)\n",
    "    trn_labels.extend(la)\n",
    "    trn_featuers.extend(o.cpu().data)\n",
    "\n",
    "# iterate through validation data, store the calculated featuers and the lables\n",
    "val_labels = []\n",
    "val_featuers = []\n",
    "for d, la in val_loader:\n",
    "    o = m(Variable(d.cuda()))\n",
    "    o = o.view(o.size(0), -1)\n",
    "    val_labels.extend(la)\n",
    "    val_featuers.extend(o.cpu().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom PyTorch dataset class for the pre-convoluted features and loader\n",
    "\n",
    "With the pre-convoluted features in hand, we need to create a custom data set that can select from the pre-convoluted features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-a-custom-PyTorch-dataset-class-for-the-pre-convoluted-features-and-loader'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8b58f7c481eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mFeatureDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatlst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabellst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatlst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatlst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabellst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabellst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# custom dataset class\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, featlst, labellst):\n",
    "        self.featlst = featlst\n",
    "        self.labellst = labellst\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.featlst[index], self.labellst[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labellst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset for train and validation\n",
    "trn_feat_dset = FeaturesDataset(trn_features, trn_labels)\n",
    "val_feat_dset = FeaturesDataset(val_features, val_labels)\n",
    "\n",
    "# creating data loader for train and validation\n",
    "trn_feat_loader = DataLoader(trn_feat_dset, batch_size=64, shuffle=True)\n",
    "val_feat_loader = DataLoader(val_feat_dset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a simple linear model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-a-simple-linear-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full connected model\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_size, out_size)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.fc(inp)\n",
    "        return out\n",
    "\n",
    "\n",
    "fc_in_size = 8192\n",
    "\n",
    "fc = FullyConnectedModel(fc_in_size, classes)\n",
    "if is_cuda:\n",
    "    fc = fc.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validating the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-and-validating-the-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training and validation loop\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 10):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, fc, trn_feat_loader, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, fc, val_feat_loader, phase=\"validation\"\n",
    "    )\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception\n",
    "\n",
    "The Inception model combines convolutions of different filter sizes and concatenates all of the outputs. The various convolutions of different sizes are applied to the input. This is the simplest variant of Inception. There is a more complicated variant where the input is passed through a 1 by 1 convolution prior to being passed through a 3 by 3 and 5 by 5 convolutions. The 1 by 1 convolution is used for dimensionality reduction, addressing computational bottlebecks. 1 by 1 convolutions evaluate one value at a time across all channels. For example a 10 by 1 by 1 filter on an input of 100 by 64 by 64 results in a 10 by 64 by 64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Inception'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN helper class\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicCOnv2, self).__init__()\n",
    "        super.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.atchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception model class\n",
    "class InceptionBasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, pool_featuers):\n",
    "        super().__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, pool_featuers, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # applies a 1 by 1 conv\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        # 1 by 1 conv followed by a 5 by 5 conv\n",
    "        branch5x5 = self.branch3x3dbl_1(x)\n",
    "        branch5x5 - self.branch3x3dbl_2(branch5x5)\n",
    "\n",
    "        # 1 by 1 conv followed by a 3 by 3 conv\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl_1)\n",
    "\n",
    "        # average max pool followed by 1 by 1 conv\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        # concatenate output\n",
    "        outputs = (branch1x1, branch5x5, branch3x3dbl, branch_pool)\n",
    "        return torch.cat(ouputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Inception model\n",
    "\n",
    "The Inception v3 model has two branches, and each creates an output, and the loss of each branch gets merged together. In this implementation, we will only use one branch to calculate pre-convoluted features. The process to do so is less straightforward with Inception when compared to ResNet.\n",
    "\n",
    "Below, we disable one of the branches by setting aux_logits to False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-an-Inception-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Inception v#\n",
    "inceptionModel = models.torchvision_v3(pretrained=True)\n",
    "\n",
    "# disabl aux_logits\n",
    "inceptionModel.aux_logits = False\n",
    "\n",
    "# set device\n",
    "if is_cuda:\n",
    "    inceptionModel = inceptionModel.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting convolutional features using register_forward_hook\n",
    "\n",
    "The techniques in this section are similar to how we calculated activations for style transfer.\n",
    "\n",
    "Since we will be capturing outputs of all the images and storing them, we cannot use the GPU, so this class moves the tensors to the CPU.\n",
    "\n",
    "The execution of the process involves extracting the output of the Inception model at the last layer, and we exclude the average pooling layer, dropout and linear layer. Pooling is skipped to avoid losing information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Extracting-convolutional-features-using-register_forward_hook'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for extracting CNN layers\n",
    "class LayerActivations:\n",
    "    features = []\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.features = []\n",
    "        self.hook = model.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features.extend(output.view(output.size(0), -1).cpu().data)\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "# create LayerActivations object to store inception model output at a particular layer\n",
    "trn_features = LayerActivations(inceptionModel.Mixed_7c)\n",
    "trn_labels = []\n",
    "\n",
    "#\n",
    "for da, la in train_loader:\n",
    "    _ = inceptionModel(Variableda.cuda())\n",
    "    trn_labels.extend(la)\n",
    "trn_features.remove()\n",
    "\n",
    "# repeat for validation\n",
    "val_features = LayerActivations(inceptionModel.Mixed_7c)\n",
    "val_labels = []\n",
    "for da, la in val_loader:\n",
    "    _ = inceptionModel(Variableda.cuda())\n",
    "    val_labels.extend(la)\n",
    "val_features.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new dataset for the convoluted features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-a-new-dataset-for-the-convoluted-features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for pre computed features for train and validation data sets\n",
    "trn_feat_dset = FeaturesDataset(trn_features.features, trn_labels)\n",
    "val_feat_dset = FeaturesDataset(val_features.features, val_labels)\n",
    "\n",
    "# data loaders for pre computed features for train and validation data sets\n",
    "\n",
    "trn_feat_loader = DataLoader(trn_feat_dset, batch_size=64, shuffle=True)\n",
    "val_feat_loader = DataLoader(val_feat_dset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a fully connected model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-a-fully-connected-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connected model\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, in_size, out_size, training=True):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_size, out_size)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = F.dropout(inp, training=self.training)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# The size of the output from the selected convolution feature\n",
    "fc_in_size = 131072\n",
    "\n",
    "# instantiate fully connected model\n",
    "fc = FullyConnectedModel(fc_in_size, classes)\n",
    "if is_cuda:\n",
    "    fc = fc.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validating the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training an--validating-the-model2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training loop\n",
    "for epoch in range(1, 10):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, fc, trn_feat_loader, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, fc, val_feat_loader, phase=\"validation\"\n",
    "    )\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet\n",
    "\n",
    "DenseNet is a modern architecture that forms connections from each layer to all layers that follow. This means that layers receives all feature maps from all preceding layer, i.e.\n",
    "\n",
    "$$\n",
    "X_l = H_l(x_0, x_1, x_2,...,x_{l-1})\n",
    "$$\n",
    "\n",
    "where $X_l$ is the layer of interest and $H_1$ is the collection of feature maps up to that point in the model.\n",
    "\n",
    "Below, DenseBlock is a sequential module where layers are added in a sequential order. num_layers controls the number of objects DenseLayer objects that are added, and each is given a name.\n",
    "\n",
    "In DenseLayer, the init method adds all layers that the input data needs to be passed to. the forward method is where the forward passing of kernels from previous layers occurs. The input is passed to the forward method of the _super_ class in nn.Sequential, which is shown below:\n",
    "\n",
    "```python\n",
    "def forward(self, input):\n",
    "    for module in self._modules.values():\n",
    "        input = module(input)\n",
    "    return input\n",
    "```\n",
    "\n",
    "The input is passed through all of the layers that were previously added to the sequential block, and then the output is concatenated to the input. This process is repeated for the specified number of layers in the block. \n",
    "\n",
    "Some of the advantages of DenseNet are:\n",
    "- Substantially reduces the number of parameters required\n",
    "- Alleviates the vanishing gradient problem\n",
    "- Encourages features reuse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'DenseNet'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DenseBlock class\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + 1 * growth_rate, growth_rate, bn_size, drop_rate\n",
    "            )\n",
    "            self.add_module(\"denselayer%d\" % (i + 1), layer)\n",
    "\n",
    "\n",
    "# create DenseLayer class\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module(\"norm.1\", nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module(\"relu.1\", nn.ReLU(inplace=True))\n",
    "        self.add_module(\n",
    "            \"conv.1\",\n",
    "            nn.Conv2d(\n",
    "                num_input_features,\n",
    "                bn_size * growth_rate,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.add_module(\"norm.2\", nn.BatchNorm2d(bn_size * growth_rate))\n",
    "        self.add_module(\"relu.2\", nn.ReLU(inplace=True))\n",
    "        self.add_module(\n",
    "            \"conv.2\",\n",
    "            conv2d(\n",
    "                bn_size * growth_rate,\n",
    "                growth_rate,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(\n",
    "                new_features, p=self.drop_rate, training=self.training\n",
    "            )\n",
    "        return torch.cat([x, new_features], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DenseNet model\n",
    "\n",
    "PyTorch's implementation of DenseNet has two moduels: features, which contains the dense blocks, and classifier, which contains the fully connected block. We will only be using DenseNet as an image feature extractor so we only need to use the feature module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-a-DenseNet-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate DenseNet121\n",
    "model_densenet = models.densenet121(pretrained=True).features\n",
    "if is_cuda:\n",
    "    model_densenet = model_densenet.cuda()\n",
    "\n",
    "# turn of gradients\n",
    "for p in model_densenet.parameters():\n",
    "    p.required._grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting DenseNet features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Extracting-DenseNet-features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "trn_labels = []\n",
    "trn_features = []\n",
    "\n",
    "# code to store densenet features for train dataset.\n",
    "for d, la in train_loader:\n",
    "    o = my_densenet(Variable(d.cuda()))\n",
    "    o = o.view(o.size(0), -1)\n",
    "    trn_labels.extend(la)\n",
    "    trn_features.extend(o.cpu().data)\n",
    "\n",
    "# For validation data\n",
    "val_labels = []\n",
    "val_features = []\n",
    "\n",
    "# code to store densenet features for validation dataset.\n",
    "for d, la in val_loader:\n",
    "    o = my_densenet(Variable(d.cuda()))\n",
    "    o = o.view(o.size(0), -1)\n",
    "    val_labels.extend(la)\n",
    "    val_features.extend(o.cpu().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset and loaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-a-dataset-and-loaders'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for train and validation convolution features\n",
    "trn_feat_dset = FeaturesDataset(trn_features, trn_labels)\n",
    "val_feat_dset = FeaturesDataset(val_features, val_labels)\n",
    "\n",
    "# create data loaders for batching the train and validation datasets\n",
    "trn_feat_loader = DataLoader(trn_feat_dset, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_feat_loader = DataLoader(val_feat_dset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a fully connected model and train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-a-fully-connected-model-and-train'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connected model\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_size, out_size)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.fc(inp)\n",
    "        return out\n",
    "\n",
    "\n",
    "# instantiate fully connected model\n",
    "fc = FullyConnectedModel(fc_in_Size, classes)\n",
    "if is_cuda:\n",
    "    fc = fc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation loop\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 10):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, fc, trn_feat_loader, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, fc, val_feat_loader, phase=\"validation\"\n",
    "    )\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ensembling\n",
    "\n",
    "We can combine outputs from features generated by three different models to build a powerful model. The architecture involves passing the images to each of the models, and each model passes to a fully connected layer. These three fully connected layers are combined into one fully connected layers, which produces the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Model-ensembling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "resnet_model = models.resnet34(pretrained=True)\n",
    "if is_cuda:\n",
    "    resnet_model = resnet_model.cuda()\n",
    "\n",
    "resnet_model = nn.Sequential(*list(resnet_model.children())[:-1])\n",
    "\n",
    "for p in resnet_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Inception_v3\n",
    "modelInception = models.inception_v3(pretrained=True)\n",
    "modelInception.aux_logits = False\n",
    "if is_cuda:\n",
    "    modelInception = modelInception.cuda()\n",
    "\n",
    "for p in modelInception.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# DenseNet\n",
    "model_densenet = models.densenet121(pretrained=True).features\n",
    "if is_cuda:\n",
    "    model_densenet = model_densenet.cuda()\n",
    "\n",
    "for p in model_densenet.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the image features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Extracting-the-image-features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "trn_labels = []\n",
    "trn_resnet_features = []\n",
    "for d, la in train_loader:\n",
    "    o = my_resnet(Variable(d.cuda()))\n",
    "    o = o.view(o.size(0), -1)\n",
    "    trn_labels.extend(la)\n",
    "    trn_resnet_features.extend(o.cpu().data)\n",
    "\n",
    "val_labels = []\n",
    "val_resnet_features = []\n",
    "for d, la in val_loader:\n",
    "    o = my_resnet(Variable(d.cuda()))\n",
    "    o = o.view(o.size(0), -1)\n",
    "    val_labels.extend(la)\n",
    "    val_resnet_features.extend(o.cpu().data)\n",
    "\n",
    "# Inception_v3\n",
    "trn_inception_features = LayerActivations(my_inception.Mixed_7c)\n",
    "for da, la in train_loader:\n",
    "    _ = my_inception(Variable(da.cuda()))\n",
    "\n",
    "trn_inception_features.remove()\n",
    "\n",
    "val_inception_features = LayerActivations(my_inception.Mixed_7c)\n",
    "for da, la in val_loader:\n",
    "    _ = my_inception(Variable(da.cuda()))\n",
    "val_inception_features.remove()\n",
    "\n",
    "# DenseNet\n",
    "trn_densenet_features = []\n",
    "for d, la in train_loader:\n",
    "    o = my_densenet(Variable(d.cuda()))\n",
    "    o = o.view(o.size(0), -1)\n",
    "    trn_densenet_features.extend(o.cpu().data)\n",
    "\n",
    "val_densenet_features = []\n",
    "for d, la in val_loader:\n",
    "    o = my_densenet(Variable(d.cuda()))\n",
    "    o = o.view(o.size(0), -1)\n",
    "    val_densenet_features.extend(o.cpu().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom dataset along with data loaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-a-custom-dataset-along-with-data-loaders'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset\n",
    "class FeaturesDataset(Dataset):\n",
    "    def __init__(self, featlist1, featlist2, featlst3, labellst):\n",
    "        self.featlst1 = featlst1\n",
    "        self.featlst2 = featlst2\n",
    "        self.featlst3 = featlst3\n",
    "        self.labellst = labellst\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.featlst1[index],\n",
    "            self.featlst2[index],\n",
    "            self.featlst3[index],\n",
    "            self.labellst[index],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labellst)\n",
    "\n",
    "\n",
    "# load data from separate models\n",
    "trn_feat_dset = FeaturesDataset(\n",
    "    trn_resnet_features,\n",
    "    trn_inception_features.features,\n",
    "    trn_densenet_features,\n",
    "    trn_labels,\n",
    ")\n",
    "val_feat_dset = FeaturesDataset(\n",
    "    val_resnet_features,\n",
    "    val_inception_features.features,\n",
    "    val_densenet_features,\n",
    "    val_labels,\n",
    ")\n",
    "\n",
    "trn_feat_loader = DataLoader(trn_feat_dset, batch_size=64, shuffle=True)\n",
    "val_feat_loader = DataLoader(val_feat_dset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an ensembling model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-an-ensembling-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble class\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, out_size, training=True):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8192, 512)\n",
    "        self.fc2 = nn.Linear(131072, 512)\n",
    "        self.fc3 = nn.Linear(82944, 512)\n",
    "        self.fc4 = nn.Linear(512, out_size)\n",
    "\n",
    "    def forward(self, inp1, inp2, inp3):\n",
    "        out1 = self.fc1(F.dropout(inp1, training=self.training))\n",
    "        out2 = self.fc2(F.dropout(inp2, training=self.training))\n",
    "        out3 = self.fc3(F.dropout(inp3, training=self.training))\n",
    "        out = out1 + out2 + out3\n",
    "        out = self.fc4(F.dropout(out, training=self.training))\n",
    "        return out\n",
    "\n",
    "\n",
    "# instantiate ensemble\n",
    "em = EnsembleModel(2)\n",
    "if is_cuda:\n",
    "    em = em.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validating the ensemble model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-and-validating-the-ensemble-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for executing fit and validation\n",
    "def fit(epoch, model, data_loader, phase=\"training\", volatile=False):\n",
    "    if phase == \"training\":\n",
    "        model.train()\n",
    "    if phase == \"validation\":\n",
    "        model.eval()\n",
    "        volatile = True\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "\n",
    "    for batch_idx, (data1, data2, data3, target) in enumerate(data_loader):\n",
    "        if is_cuda():\n",
    "            data1, data2, data3, target = (\n",
    "                data1.cuda(),\n",
    "                data2.cuda(),\n",
    "                data3.cuda(),\n",
    "                target.cuda(),\n",
    "            )\n",
    "        data1, data2, data3, target = (\n",
    "            Variable(data1, volatile),\n",
    "            Variable(data2, volatile),\n",
    "            Variable(data3, volatile),\n",
    "            Variable(target),\n",
    "        )\n",
    "        if phase == \"training\":\n",
    "            optimizer.zero_grad()\n",
    "        output = model(data1, data2, data3)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        running_loss += F.Cross_entropy(output, target, size_average=False).data.item()\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == \"training\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100.0 * running_correct / len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}\"\n",
    "    )\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation loop\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 10):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, em, trn_feat_loader, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, em, val_feat_loader, phase=\"validation\"\n",
    "    )\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
