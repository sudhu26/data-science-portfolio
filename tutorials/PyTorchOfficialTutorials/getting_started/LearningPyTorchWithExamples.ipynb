{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Learning PyTorch with examples__\n",
    "\n",
    "1. [Load tools](#Load-tools)\n",
    "1. [Tensors](#Tensors)\n",
    "    1. [Numpy tensors](#Numpy-tensors)\n",
    "    1. [PyTorch tensors](#PyTorch-tensors)\n",
    "1. [Autograd](#Autograd)\n",
    "    1. [PyTorch: tensors and autograd](#PyTorch:-tensors-and-autograd)\n",
    "    1. [PyTorch: defining new autograd functions](#PyTorch:-defining-new-autograd-functions)\n",
    "1. [torch.nn module](#torch.nn-module)\n",
    "1. [torch.optim module](#torch.optim-module)\n",
    "1. [PyTorch: custom torch.nn modules](#PyTorch:-custom-torch.nn-modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Load-tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:50:01.242507Z",
     "start_time": "2019-05-02T12:50:00.336323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# import PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch. autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.jit import script, trace\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tensors'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy tensors\n",
    "\n",
    "Numpy isn't intended to do anything related to computational graphs, deep learning or gradients, but we can easily use numpy to create a neural network anyway. this example demonstrates how to create and fit a two-layer network by manually implementing the forward and backward passes through the network using only numpy operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Numpy-tensors'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss = 43601548.37041\n",
      "Iter: 25, loss = 133819.75098\n",
      "Iter: 50, loss = 16471.53566\n",
      "Iter: 75, loss = 3594.05673\n",
      "Iter: 100, loss = 983.49389\n",
      "Iter: 125, loss = 298.90034\n",
      "Iter: 150, loss = 96.25113\n",
      "Iter: 175, loss = 32.04538\n",
      "Iter: 200, loss = 10.91174\n",
      "Iter: 225, loss = 3.77718\n",
      "Iter: 250, loss = 1.32445\n",
      "Iter: 275, loss = 0.46940\n",
      "Iter: 300, loss = 0.16790\n",
      "Iter: 325, loss = 0.06056\n",
      "Iter: 350, loss = 0.02201\n",
      "Iter: 375, loss = 0.00806\n",
      "Iter: 400, loss = 0.00297\n",
      "Iter: 425, loss = 0.00110\n",
      "Iter: 450, loss = 0.00041\n",
      "Iter: 475, loss = 0.00015\n"
     ]
    }
   ],
   "source": [
    "# batch size, input dimension, hidden layer dimension, output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input data and labels\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward pass: compute prediction for y\n",
    "    h = x.dot(w1) # 64 x 1000 multiplied by 1000 x 10 yields 64 x 100\n",
    "    h_relu = np.maximum(h, 0) # set everyth subzero element to 0\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 25 == 0:\n",
    "        print('Iter: {}, loss = {:.5f}'.format(t, loss))\n",
    "    \n",
    "    # backpropagate to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch tensors\n",
    "\n",
    "PyTorch makes many, many improvements toward the goal of executing neural network. It can utilize GPYs, which provide speedups of 50x or grater compare to numpy or similar libraries.\n",
    "\n",
    "The code block below executes the same two layer network created above in the numpy example, but this time using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'PyTorch-tensors'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss = 39673300.00000\n",
      "Iter: 25, loss = 128253.67188\n",
      "Iter: 50, loss = 13413.47461\n",
      "Iter: 75, loss = 2648.63599\n",
      "Iter: 100, loss = 700.27631\n",
      "Iter: 125, loss = 211.36861\n",
      "Iter: 150, loss = 68.03728\n",
      "Iter: 175, loss = 22.69126\n",
      "Iter: 200, loss = 7.73286\n",
      "Iter: 225, loss = 2.67220\n",
      "Iter: 250, loss = 0.93227\n",
      "Iter: 275, loss = 0.32749\n",
      "Iter: 300, loss = 0.11565\n",
      "Iter: 325, loss = 0.04108\n",
      "Iter: 350, loss = 0.01475\n",
      "Iter: 375, loss = 0.00545\n",
      "Iter: 400, loss = 0.00215\n",
      "Iter: 425, loss = 0.00094\n",
      "Iter: 450, loss = 0.00046\n",
      "Iter: 475, loss = 0.00026\n"
     ]
    }
   ],
   "source": [
    "# batch size, input dimension, hidden layer dimension, output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# create random input data and labels\n",
    "x = torch.randn(N, D_in, device = device, dtype = dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
    "\n",
    "\n",
    "# randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device = device, dtype = dtype)\n",
    "w2 = torch.randn(H, D_out, device = device, dtype = dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward pass: compute prediction for y\n",
    "    h = x.mm(w1) # 64 x 1000 multiplied by 1000 x 10 yields 64 x 100\n",
    "    h_relu = h.clamp(min = 0) # set every subzero element to 0\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    #y_pred = F.relu(x.mm(w1)).mm(x2) # OR this works\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 25 == 0:\n",
    "        print('Iter: {}, loss = {:.5f}'.format(t, loss))\n",
    "    \n",
    "    # backpropagate to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Autograd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: tensors and autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'PyTorch:-tensors-and-autograd'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss = 38466576.00000\n",
      "Iter: 25, loss = 136429.07812\n",
      "Iter: 50, loss = 13017.18164\n",
      "Iter: 75, loss = 1974.59363\n",
      "Iter: 100, loss = 383.76691\n",
      "Iter: 125, loss = 86.46795\n",
      "Iter: 150, loss = 21.27151\n",
      "Iter: 175, loss = 5.52313\n",
      "Iter: 200, loss = 1.48440\n",
      "Iter: 225, loss = 0.40827\n",
      "Iter: 250, loss = 0.11414\n",
      "Iter: 275, loss = 0.03236\n",
      "Iter: 300, loss = 0.00939\n",
      "Iter: 325, loss = 0.00290\n",
      "Iter: 350, loss = 0.00103\n",
      "Iter: 375, loss = 0.00044\n",
      "Iter: 400, loss = 0.00022\n",
      "Iter: 425, loss = 0.00013\n",
      "Iter: 450, loss = 0.00008\n",
      "Iter: 475, loss = 0.00005\n"
     ]
    }
   ],
   "source": [
    "# batch size, input dimension, hidden layer dimension, output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# create random input data and labels\n",
    "x = torch.randn(N, D_in, device = device, dtype = dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
    "\n",
    "\n",
    "# randomly initialize weights\n",
    "# requires_grad = True ensures that gradients are computed with respect to these Tensors during the backward pass\n",
    "w1 = torch.randn(D_in, H, device = device, dtype = dtype, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, device = device, dtype = dtype, requires_grad = True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward pass: exactly the same ass the operation above but we don't need to manually\n",
    "    # incorporate intermediate steps\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    \n",
    "    # compute loss on tensors, which is a Tensor of shape (1,). loss.item() retrieves the scalar\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 25 == 0:\n",
    "        print('Iter: {}, loss = {:.5f}'.format(t, loss.item()))\n",
    "        \n",
    "    # use autograd to perform the backward pass. this computes the gradient of loss wrt all Tensors with requires_grad = True\n",
    "    # following this call w1.grad and w2.grad will hold the gradient of the loss wrt w1 and w2\n",
    "    loss.backward()\n",
    "    \n",
    "    # manual update weights by gradient descent. wrapping in torch.no_grad() keeps the operation from being\n",
    "    # tracked in the weight Tensors that have requires_grad = True. This doesn't need to be tracked in autograd\n",
    "    # this method is a less efficient way that utilizes weight.data and weight.grad.data\n",
    "    # torch.optim.SGD can also achieve this but more efficiently\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # manually zero out gradients\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: defining new autograd functions\n",
    "\n",
    "Each builtin autograd operator contains two function that operate on Tensors:\n",
    "- forward - compures the output Tensors from the input Tensors\n",
    "- backward - receives the gradient of the output Tensors wrt the ground truth and computes the gradient of the input Tensors wrt the ground truth.\n",
    "\n",
    "Custom autograd operators can be created and incorporated into networks by defining a subclass of torch.autograd.Function and creating the required forward and backward functions. To use the custom autograd operator, we need to create an instance of the operator and call it like a function, to which we pass Tensors containing the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'PyTorch:-defining-new-autograd-functions'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss = 31708310.00000\n",
      "Iter: 25, loss = 138472.06250\n",
      "Iter: 50, loss = 14044.58008\n",
      "Iter: 75, loss = 2221.72803\n",
      "Iter: 100, loss = 422.07016\n",
      "Iter: 125, loss = 89.12749\n",
      "Iter: 150, loss = 20.24022\n",
      "Iter: 175, loss = 4.85380\n",
      "Iter: 200, loss = 1.21483\n",
      "Iter: 225, loss = 0.31466\n",
      "Iter: 250, loss = 0.08379\n",
      "Iter: 275, loss = 0.02290\n",
      "Iter: 300, loss = 0.00650\n",
      "Iter: 325, loss = 0.00203\n",
      "Iter: 350, loss = 0.00074\n",
      "Iter: 375, loss = 0.00033\n",
      "Iter: 400, loss = 0.00017\n",
      "Iter: 425, loss = 0.00010\n",
      "Iter: 450, loss = 0.00006\n",
      "Iter: 475, loss = 0.00004\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "class MyRelu(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # ctx is a context object that can be used to stash data for backpropagation\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min = 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # receives a Tensor containing the gradient of the los wrt the output. we need to compute\n",
    "        # the gradient of the loss wrt the input\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "# batch size, input dimension, hidden layer dimension, output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# create random input data and labels\n",
    "x = torch.randn(N, D_in, device = device, dtype = dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
    "\n",
    "\n",
    "# randomly initialize weights\n",
    "# requires_grad = True ensures that gradients are computed with respect to these Tensors during the backward pass\n",
    "w1 = torch.randn(D_in, H, device = device, dtype = dtype, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, device = device, dtype = dtype, requires_grad = True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # apply operator using Function.apply\n",
    "    relu = MyRelu.apply\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "        \n",
    "    # compute loss \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 25 == 0:\n",
    "        print('Iter: {}, loss = {:.5f}'.format(t, loss.item()))\n",
    "        \n",
    "    # use autograd to perform the backward pass. \n",
    "    loss.backward()\n",
    "    \n",
    "    # manual update weights by gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # manually zero out gradients\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn module\n",
    "\n",
    "Computational graphs and autograd in the manner implemented above can be too low-level when fit large neural network. the torch.nn package serves the purpose of providing higher-level abstractions that more efficiently handle autograd operations. torch.nn defines a set of Modules which in general can be thought of as types of neural network layers. A Module receives input Tensors and computes output Tensors, and can also hold an internal state which can manage Tensors that contain learnable parameters. torch.nn also contains many loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'torch.nn-module'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss = 712.22937\n",
      "Iter: 25, loss = 552.48029\n",
      "Iter: 50, loss = 313.91757\n",
      "Iter: 75, loss = 179.24721\n",
      "Iter: 100, loss = 122.44864\n",
      "Iter: 125, loss = 180.76990\n",
      "Iter: 150, loss = 233.33000\n",
      "Iter: 175, loss = 136.27899\n",
      "Iter: 200, loss = 165.84132\n",
      "Iter: 225, loss = 276.81699\n",
      "Iter: 250, loss = 193.24693\n",
      "Iter: 275, loss = 232.00133\n",
      "Iter: 300, loss = 214.97557\n",
      "Iter: 325, loss = 164.68013\n",
      "Iter: 350, loss = 222.17741\n",
      "Iter: 375, loss = 205.73183\n",
      "Iter: 400, loss = 153.95300\n",
      "Iter: 425, loss = 228.58423\n",
      "Iter: 450, loss = 199.14041\n",
      "Iter: 475, loss = 128.64659\n"
     ]
    }
   ],
   "source": [
    "# batch size, input dimension, hidden layer dimension, output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input data and labels\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# use torch.nn to define the model as a sequence of layers. nn.Sequential is a Module that contains other Modules, and these\n",
    "# are applied in the specified order to produce an output.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H)\n",
    "    ,torch.nn.ReLU()\n",
    "    ,torch.nn.Linear(H, D_out)\n",
    ")\n",
    "\n",
    "# define torch.nn loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # perform forward pass by passing the input  to the model\n",
    "    y_pred = model(x)\n",
    "        \n",
    "    # compute loss. this returns a Tensor containing the loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 25 == 0:\n",
    "        print('Iter: {}, loss = {:.5f}'.format(t, loss.item()))\n",
    "        \n",
    "    # perform backward pass by computing the gradient of the loss wrt to all the learnable parameters in the model. internally\n",
    "    # the parameters of each module are stored in Tensors where requires_grad = True. This will compute gradients for all \n",
    "    # learnable parameters in the model\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights using gradient descent. \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.optim module\n",
    "\n",
    "Weight updates up to this point have been updated by manually adjusting the Tensors holding learnable parameters. The torch.optim package offers a more efficient way to update the parameters, and includes implementations for sophisticated optimizers like AdaGrad, RMSProp, Adam, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'torch.optim-module'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss = 681.32642\n",
      "Iter: 25, loss = 370.14603\n",
      "Iter: 50, loss = 208.11354\n",
      "Iter: 75, loss = 110.35217\n",
      "Iter: 100, loss = 52.02501\n",
      "Iter: 125, loss = 21.29395\n",
      "Iter: 150, loss = 7.57197\n",
      "Iter: 175, loss = 2.38376\n",
      "Iter: 200, loss = 0.68513\n",
      "Iter: 225, loss = 0.18359\n",
      "Iter: 250, loss = 0.04648\n",
      "Iter: 275, loss = 0.01133\n",
      "Iter: 300, loss = 0.00269\n",
      "Iter: 325, loss = 0.00062\n",
      "Iter: 350, loss = 0.00014\n",
      "Iter: 375, loss = 0.00003\n",
      "Iter: 400, loss = 0.00001\n",
      "Iter: 425, loss = 0.00000\n",
      "Iter: 450, loss = 0.00000\n",
      "Iter: 475, loss = 0.00000\n"
     ]
    }
   ],
   "source": [
    "# batch size, input dimension, hidden layer dimension, output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input data and labels\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# use torch.nn to define the model as a sequence of layers. nn.Sequential is a Module that contains other Modules, and these\n",
    "# are applied in the specified order to produce an output.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H)\n",
    "    ,torch.nn.ReLU()\n",
    "    ,torch.nn.Linear(H, D_out)\n",
    ")\n",
    "\n",
    "# define torch.nn loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "# define an optimizer. this will update the model weights. the first parameter tells the constructor which Tensors to update\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "\n",
    "for t in range(500):\n",
    "    # perform forward pass by passing the input  to the model\n",
    "    y_pred = model(x)\n",
    "        \n",
    "    # compute loss. this returns a Tensor containing the loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 25 == 0:\n",
    "        print('Iter: {}, loss = {:.5f}'.format(t, loss.item()))\n",
    "    \n",
    "    # before each backward pass all of the gradients need to be zeroed out. otherwise gradients are accumulated\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform backward pass \n",
    "    loss.backward()\n",
    "    \n",
    "    # call the step function to perform the weight updates\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: custom torch.nn modules\n",
    "\n",
    "Custom modules can be created by subclassing nn.Module and defining a forward function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'PyTorch:-custom-torch.nn-modules'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss = 702.34802\n",
      "Iter: 25, loss = 386.69614\n",
      "Iter: 50, loss = 214.55013\n",
      "Iter: 75, loss = 110.25887\n",
      "Iter: 100, loss = 49.78801\n",
      "Iter: 125, loss = 19.40651\n",
      "Iter: 150, loss = 6.43203\n",
      "Iter: 175, loss = 1.89738\n",
      "Iter: 200, loss = 0.54688\n",
      "Iter: 225, loss = 0.16783\n",
      "Iter: 250, loss = 0.05717\n",
      "Iter: 275, loss = 0.02141\n",
      "Iter: 300, loss = 0.00844\n",
      "Iter: 325, loss = 0.00333\n",
      "Iter: 350, loss = 0.00128\n",
      "Iter: 375, loss = 0.00047\n",
      "Iter: 400, loss = 0.00016\n",
      "Iter: 425, loss = 0.00006\n",
      "Iter: 450, loss = 0.00002\n",
      "Iter: 475, loss = 0.00001\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min = 0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "# batch size, input dimension, hidden layer dimension, output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input data and labels\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# construct the model by instantiating the custom class\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# define torch.nn loss function\n",
    "criterion = torch.nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "# define an optimizer. this will update the model weights. the first parameter tells the constructor which Tensors to update\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "\n",
    "for t in range(500):\n",
    "    # perform forward pass by passing the input  to the model\n",
    "    y_pred = model(x)\n",
    "        \n",
    "    # compute loss. this returns a Tensor containing the loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 25 == 0:\n",
    "        print('Iter: {}, loss = {:.5f}'.format(t, loss.item()))\n",
    "    \n",
    "    # before each backward pass all of the gradients need to be zeroed out. otherwise gradients are accumulated\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform backward pass \n",
    "    loss.backward()\n",
    "    \n",
    "    # call the step function to perform the weight updates\n",
    "    optimizer.step()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
