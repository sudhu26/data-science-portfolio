{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "__Table of contents__\n",
    "\n",
    "1. [Module 7 walkthrough](#Module-7-walkthrough)\n",
    "1. [Module 8 walkthrough](#Module-8-walkthrough)\n",
    "1. [Assignment 5](#assignment)\n",
    "    1. [Acquire tweets](#Acquire-tweets)\n",
    "    1. [Load tweets](#Load-tweets)\n",
    "    1. [HTML Parser](#HTML-Parser)\n",
    "    1. [Remove username, URL](#Remove-username-URL)\n",
    "    1. [Remove punctuation](#Remove-punctuation)\n",
    "    1. [Remove apostrophes](#Remove-apostrophes)\n",
    "    1. [Word pattern formatting](#Word-pattern-formatting)\n",
    "    1. [Remove hashtags](#Remove-hashtags)\n",
    "    1. [Polarity analysis](#Polarity-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:13:30.047888Z",
     "start_time": "2018-11-18T23:13:12.859453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\petersont\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'config' has no attribute 'apiKey'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7cf7dc5d6b7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Standard tweepy API setup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mauth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOAuthHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapiKey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapiSec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_access_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccessToken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccessSec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'config' has no attribute 'apiKey'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import jsonpickle\n",
    "import json\n",
    "import tweepy\n",
    "import html.parser as HTMLParser\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "modulePath = os.path.abspath(os.path.join('../../..'))\n",
    "if modulePath not in sys.path:\n",
    "    sys.path.append(modulePath)\n",
    "import config\n",
    "\n",
    "# Standard tweepy API setup\n",
    "\n",
    "auth = tweepy.OAuthHandler(config.apiKey, config.apiSec)\n",
    "auth.set_access_token(config.accessToken, config.accessSec)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Application authentication tweepy setup\n",
    "# Use application-only authentication for higher Twitter API rate limit\n",
    "# Twitter API returns a max of 100 tweets per query\n",
    "# Allows for 450 queries every 15 minutes\n",
    "# So we can gather 45,000 tweets every 15 minutes\n",
    "\n",
    "#Switching to application authentication\n",
    "auth = tweepy.AppAuthHandler(config.apiKey, config.apiSec)\n",
    "\n",
    "#Setting up new api wrapper, using authentication only\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True\n",
    "                 ,wait_on_rate_limit_notify = True)\n",
    " \n",
    "# View rate limit status\n",
    "\n",
    "api.rate_limit_status()['resources']['search']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Module-7-walkthrough'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7 walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:13:37.433024Z",
     "start_time": "2018-11-18T23:13:37.385484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user_@34 Life is great & I like it sooooooooo much. It's whatis life. #life #great#like http://lifeisgreat.com .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\petersont\\Miniconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "htmlParser = HTMLParser.HTMLParser()\n",
    "\n",
    "tweet = \"@user_@34 Life is great & I like it sooooooooo much. It's whatis life. #life #great#like http://lifeisgreat.com .\"\n",
    "parsedTweet = htmlParser.unescape(tweet)\n",
    "print(parsedTweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:13:49.142548Z",
     "start_time": "2018-11-18T23:13:49.136600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user_@34 Life is great & I like it sooooooooo much. It's whatis life. #life #great#like  .\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "urlPattern = re.compile('http\\S+')\n",
    "tweet_v1 = re.sub(urlPattern, '', parsedTweet)\n",
    "print(tweet_v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:13:57.483262Z",
     "start_time": "2018-11-18T23:13:57.479017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Life is great & I like it sooooooooo much. It's whatis life. #life #great#like  .\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "usernamePattern = re.compile('@\\S+')\n",
    "tweet_v2 = re.sub(usernamePattern, '', tweet_v1)\n",
    "print(tweet_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:14:01.469499Z",
     "start_time": "2018-11-18T23:14:01.464368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Life is great & I like it so much. It's whatis life. #life #great#like  .\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "wordPattern = re.compile('s[o]+')\n",
    "tweet_v3 = re.sub(wordPattern, 'so', tweet_v2)\n",
    "print(tweet_v3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Module-8-walkthrough'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:15:56.399732Z",
     "start_time": "2018-11-18T23:15:56.368573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/petersontylerd/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "positive score for the word \"happy\": 0.875\n",
      "negative score for the word \"happy\": 0.0\n",
      "neutral score for the word \"happy\": 0.125\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print('positive score for the word \"happy\": {0}'.format(list(swn.senti_synsets('happy','a'))[0].pos_score()))\n",
    "print('negative score for the word \"happy\": {0}'.format(list(swn.senti_synsets('happy','a'))[0].neg_score()))\n",
    "print('neutral score for the word \"happy\": {0}'.format(list(swn.senti_synsets('happy','a'))[0].obj_score()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:18:29.606078Z",
     "start_time": "2018-11-18T23:18:29.601908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/petersontylerd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Tokens: ['i', 'am', 'happy']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentence = 'i am happy'\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "print('Tokens: {0}'.format(tokens))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:19:29.663019Z",
     "start_time": "2018-11-18T23:19:28.470738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/petersontylerd/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'), ('am', 'VBP'), ('happy', 'JJ')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_tag(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN - noun\n",
    "VBP - verb\n",
    "JJ - adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:20:08.632679Z",
     "start_time": "2018-11-18T23:20:08.285490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence has been reduced from 'i am happy' \n",
      " to '['happy']'\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "sentence = 'i am happy'\n",
    "newSentence = []\n",
    "for word in tokens:\n",
    "    if word not in stop:\n",
    "        newSentence.append(word)\n",
    "\n",
    "print('The sentence has been reduced from \\'{0}\\' \\n to \\'{1}\\''.format(sentence, newSentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'assignment'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "* Try cleaning the tweets that you have extracted in the the previous chapter. Apply the above rules and in addition to that apply the below mentioned rules as well:\n",
    "    * Remove Punctuations. Puntuations sometimes don't carry any weight. You can remove them. Try writing a regular expression to remove , from sentences. Dont remove question marks \"?\" or exclamatory marks as they have effect upon any sentence.\n",
    "    * Remove apostrophes and expand the words. For example in the sentence \"It's a great time to code!\" the first word It's can be expanded to 'it is'. You can do this either with regular expressions.\n",
    "    * Create a list of word patterns for word formatting. For example 'gud' should be substitued with 'good'\n",
    "\n",
    "* Calculate the polarity of a sentence and write a progam to calculate the polarity of all the tweets that you have extracted and preprocessed in the previous questions. You progam should also include the below features:\n",
    "\n",
    "    * Tweets have hashtags. Remove the hashtags and then find the polarity of each tweet.\n",
    "\n",
    "    * There might be words that are not present in the sentiwordnet lexicon.\n",
    "    * The program should handle these cases, by giving a zero score for such words.\n",
    "    *Depending on the questions,file uploads or screenshots are necessary to show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Acquire-tweets'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T02:26:30.976683Z",
     "start_time": "2018-11-13T18:13:22.493972Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find up to 500,000 tweets from the last week containing the word election.\n",
    "# Store in JSON file\n",
    "\n",
    "maxTweets = 500000\n",
    "tweetCount = 0\n",
    "with open('trumpTweets.json','w') as f:\n",
    "    for tweet in tweepy.Cursor(api.search, q = 'trump', tweet_mode = 'extended', lang = 'en').items(maxTweets):\n",
    "        f.write(jsonpickle.encode(tweet._json, unpicklable = False) + '\\n')\n",
    "        tweetCount += 1\n",
    "    print('Downloaded {0} tweets'.format(tweetCount))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Load-tweets'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T00:32:26.426389Z",
     "start_time": "2018-11-19T00:31:59.116637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets loaded: 221072\n"
     ]
    }
   ],
   "source": [
    "# Load election tweets into memory\n",
    "\n",
    "data = []\n",
    "with open('./trumpTweets.json', 'r') as jsonFile:\n",
    "    for line in jsonFile:\n",
    "        data.append(json.loads(line))\n",
    "print('Total number of tweets loaded: {0}'.format(len(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T00:54:37.787786Z",
     "start_time": "2018-11-19T00:54:37.629407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets extracted from json: 221072\n"
     ]
    }
   ],
   "source": [
    "# Unpack all tweets in data\n",
    "\n",
    "tweets = []\n",
    "for item in data:\n",
    "    if 'full_text' in item.keys():\n",
    "        tweet = item['full_text']\n",
    "        tweets.append(tweet)\n",
    "print('Total number of tweets extracted from json: {0}'.format(len(tweets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @pettyasamug: Trump hates pics of his hair circulating on social media #Retweet ‚ôªÔ∏è\\n\\n#NotMyPresident https://t.co/4uvEKDaING',\n",
       " 'RT @maggieNYT: Ted OLSON, who Trump praised in one of his Fla tweets and who Trump tried repeatedly to hire for his own personal legal team‚Ä¶',\n",
       " '@realDonaldTrump Stock goes up, credit to trump , goes down? Blame the Democrats. Got it üëåüèæ',\n",
       " 'This. The crisis is here; it can‚Äôt be avoided, only mitigated. When I look at Trump‚Äôs GOP &amp; their supporters, I see people knowingly and gleefully poisoning my grandchildren and my planet. This is why bipartisanship is BS. I won‚Äôt compromise with murderers. https://t.co/mEYs7IszjB',\n",
       " 'RT @politico: Despite Democrats‚Äô massive House gains ‚Äî the party‚Äôs biggest since 1974, after Richard Nixon‚Äôs resignation ‚Äî redistricting cl‚Ä¶']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T00:54:37.854171Z",
     "start_time": "2018-11-19T00:54:37.790044Z"
    }
   },
   "outputs": [],
   "source": [
    "# I only want to look at original tweets, not retweets\n",
    "\n",
    "tweets = [x for x in tweets if not x.startswith('RT ')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@realDonaldTrump Stock goes up, credit to trump , goes down? Blame the Democrats. Got it üëåüèæ',\n",
       " 'This. The crisis is here; it can‚Äôt be avoided, only mitigated. When I look at Trump‚Äôs GOP &amp; their supporters, I see people knowingly and gleefully poisoning my grandchildren and my planet. This is why bipartisanship is BS. I won‚Äôt compromise with murderers. https://t.co/mEYs7IszjB',\n",
       " '@PrincessBravato Suburban white women who do not have a hard on for Trump like @lindseygraham',\n",
       " '@speakerRyan I am so weary of how often you lie about this. It is early but still not going to work. Trump‚Äôs Tax Cut Was Supposed to Change Corporate Behavior. Here‚Äôs What Happened. https://t.co/9X9LR1aXJ1',\n",
       " 'Ummmmmmm...no. \\n\\nTrump claims he tried to salvage trip to French cemetery for U.S. troops - POLITICO https://t.co/G9cRGSv06s']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T00:54:37.861553Z",
     "start_time": "2018-11-19T00:54:37.856176Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review first 3 tweets\n",
    "\n",
    "for i in range(3):\n",
    "    print(tweets[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'HTML-Parser'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T00:54:37.907596Z",
     "start_time": "2018-11-19T00:54:37.863577Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "import html\n",
    "\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = html.unescape(tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@realDonaldTrump Stock goes up, credit to trump , goes down? Blame the Democrats. Got it üëåüèæ',\n",
       " 'This. The crisis is here; it can‚Äôt be avoided, only mitigated. When I look at Trump‚Äôs GOP & their supporters, I see people knowingly and gleefully poisoning my grandchildren and my planet. This is why bipartisanship is BS. I won‚Äôt compromise with murderers. https://t.co/mEYs7IszjB',\n",
       " '@PrincessBravato Suburban white women who do not have a hard on for Trump like @lindseygraham',\n",
       " '@speakerRyan I am so weary of how often you lie about this. It is early but still not going to work. Trump‚Äôs Tax Cut Was Supposed to Change Corporate Behavior. Here‚Äôs What Happened. https://t.co/9X9LR1aXJ1',\n",
       " 'Ummmmmmm...no. \\n\\nTrump claims he tried to salvage trip to French cemetery for U.S. troops - POLITICO https://t.co/G9cRGSv06s']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-username-URL'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove username, URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T00:54:38.116890Z",
     "start_time": "2018-11-19T00:54:37.909798Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove URLs and usernames\n",
    "\n",
    "urlPattern = re.compile(r'(?:\\@|https?\\://)\\S+')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(urlPattern, '', tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Stock goes up, credit to trump , goes down? Blame the Democrats. Got it üëåüèæ',\n",
       " 'This. The crisis is here; it can‚Äôt be avoided, only mitigated. When I look at Trump‚Äôs GOP & their supporters, I see people knowingly and gleefully poisoning my grandchildren and my planet. This is why bipartisanship is BS. I won‚Äôt compromise with murderers. ',\n",
       " ' Suburban white women who do not have a hard on for Trump like ',\n",
       " ' I am so weary of how often you lie about this. It is early but still not going to work. Trump‚Äôs Tax Cut Was Supposed to Change Corporate Behavior. Here‚Äôs What Happened. ',\n",
       " 'Ummmmmmm...no. \\n\\nTrump claims he tried to salvage trip to French cemetery for U.S. troops - POLITICO ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-punctuation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T00:54:39.241497Z",
     "start_time": "2018-11-19T00:54:38.688322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary white space, newlines and tabs\n",
    "\n",
    "stripPattern = re.compile(r'\\s+')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(stripPattern, ' ', tweet).strip()\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stock goes up, credit to trump , goes down? Blame the Democrats. Got it üëåüèæ',\n",
       " 'This. The crisis is here; it can‚Äôt be avoided, only mitigated. When I look at Trump‚Äôs GOP & their supporters, I see people knowingly and gleefully poisoning my grandchildren and my planet. This is why bipartisanship is BS. I won‚Äôt compromise with murderers.',\n",
       " 'Suburban white women who do not have a hard on for Trump like',\n",
       " 'I am so weary of how often you lie about this. It is early but still not going to work. Trump‚Äôs Tax Cut Was Supposed to Change Corporate Behavior. Here‚Äôs What Happened.',\n",
       " 'Ummmmmmm...no. Trump claims he tried to salvage trip to French cemetery for U.S. troops - POLITICO',\n",
       " 'We couldn‚Äôt look more closely if we tried. Every single story MSM gives us proves they are united with us against Trump.',\n",
       " \"Putin will send a Putin bear to Trump in his new prison surrounding's. And will make sure Ivan is his daddy I mean cellmate\",\n",
       " 'Donald Trump gets back to the United States, and someone explains what they were saying in Europe',\n",
       " 'Before tRUMP, the US would have been embarrassed to ally itself with Russia and North Korea. And \\U0001f997\\U0001f997\\U0001f997',\n",
       " \"They wouldn't have done this when Obama was in office or Bush come to that but with Trump in they feel he'll back them\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-apostrophes'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T00:54:39.241497Z",
     "start_time": "2018-11-19T00:54:38.688322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fix fancy single quote that's used as apostrophe in tweets with standard apostrophe\n",
    "\n",
    "apostropheFix = re.compile(u'\\u2019')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(apostropheFix, \"'\", tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stock goes up, credit to trump , goes down? Blame the Democrats. Got it üëåüèæ',\n",
       " \"This. The crisis is here; it can't be avoided, only mitigated. When I look at Trump's GOP & their supporters, I see people knowingly and gleefully poisoning my grandchildren and my planet. This is why bipartisanship is BS. I won't compromise with murderers.\",\n",
       " 'Suburban white women who do not have a hard on for Trump like',\n",
       " \"I am so weary of how often you lie about this. It is early but still not going to work. Trump's Tax Cut Was Supposed to Change Corporate Behavior. Here's What Happened.\",\n",
       " 'Ummmmmmm...no. Trump claims he tried to salvage trip to French cemetery for U.S. troops - POLITICO',\n",
       " \"We couldn't look more closely if we tried. Every single story MSM gives us proves they are united with us against Trump.\",\n",
       " \"Putin will send a Putin bear to Trump in his new prison surrounding's. And will make sure Ivan is his daddy I mean cellmate\",\n",
       " 'Donald Trump gets back to the United States, and someone explains what they were saying in Europe',\n",
       " 'Before tRUMP, the US would have been embarrassed to ally itself with Russia and North Korea. And \\U0001f997\\U0001f997\\U0001f997',\n",
       " \"They wouldn't have done this when Obama was in office or Bush come to that but with Trump in they feel he'll back them\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove apostrophes\n",
    "\n",
    "- Remove apostrophes and expand words\n",
    "    - \"It's\" becomes \"It is\", however \"Trump's\" stays \"Trump's\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\"\"\"\n",
    "I am copying this approach from this stackoverflow post http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "\"\"\"\n",
    "\n",
    "cList = {\n",
    "          \"ain't\": \"am not\",\n",
    "          \"aren't\": \"are not\",\n",
    "          \"can't've\": \"cannot have\",\n",
    "          \"can't\": \"cannot\",\n",
    "          \"'cause\": \"because\",\n",
    "          \"couldn't've\": \"could not have\",\n",
    "          \"could've\": \"could have\",\n",
    "          \"couldn't\": \"could not\",\n",
    "          \"didn't\": \"did not\",\n",
    "          \"doesn't\": \"does not\",\n",
    "          \"don't\": \"do not\",\n",
    "          \"hadn't've\": \"had not have\",\n",
    "          \"hadn't\": \"had not\",\n",
    "          \"hasn't\": \"has not\",\n",
    "          \"haven't\": \"have not\",\n",
    "          \"he'd've\": \"he would have\",\n",
    "          \"he'd\": \"he would\",\n",
    "          \"he'll've\": \"he will have\",\n",
    "          \"he'll\": \"he will\",\n",
    "          \"he's\": \"he is\",\n",
    "          \"how'd'y\": \"how do you\",\n",
    "          \"how'd\": \"how did\",\n",
    "          \"how'll\": \"how will\",\n",
    "          \"how's\": \"how is\",\n",
    "          \"i'd've\": \"i would have\",\n",
    "          \"i'd\": \"i would\",\n",
    "          \"i'll've\": \"i will have\",\n",
    "          \"i'll\": \"i will\",\n",
    "          \"i'm\": \"i am\",\n",
    "          \"i've\": \"i have\",\n",
    "          \"isn't\": \"is not\",\n",
    "          \"it'd've\": \"it would have\",\n",
    "          \"it'd\": \"it had\",\n",
    "          \"it'll've\": \"it will have\",\n",
    "          \"it'll\": \"it will\",\n",
    "          \"it's\": \"it is\",\n",
    "          \"let's\": \"let us\",\n",
    "          \"ma'am\": \"madam\",\n",
    "          \"mayn't\": \"may not\",\n",
    "          \"might've\": \"might have\",\n",
    "          \"mightn't've\": \"might not have\",\n",
    "          \"mightn't\": \"might not\",\n",
    "          \"must've\": \"must have\",\n",
    "          \"mustn't've\": \"must not have\",\n",
    "          \"mustn't\": \"must not\",\n",
    "          \"needn't've\": \"need not have\",\n",
    "          \"needn't\": \"need not\",\n",
    "          \"o'clock\": \"of the clock\",\n",
    "          \"oughtn't've\": \"ought not have\",\n",
    "          \"oughtn't\": \"ought not\",\n",
    "          \"shan't\": \"shall not\",\n",
    "          \"shan't've\": \"shall not have\",\n",
    "          \"sha'n't\": \"shall not\",\n",
    "          \"she'd've\": \"she would have\",\n",
    "          \"she'd\": \"she would\",\n",
    "          \"she'll've\": \"she will have\",\n",
    "          \"she'll\": \"she will\",\n",
    "          \"she's\": \"she is\",\n",
    "          \"shouldn't've\": \"should not have\",\n",
    "          \"should've\": \"should have\",\n",
    "          \"shouldn't\": \"should not\",\n",
    "          \"so've\": \"so have\",\n",
    "          \"so's\": \"so is\",\n",
    "          \"that'd've\": \"that would have\",\n",
    "          \"that'd\": \"that would\",\n",
    "          \"that's\": \"that is\",\n",
    "          \"there'd've\": \"there would have\",\n",
    "          \"there'd\": \"there had\",\n",
    "          \"there's\": \"there is\",\n",
    "          \"they'd've\": \"they would have\",\n",
    "          \"they'd\": \"they would\",\n",
    "          \"they'll've\": \"they will have\",\n",
    "          \"they'll\": \"they will\",\n",
    "          \"they're\": \"they are\",\n",
    "          \"they've\": \"they have\",\n",
    "          \"to've\": \"to have\",\n",
    "          \"wasn't\": \"was not\",\n",
    "          \"we'd've\": \"we would have\",\n",
    "          \"we'd\": \"we had\",\n",
    "          \"we'll've\": \"we will have\",\n",
    "          \"we'll\": \"we will\",\n",
    "          \"we're\": \"we are\",\n",
    "          \"we've\": \"we have\",\n",
    "          \"weren't\": \"were not\",\n",
    "          \"what'll\": \"what will\",\n",
    "          \"what'll've\": \"what will have\",\n",
    "          \"what're\": \"what are\",\n",
    "          \"what's\": \"what is\",\n",
    "          \"what've\": \"what have\",\n",
    "          \"when's\": \"when is\",\n",
    "          \"when've\": \"when have\",\n",
    "          \"where'd\": \"where did\",\n",
    "          \"where's\": \"where is\",\n",
    "          \"where've\": \"where have\",\n",
    "          \"who'll\": \"who will\",\n",
    "          \"who'll've\": \"who will have\",\n",
    "          \"who's\": \"who is\",\n",
    "          \"who've\": \"who have\",\n",
    "          \"why's\": \"why is\",\n",
    "          \"why've\": \"why have\",\n",
    "          \"will've\": \"will have\",\n",
    "          \"won't\": \"will not\",\n",
    "          \"won't've\": \"will not have\",\n",
    "          \"would've\": \"would have\",\n",
    "          \"wouldn't\": \"would not\",\n",
    "          \"wouldn't've\": \"would not have\",\n",
    "          \"y'all'd've\": \"you all would have\",\n",
    "          \"y'all'd\": \"you all would\",\n",
    "          \"y'all're\": \"you all are\",\n",
    "          \"y'all've\": \"you all have\",\n",
    "          \"y'all\": \"you all\",\n",
    "          \"y'alls\": \"you alls\",\n",
    "          \"you'd've\": \"you would have\",\n",
    "          \"you'd\": \"you had\",\n",
    "          \"you'll've\": \"you you will have\",\n",
    "          \"you'll\": \"you you will\",\n",
    "          \"you're\": \"you are\",\n",
    "          \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "contractionPatterns = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re = contractionPatterns):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text.lower())\n",
    "\n",
    "\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = expandContractions(tweet)\n",
    "    tweets[ix] = parsedTweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stock goes up, credit to trump , goes down? blame the democrats. got it üëåüèæ',\n",
       " \"this. the crisis is here; it cannot be avoided, only mitigated. when i look at trump's gop & their supporters, i see people knowingly and gleefully poisoning my grandchildren and my planet. this is why bipartisanship is bs. i will not compromise with murderers.\",\n",
       " 'suburban white women who do not have a hard on for trump like',\n",
       " \"i am so weary of how often you lie about this. it is early but still not going to work. trump's tax cut was supposed to change corporate behavior. here's what happened.\",\n",
       " 'ummmmmmm...no. trump claims he tried to salvage trip to french cemetery for u.s. troops - politico']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation\n",
    "\n",
    "- Remove ','\n",
    "- Keep '?','!'\n",
    "- Remove newline (\\n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T00:54:38.315807Z",
     "start_time": "2018-11-19T00:54:38.119083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace ellipses with a single space\n",
    "\n",
    "ellipsesPattern = re.compile(r'\\.{3,}')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(ellipsesPattern, ' ', tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stock goes up, credit to trump , goes down? blame the democrats. got it üëåüèæ',\n",
       " \"this. the crisis is here; it cannot be avoided, only mitigated. when i look at trump's gop & their supporters, i see people knowingly and gleefully poisoning my grandchildren and my planet. this is why bipartisanship is bs. i will not compromise with murderers.\",\n",
       " 'suburban white women who do not have a hard on for trump like',\n",
       " \"i am so weary of how often you lie about this. it is early but still not going to work. trump's tax cut was supposed to change corporate behavior. here's what happened.\",\n",
       " 'ummmmmmm no. trump claims he tried to salvage trip to french cemetery for u.s. troops - politico']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T00:54:38.686767Z",
     "start_time": "2018-11-19T00:54:38.317676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove all punctuation except '?', '!', '#',and apostrophes\n",
    "\n",
    "punctuationPattern = re.compile(r\"[^\\w\\d\\s?!#']+\")\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(punctuationPattern, '', tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stock goes up credit to trump  goes down? blame the democrats got it ',\n",
       " \"this the crisis is here it cannot be avoided only mitigated when i look at trump's gop  their supporters i see people knowingly and gleefully poisoning my grandchildren and my planet this is why bipartisanship is bs i will not compromise with murderers\",\n",
       " 'suburban white women who do not have a hard on for trump like',\n",
       " \"i am so weary of how often you lie about this it is early but still not going to work trump's tax cut was supposed to change corporate behavior here's what happened\",\n",
       " 'ummmmmmm no trump claims he tried to salvage trip to french cemetery for us troops  politico']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Word-pattern-formatting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word pattern formatting\n",
    "\n",
    "- Condense extended strings of vowels and consonants down to form correctly spelled word\n",
    "    - \"Gooooooood\" becomes \"Good\"\n",
    "    - \"Realllllly\" becomes \"Really\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all overly repetitive vowels and consonants\n",
    "\n",
    "repetitionPattern = re.compile(r'(.)\\1+')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(repetitionPattern, r'\\1\\1', tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-hashtags'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all hashtags, including # and the associated word.\n",
    "\n",
    "hashtagPattern = re.compile(r'([#?])(\\w+)\\b')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(hashtagPattern, '', tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Polarity-analysis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\petersont\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-1d1c12764fe0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wordnet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'positive score for the word \"happy\": {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mswn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msenti_synsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'happy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)\u001b[0m\n\u001b[0;32m    668\u001b[0m                                     subsequent_indent=prefix+prefix2+' '*4))\n\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[1;31m# Error messages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[1;31m# Handle Packages (delegate to a helper function).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_download_package\u001b[1;34m(self, info, download_dir, force)\u001b[0m\n\u001b[0;32m    622\u001b[0m                 \u001b[0mnum_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 16k blocks.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m                     \u001b[0moutfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1007\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1009\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    869\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m    630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print('positive score for the word \"happy\": {0}'.format(list(swn.senti_synsets('happy','a'))[0].pos_score()))\n",
    "print('negative score for the word \"happy\": {0}'.format(list(swn.senti_synsets('happy','a'))[0].neg_score()))\n",
    "print('neutral score for the word \"happy\": {0}'.format(list(swn.senti_synsets('happy','a'))[0].obj_score()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "sentence = 'i am happy'\n",
    "newSentence = []\n",
    "for word in tokens:\n",
    "    if word not in stop:\n",
    "        newSentence.append(word)\n",
    "\n",
    "print('The sentence has been reduced from \\'{0}\\' \\n to \\'{1}\\''.format(sentence, newSentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\petersont/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\petersont\\\\Miniconda\\\\nltk_data'\n    - 'C:\\\\Users\\\\petersont\\\\Miniconda\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\petersont\\\\Miniconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\petersont\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-cfb2b716dab4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'i am happy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tokens: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[1;32m~\\Miniconda\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\petersont/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\petersont\\\\Miniconda\\\\nltk_data'\n    - 'C:\\\\Users\\\\petersont\\\\Miniconda\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\petersont\\\\Miniconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\petersont\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sentence = 'i am happy'\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "print('Tokens: {0}'.format(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "stop = stopwords.words('english')\n",
    "sentimentDict = {}\n",
    "for tweet in tweets:\n",
    "    newSentence = []\n",
    "    sentenceSentiment = 0.\n",
    "    for word in tokens:\n",
    "        if word not in stop:\n",
    "            newSentence.append(word)\n",
    "            swn.senti_synsets(word,'a')[0].neg_score()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix typos\n",
    "# handle words not in sentiwordnet by giving 0\n",
    "\n",
    "0 if word not in sentiwordnet else senti_synsets(word,'a')[0].neg_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
