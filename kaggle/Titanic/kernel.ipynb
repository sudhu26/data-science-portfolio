{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kaggle competition - Titanic survivorship__\n",
    "\n",
    "1. [Import](#Import)\n",
    "    1. [Tools](#Tools)\n",
    "    1. [Data](#Data)    \n",
    "1. [EDA](#EDA)\n",
    "    1. [Categorical feature EDA](#Categorical-feature-EDA)\n",
    "    1. [numeric feature EDA](#numeric-feature-EDA)\n",
    "    1. [Faceting](#Faceting)\n",
    "    1. [Target variable evaluation](#Target-variable-evaluation)    \n",
    "1. [Data preparation](#Data-preparation)\n",
    "    1. [Missing data](#Missing-data)\n",
    "    1. [Engineering](#Engineering)\n",
    "    1. [Encoding](#Encoding)\n",
    "    1. [Transformation](#Transformation)\n",
    "        1. [Polynomial features](#Polynomial-features)\n",
    "        1. [Skew](#Skew)\n",
    "        1. [Scale](#Scale)\n",
    "    1. [Outliers](#Outliers)\n",
    "1. [Feature importance](#Feature-importance)    \n",
    "1. [Modeling](#Modeling)\n",
    "    1. [Data preparation](#Data-preparation-1)\n",
    "    1. [Bayesian hyper-parameter optimization](#Bayesian-hyper-parameter-optimization)\n",
    "    1. [Model performance evaluation - standard models](#Model-performance-evaluation-standard-models)\n",
    "    1. [Model explanability](#Model-explanability)\n",
    "    1. [Submission - standard models](#Submission-standard-models)\n",
    "1. [Stacking](#Stacking)\n",
    "    1. [Primary models](#Primary-models)\n",
    "    1. [Meta model](#Meta-model)                \n",
    "    1. [Model performance evaluation - stacked models](#Model-performance-evaluation-stacked-models)\n",
    "    1. [Submission - stacked models](#Submission-stacked-models)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T00:26:19.191249Z",
     "start_time": "2019-10-30T00:26:16.677041Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from functools import reduce\n",
    "import time; rundate = time.strftime(\"%Y%m%d\")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 500); pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.impute as impute\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "\n",
    "import lightgbm\n",
    "import xgboost\n",
    "\n",
    "from hyperopt import hp\n",
    "\n",
    "import eif\n",
    "import shap\n",
    "shap.initjs()\n",
    "# from eli5.sklearn import PermutationImportance\n",
    "# from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import category_encoders as ce\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    #     import mlmachine as mlm\n",
    "    #     from prettierplot.plotter import PrettierPlot\n",
    "    #     import prettierplot.style as style\n",
    "    import asdfasd\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(\n",
    "        \"../../../mlmachine\"\n",
    "    ) if \"../../../../mlmachine\" not in sys.path else None\n",
    "    sys.path.append(\n",
    "        \"../../../prettierplot\"\n",
    "    ) if \"../../../../prettierplot\" not in sys.path else None\n",
    "\n",
    "    import mlmachine as mlm\n",
    "    import mlmachine.data as data\n",
    "    from mlmachine.features.preprocessing import (\n",
    "        DataFrameSelector,\n",
    "        PlayWithPandas,\n",
    "        UnprocessedColumnAdder,\n",
    "        KFoldTargetEncoderTrain,\n",
    "        ContextImputer,\n",
    "        PandasFeatureUnion,\n",
    "        DualTransformer,\n",
    "    )\n",
    "    from prettierplot.plotter import PrettierPlot\n",
    "    import prettierplot.style as style\n",
    "else:\n",
    "    print(\n",
    "        \"This notebook relies on the libraries mlmachine and prettierplot. Please run:\"\n",
    "    )\n",
    "    print(\"\\tpip install mlmachine\")\n",
    "    print(\"\\tpip install prettierplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:36.535340Z",
     "start_time": "2019-10-14T03:26:33.570723Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load data and print dimensions\n",
    "dfTrain, dfValid = data.titanic()\n",
    "# dfTrain = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-titanic/train.csv\")\n",
    "# dfValid = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-titanic/test.csv\")\n",
    "\n",
    "print(\"Training data dimensions: {}\".format(dfTrain.shape))\n",
    "print(\"Validation data dimensions: {}\".format(dfValid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:36.557215Z",
     "start_time": "2019-10-14T03:26:36.538715Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# display info and first 5 rows\n",
    "dfTrain.info()\n",
    "display(dfTrain[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:36.566421Z",
     "start_time": "2019-10-14T03:26:36.560312Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# review counts of different column types\n",
    "dfTrain.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:36.582603Z",
     "start_time": "2019-10-14T03:26:36.568557Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load training data into mlmachine\n",
    "train = mlm.Machine(\n",
    "    data=dfTrain,\n",
    "    target=\"Survived\",\n",
    "    removeFeatures=[\"PassengerId\", \"Ticket\"],\n",
    "    overrideCat=[\"Pclass\"],\n",
    "    targetType=\"categorical\",\n",
    ")\n",
    "print(train.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:36.596891Z",
     "start_time": "2019-10-14T03:26:36.584580Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load training data into mlmachine\n",
    "valid = mlm.Machine(\n",
    "    data=dfValid,\n",
    "    removeFeatures=[\"PassengerId\", \"Ticket\"],\n",
    "    overrideCat=[\"Pclass\"],\n",
    ")\n",
    "print(valid.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Categorical feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Categorical-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:37.917337Z",
     "start_time": "2019-10-14T03:26:36.608581Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# categorical features\n",
    "for feature in train.featureType[\"categorical\"]:\n",
    "    train.edaCatTargetCatFeat(feature=feature, levelCountCap=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## numeric feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'numeric-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:41.376088Z",
     "start_time": "2019-10-14T03:26:37.922097Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numeric features\n",
    "for feature in train.featureType[\"numeric\"]:\n",
    "    train.edaCatTargetNumFeat(feature=feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Correlation (all samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:41.720743Z",
     "start_time": "2019-10-14T03:26:41.379561Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmap(df=train.data, annot=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:41.942775Z",
     "start_time": "2019-10-14T03:26:41.722910Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot(plotOrientation='tall',chartProp=10)\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmapTarget(\n",
    "    df=train.data, target=train.target, thresh=0.01, annot=True, ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Pair plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Pair-plot'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:41.947642Z",
     "start_time": "2019-10-14T03:26:41.944976Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # pair plot\n",
    "# p = PrettierPlot(chartProp=12)\n",
    "# p.prettyPairPlot(df=train.data[[\"Age\",\"Fare\",\"Sex\",\"Pclass\",\"Embarked\"]].dropna(), diag_kind=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:41.952506Z",
     "start_time": "2019-10-14T03:26:41.949891Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # pair plot\n",
    "# p = PrettierPlot(chartProp=12)\n",
    "# p.prettyPairPlot(\n",
    "#     df=train.data.dropna(),\n",
    "#     diag_kind=\"kde\",\n",
    "#     target=train.target,\n",
    "#     cols=[\"Age\", \"Fare\", \"Pclass\", \"Parch\", \"SibSp\"],\n",
    "#     legendLabels=[\"Died\", \"Survived\"],\n",
    "#     bbox=(2.0, 0.0),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Faceting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Faceting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Categorical by categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:42.248948Z",
     "start_time": "2019-10-14T03:26:41.954765Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chartProp=12)\n",
    "ax = p.makeCanvas(title=\"Survivorship, embark location by passenger class\", yShift=0.7)\n",
    "p.prettyFacetTwoCatBar(\n",
    "    df=train.recombineData(train.data, train.target),\n",
    "    x=\"Embarked\",\n",
    "    y=train.target.name,\n",
    "    split=\"Pclass\",\n",
    "    yUnits=\"ff\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:42.512662Z",
     "start_time": "2019-10-14T03:26:42.251383Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chartProp=12)\n",
    "ax = p.makeCanvas(title=\"Survivorship, passenger class by gender\", yShift=0.7)\n",
    "p.prettyFacetTwoCatBar(\n",
    "    df=train.recombineData(train.data, train.target),\n",
    "    x=\"Pclass\",\n",
    "    y=train.target.name,\n",
    "    split=\"Sex\",\n",
    "    yUnits=\"ff\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:42.773031Z",
     "start_time": "2019-10-14T03:26:42.514757Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chartProp=12)\n",
    "ax = p.makeCanvas(title=\"Survivorship,embark location by gender\", yShift=0.7)\n",
    "p.prettyFacetTwoCatBar(\n",
    "    df=train.recombineData(train.data, train.target),\n",
    "    x=\"Embarked\",\n",
    "    y=train.target.name,\n",
    "    split=\"Sex\",\n",
    "    yUnits=\"ff\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:43.357122Z",
     "start_time": "2019-10-14T03:26:42.775382Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.prettyFacetTwoCatPoint(\n",
    "    df=train.recombineData(train.data, train.target),\n",
    "    x=\"Sex\",\n",
    "    y=train.target.name,\n",
    "    split=\"Pclass\",\n",
    "    catCol=\"Embarked\",\n",
    "    aspect=1.0,\n",
    "    height=5,\n",
    "    bbox=(1.3, 1.2),\n",
    "    legendLabels=[\"1st class\", \"2nd class\", \"3rd class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:43.777368Z",
     "start_time": "2019-10-14T03:26:43.359339Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.prettyFacetTwoCatPoint(\n",
    "    df=train.recombineData(train.data, train.target).dropna(subset=[\"Embarked\"]),\n",
    "    x=\"Embarked\",\n",
    "    y=train.target.name,\n",
    "    split=\"Pclass\",\n",
    "    catCol=\"Sex\",\n",
    "    aspect=1.0,\n",
    "    height=5,\n",
    "    bbox=(1.5, 0.8),\n",
    "    legendLabels=[\"1st class\", \"2nd class\", \"3rd class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Categorical by numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:45.355100Z",
     "start_time": "2019-10-14T03:26:43.779548Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.prettyFacetCatNumHist(\n",
    "    df=train.recombineData(train.data, train.target),\n",
    "    split=train.target.name,\n",
    "    legendLabels=[\"Died\", \"Lived\"],\n",
    "    catRow=\"Sex\",\n",
    "    catCol=\"Embarked\",\n",
    "    numCol=\"Age\",\n",
    "    bbox=(1.9, 1.0),\n",
    "    height=4,\n",
    "    aspect=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:46.638472Z",
     "start_time": "2019-10-14T03:26:45.357291Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot(chartProp=15)\n",
    "p.prettyFacetCatNumScatter(\n",
    "    df=train.recombineData(train.data, train.target),\n",
    "    split=train.target.name,\n",
    "    legendLabels=[\"Died\", \"Lived\"],\n",
    "    catRow=\"Sex\",\n",
    "    catCol=\"Embarked\",\n",
    "    x=\"Fare\",\n",
    "    y=\"Age\",\n",
    "    bbox=(1.9, 1.0),\n",
    "    height=4,\n",
    "    aspect=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Target-variable-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:46.647326Z",
     "start_time": "2019-10-14T03:26:46.640684Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# null score\n",
    "pd.Series(train.target).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Data-preparation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Missing-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:46.897740Z",
     "start_time": "2019-10-14T03:26:46.649385Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "train.edaMissingSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:47.581572Z",
     "start_time": "2019-10-14T03:26:46.900059Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno matrix\n",
    "msno.matrix(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:48.488051Z",
     "start_time": "2019-10-14T03:26:47.583619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno bar\n",
    "msno.bar(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:49.008249Z",
     "start_time": "2019-10-14T03:26:48.490406Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno heatmap\n",
    "msno.heatmap(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:49.521641Z",
     "start_time": "2019-10-14T03:26:49.010444Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno dendrogram\n",
    "msno.dendrogram(train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:49.762236Z",
     "start_time": "2019-10-14T03:26:49.529376Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "valid.edaMissingSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:50.422757Z",
     "start_time": "2019-10-14T03:26:49.766448Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno matrix\n",
    "msno.matrix(valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:51.323533Z",
     "start_time": "2019-10-14T03:26:50.424751Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno bar\n",
    "msno.bar(valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:51.830850Z",
     "start_time": "2019-10-14T03:26:51.326027Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno heatmap\n",
    "msno.heatmap(valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:52.369979Z",
     "start_time": "2019-10-14T03:26:51.833824Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno dendrogram\n",
    "msno.dendrogram(valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training vs. validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:52.381312Z",
     "start_time": "2019-10-14T03:26:52.372066Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "train.missingColCompare(train=train.data, validation=valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:52.541218Z",
     "start_time": "2019-10-14T03:26:52.490910Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# impute pipeline\n",
    "imputePipe = PandasFeatureUnion([\n",
    "    (\"age\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Age\",\"Pclass\"]),\n",
    "        ContextImputer(nullCol=\"Age\", contextCol=\"Pclass\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"fare\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Fare\",\"Pclass\"]),\n",
    "        ContextImputer(nullCol=\"Fare\", contextCol=\"Pclass\")\n",
    "    )),\n",
    "    (\"embarked\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Embarked\"]),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"cabin\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Cabin\"]),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"constant\", fill_value=\"X\"))\n",
    "    )),\n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference([\"Age\",\"Fare\",\"Embarked\",\"Cabin\"]))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = imputePipe.fit_transform(train.data)\n",
    "valid.data = imputePipe.transform(valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:52.552263Z",
     "start_time": "2019-10-14T03:26:52.543334Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train.edaMissingSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:52.564475Z",
     "start_time": "2019-10-14T03:26:52.555175Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid.edaMissingSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:52.612363Z",
     "start_time": "2019-10-14T03:26:52.566637Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train.data[\"Name\"]]\n",
    "train.data[\"Title\"] = pd.Series(\n",
    "    title,\n",
    "    index=train.data.index,\n",
    "    dtype=\"category\",\n",
    ")\n",
    "train.data[\"Title\"] = train.data[\"Title\"].replace(\n",
    "    [\n",
    "        \"Lady\",\n",
    "        \"the Countess\",\n",
    "        \"Countess\",\n",
    "        \"Capt\",\n",
    "        \"Col\",\n",
    "        \"Don\",\n",
    "        \"Dr\",\n",
    "        \"Major\",\n",
    "        \"Rev\",\n",
    "        \"Sir\",\n",
    "        \"Jonkheer\",\n",
    "        \"Dona\",\n",
    "    ],\n",
    "    \"Rare\",\n",
    ")\n",
    "train.data[\"Title\"] = train.data[\"Title\"].map(\n",
    "    {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    ")\n",
    "train.data[\"Title\"] = train.data[\"Title\"].astype(\"category\")\n",
    "\n",
    "# distill cabin feature\n",
    "train.data[\"CabinQuarter\"] = pd.Series(\n",
    "    [i[0] if not pd.isnull(i) else \"X\" for i in train.data[\"Cabin\"]],\n",
    "    index=train.data.index,\n",
    "    dtype=\"category\",\n",
    ")\n",
    "\n",
    "# family size features and binning\n",
    "train.data[\"FamilySize\"] = train.data[\"SibSp\"] + train.data[\"Parch\"] + 1\n",
    "train.data[\"FamilySize\"] = train.data[\"FamilySize\"].astype(\"int64\")\n",
    "\n",
    "# update featureType and drop columns\n",
    "train.featureTypeUpdate(columnsToDrop=[\"Name\",\"Cabin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:59.378938Z",
     "start_time": "2019-10-14T03:26:59.353519Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(\",\")[1].split(\".\")[0].strip() for i in valid.data[\"Name\"]]\n",
    "valid.data[\"Title\"] = pd.Series(\n",
    "    title,\n",
    "    index=valid.data.index,\n",
    "    dtype=\"category\",\n",
    ")\n",
    "valid.data[\"Title\"] = valid.data[\"Title\"].replace(\n",
    "    [\n",
    "        \"Lady\",\n",
    "        \"the Countess\",\n",
    "        \"Countess\",\n",
    "        \"Capt\",\n",
    "        \"Col\",\n",
    "        \"Don\",\n",
    "        \"Dr\",\n",
    "        \"Major\",\n",
    "        \"Rev\",\n",
    "        \"Sir\",\n",
    "        \"Jonkheer\",\n",
    "        \"Dona\",\n",
    "    ],\n",
    "    \"Rare\",\n",
    ")\n",
    "valid.data[\"Title\"] = valid.data[\"Title\"].map(\n",
    "    {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    ")\n",
    "valid.data[\"Title\"] = valid.data[\"Title\"].astype(\"category\")\n",
    "\n",
    "# distill cabin feature\n",
    "valid.data[\"CabinQuarter\"] = pd.Series(\n",
    "    [i[0] if not pd.isnull(i) else \"X\" for i in valid.data[\"Cabin\"]],\n",
    "    index=valid.data.index,\n",
    "    dtype=\"category\",\n",
    ")\n",
    "\n",
    "# additional features\n",
    "valid.data[\"FamilySize\"] = valid.data[\"SibSp\"] + valid.data[\"Parch\"] + 1\n",
    "valid.data[\"FamilySize\"] = valid.data[\"FamilySize\"].astype(\"int64\")\n",
    "\n",
    "# update featureType and drop columns\n",
    "valid.featureTypeUpdate(columnsToDrop=[\"Name\",\"Cabin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Encoding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:59.402401Z",
     "start_time": "2019-10-14T03:26:59.380987Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in training data string columns\n",
    "train.data[train.featureType[\"categorical\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:59.412744Z",
     "start_time": "2019-10-14T03:26:59.404616Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in train.data[train.featureType[\"categorical\"]]:\n",
    "    print(col, np.unique(train.data[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:59.427370Z",
     "start_time": "2019-10-14T03:26:59.415272Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in validation data string columns\n",
    "valid.data[valid.featureType[\"categorical\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:59.439356Z",
     "start_time": "2019-10-14T03:26:59.430821Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in valid.data[valid.featureType[\"categorical\"]]:\n",
    "    print(col, np.unique(valid.data[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training vs. validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:59.455159Z",
     "start_time": "2019-10-14T03:26:59.441524Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify values that are present in the training data but not the validation data, and vice versa\n",
    "for col in train.featureType[\"categorical\"]:\n",
    "    if col not in [\"Name\", \"Cabin\"]:\n",
    "        trainValues = train.data[col].unique()\n",
    "        validValues = valid.data[col].unique()\n",
    "\n",
    "        trainDiff = set(trainValues) - set(validValues)\n",
    "        validDiff = set(validValues) - set(trainValues)\n",
    "\n",
    "        if len(trainDiff) > 0 or len(validDiff) > 0:\n",
    "            print(\"\\n\\n*** \" + col)\n",
    "            print(\"Value present in training data, not in validation data\")\n",
    "            print(trainDiff)\n",
    "            print(\"Value present in validation data, not in training data\")\n",
    "            print(validDiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:59.490786Z",
     "start_time": "2019-10-14T03:26:59.457041Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "nominalColumns = [\"Embarked\",\"Sex\",\"CabinQuarter\",\"Title\"]\n",
    "\n",
    "ordinalColumns = [\"Pclass\"]\n",
    "ordinalEncodings = [\n",
    "        [0, 1, 2, 3], # Pclass \n",
    "    ]\n",
    "\n",
    "# encode pipeline\n",
    "encodePipe = PandasFeatureUnion([\n",
    "    (\"ordinal\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(ordinalColumns),\n",
    "        PlayWithPandas(preprocessing.OrdinalEncoder(categories=ordinalEncodings)),\n",
    "    )),\n",
    "    (\"nominal\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(nominalColumns),\n",
    "        PlayWithPandas(preprocessing.OneHotEncoder(drop=\"first\")),\n",
    "    )),\n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference(nominalColumns + ordinalColumns))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = encodePipe.fit_transform(train.data)\n",
    "valid.data = encodePipe.transform(valid.data)\n",
    "\n",
    "train.featureTypeUpdate()\n",
    "valid.featureTypeUpdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Transformation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Polynomial-features'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:26:59.552883Z",
     "start_time": "2019-10-14T03:26:59.501470Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transform pipe\n",
    "polynomialPipe = PandasFeatureUnion([\n",
    "    (\"polynomial\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(train.featureType[\"numeric\"]),\n",
    "        PlayWithPandas(preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=False))\n",
    "    )),\n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference(train.featureType[\"numeric\"]))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = polynomialPipe.fit_transform(train.data)\n",
    "valid.data = polynomialPipe.transform(valid.data)\n",
    "\n",
    "train.featureTypeUpdate()\n",
    "valid.featureTypeUpdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Skew'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:27:00.389100Z",
     "start_time": "2019-10-14T03:26:59.555159Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of numeric features - training data\n",
    "train.skewSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:27:01.038045Z",
     "start_time": "2019-10-14T03:27:00.391495Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of numeric features - validation data\n",
    "valid.skewSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:27:02.607342Z",
     "start_time": "2019-10-14T03:27:01.040526Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# skew correction pipeline\n",
    "skewPipe = PandasFeatureUnion([\n",
    "    (\"skew\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(train.featureType[\"numeric\"]),\n",
    "        DualTransformer(),\n",
    "    )),    \n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference(train.featureType[\"numeric\"]))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = skewPipe.fit_transform(train.data)\n",
    "valid.data = skewPipe.transform(valid.data)\n",
    "\n",
    "train.featureTypeUpdate()\n",
    "valid.featureTypeUpdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Scale'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:27:07.114265Z",
     "start_time": "2019-10-14T03:27:02.886503Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "scalePipe = PandasFeatureUnion([\n",
    "    (\"scale\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(train.featureType[\"numeric\"]),\n",
    "        PlayWithPandas(preprocessing.StandardScaler())\n",
    "    )),\n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference(train.featureType[\"numeric\"]))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = scalePipe.fit_transform(train.data)\n",
    "valid.data = scalePipe.transform(valid.data)\n",
    "\n",
    "train.featureTypeUpdate()\n",
    "valid.featureTypeUpdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Outliers'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:27:08.241826Z",
     "start_time": "2019-10-14T03:27:07.116525Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    (\"outlier\",train.OutlierIQR(\n",
    "                outlierCount=25,\n",
    "                iqrStep=1.5,\n",
    "                features=train.featureType[\"numeric\"],\n",
    "                dropOutliers=False,))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqrOutliers = np.array(sorted(trainPipe.named_steps[\"outlier\"].outliers_))\n",
    "print(iqrOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:27:09.798429Z",
     "start_time": "2019-10-14T03:27:08.244138Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = ensemble.IsolationForest(\n",
    "    behaviour=\"new\", max_samples=train.data.shape[0], random_state=0, contamination=0.01\n",
    ")\n",
    "clf.fit(train.data[train.data.columns])\n",
    "preds = clf.predict(train.data[train.data.columns])\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)\n",
    "ifOutliers = np.array(train.data[mask].index)\n",
    "print(ifOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:27:16.701377Z",
     "start_time": "2019-10-14T03:27:09.800890Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers using extended isolation forest\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    (\"outlier\",train.ExtendedIsoForest(\n",
    "                cols=train.featureType[\"numeric\"],\n",
    "                nTrees=100,\n",
    "                sampleSize=256,\n",
    "                ExtensionLevel=1,\n",
    "                anomaliesRatio=0.03,\n",
    "                dropOutliers=False,))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "eifOutliers = np.array(sorted(trainPipe.named_steps[\"outlier\"].outliers_))\n",
    "print(eifOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:27:16.707468Z",
     "start_time": "2019-10-14T03:27:16.703534Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "outliers = reduce(np.intersect1d, (iqrOutliers, ifOutliers, eifOutliers))\n",
    "# outliers = reduce(np.intersect1d, (ifOutliers, eifOutliers))\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:27:16.727847Z",
     "start_time": "2019-10-14T03:27:16.709576Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# review outlier identification summary\n",
    "outlierSummary = train.outlierSummary(iqrOutliers=iqrOutliers,\n",
    "                             ifOutliers=ifOutliers,\n",
    "                             eifOutliers=eifOutliers\n",
    "                            )\n",
    "outlierSummary[outlierSummary[\"Count\"] >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:27:16.742089Z",
     "start_time": "2019-10-14T03:27:16.730245Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove outlers from predictors and response\n",
    "outliers = np.array([27, 88, 258, 311, 341, 438, 679, 737, 742])\n",
    "train.data = train.data.drop(outliers)\n",
    "train.target = train.target.drop(index=outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T23:37:10.039666Z",
     "start_time": "2019-10-29T23:37:10.035667Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate feature importance summary\n",
    "estimators = [\n",
    "    lightgbm.LGBMClassifier,\n",
    "    ensemble.RandomForestClassifier,\n",
    "    ensemble.GradientBoostingClassifier,\n",
    "    ensemble.ExtraTreesClassifier,\n",
    "    ensemble.AdaBoostClassifier,\n",
    "    xgboost.XGBClassifier,\n",
    "]\n",
    "\n",
    "fs = train.FeatureSelector(\n",
    "    data=train.data, target=train.target, estimators=estimators, rank=True\n",
    ")\n",
    "# featureSelectorSummary = fs.featureSelectorSuite()\n",
    "# featureSelectorSummary[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T17:39:25.589133Z",
     "start_time": "2019-10-28T17:34:52.176736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# calculate cross-validation performance\n",
    "estimators = [\n",
    "    svm.SVC,\n",
    "    lightgbm.LGBMClassifier,\n",
    "    linear_model.LogisticRegression,\n",
    "    xgboost.XGBClassifier,\n",
    "    ensemble.RandomForestClassifier,\n",
    "    ensemble.GradientBoostingClassifier,\n",
    "    ensemble.AdaBoostClassifier,\n",
    "    ensemble.ExtraTreesClassifier,\n",
    "    neighbors.KNeighborsClassifier,\n",
    "]\n",
    "\n",
    "cvSummary = fs.featureSelectorCrossVal(\n",
    "    featureSelectorSummary = pd.read_csv(\"featureSelectionSummary_20191028_173422.csv\", index_col=0),\n",
    "    estimators=estimators,\n",
    "    scoring=[\"accuracy\"],\n",
    "    nFolds=8,\n",
    "    step=1,\n",
    "    nJobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T17:29:01.694033Z",
     "start_time": "2019-10-28T17:28:59.742051Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize CV performance for diminishing feature set\n",
    "fs.featureSelectorResultsPlot(\n",
    "    metric=\"accuracy\",\n",
    "    cvSummary= pd.read_csv(\"cvSummary_20191028_173925.csv\", index_col=0),\n",
    "    featureSelectorSummary=pd.read_csv(\"featureSelectionSummary_20191028_173422.csv\", index_col=0),\n",
    "    titleScale=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T23:37:15.435913Z",
     "start_time": "2019-10-29T23:37:15.363903Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crossValFeaturesDf = fs.createCrossValFeaturesDf(\n",
    "    metric=\"accuracy\",\n",
    "    cvSummary= pd.read_csv(\"cvSummary_20191028_173925.csv\", index_col=0),\n",
    "    featureSelectorSummary=pd.read_csv(\"featureSelectionSummary_20191028_173422.csv\", index_col=0),\n",
    ")\n",
    "crossValFeaturesDf#[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T23:37:15.448912Z",
     "start_time": "2019-10-29T23:37:15.436904Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crossValFeaturesDict = fs.createCrossValFeaturesDict(\n",
    "    crossValFeaturesDf=crossValFeaturesDf\n",
    ")\n",
    "# crossValFeaturesDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-preparation-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T23:37:08.208496Z",
     "start_time": "2019-10-29T23:37:07.984478Z"
    },
    "code_folding": [
     57,
     102
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# import training data\n",
    "dfTrain, dfValid = data.titanic()\n",
    "# dfTrain = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-titanic/train.csv\")\n",
    "train = mlm.Machine(\n",
    "    data=dfTrain,\n",
    "    target=\"Survived\",\n",
    "    removeFeatures=[\"PassengerId\", \"Ticket\"],\n",
    "    overrideCat=[\"Pclass\"],\n",
    "    targetType=\"categorical\",\n",
    ")\n",
    "\n",
    "# import validation data\n",
    "# dfValid = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-titanic/test.csv\")\n",
    "valid = mlm.Machine(\n",
    "    data=dfValid,\n",
    "    removeFeatures=[\"PassengerId\", \"Ticket\"],\n",
    "    overrideCat=[\"Pclass\"],\n",
    ")\n",
    "\n",
    "#################################################################################\n",
    "# impute pipeline\n",
    "imputePipe = PandasFeatureUnion([\n",
    "    (\"age\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Age\",\"Pclass\"]),\n",
    "        ContextImputer(nullCol=\"Age\", contextCol=\"Pclass\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"fare\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Fare\",\"Pclass\"]),\n",
    "        ContextImputer(nullCol=\"Fare\", contextCol=\"Pclass\")\n",
    "    )),\n",
    "    (\"embarked\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Embarked\"]),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"cabin\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Cabin\"]),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"constant\", fill_value=\"X\"))\n",
    "    )),\n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference([\"Age\",\"Fare\",\"Embarked\",\"Cabin\"]))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = imputePipe.fit_transform(train.data)\n",
    "valid.data = imputePipe.transform(valid.data)\n",
    "\n",
    "#################################################################################\n",
    "# feature engineering - training\n",
    "\n",
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train.data[\"Name\"]]\n",
    "train.data[\"Title\"] = pd.Series(\n",
    "    title,\n",
    "    index=train.data.index,\n",
    "    dtype=\"category\",\n",
    ")\n",
    "train.data[\"Title\"] = train.data[\"Title\"].replace(\n",
    "    [\n",
    "        \"Lady\",\n",
    "        \"the Countess\",\n",
    "        \"Countess\",\n",
    "        \"Capt\",\n",
    "        \"Col\",\n",
    "        \"Don\",\n",
    "        \"Dr\",\n",
    "        \"Major\",\n",
    "        \"Rev\",\n",
    "        \"Sir\",\n",
    "        \"Jonkheer\",\n",
    "        \"Dona\",\n",
    "    ],\n",
    "    \"Rare\",\n",
    ")\n",
    "train.data[\"Title\"] = train.data[\"Title\"].map(\n",
    "    {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    ")\n",
    "train.data[\"Title\"] = train.data[\"Title\"].astype(\"category\")\n",
    "\n",
    "# distill cabin feature\n",
    "train.data[\"CabinQuarter\"] = pd.Series(\n",
    "    [i[0] if not pd.isnull(i) else \"X\" for i in train.data[\"Cabin\"]],\n",
    "    index=train.data.index,\n",
    "    dtype=\"category\",\n",
    ")\n",
    "\n",
    "# family size features\n",
    "train.data[\"FamilySize\"] = train.data[\"SibSp\"] + train.data[\"Parch\"] + 1\n",
    "train.data[\"FamilySize\"] = train.data[\"FamilySize\"].astype(\"int64\")\n",
    "\n",
    "train.featureTypeUpdate(columnsToDrop=[\"Name\",\"Cabin\"])\n",
    "\n",
    "#################################################################################\n",
    "# feature engineering - validation\n",
    "\n",
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(\",\")[1].split(\".\")[0].strip() for i in valid.data[\"Name\"]]\n",
    "valid.data[\"Title\"] = pd.Series(\n",
    "    title,\n",
    "    index=valid.data.index,\n",
    "    dtype=\"category\"\n",
    ")\n",
    "valid.data[\"Title\"] = valid.data[\"Title\"].replace(\n",
    "    [\n",
    "        \"Lady\",\n",
    "        \"the Countess\",\n",
    "        \"Countess\",\n",
    "        \"Capt\",\n",
    "        \"Col\",\n",
    "        \"Don\",\n",
    "        \"Dr\",\n",
    "        \"Major\",\n",
    "        \"Rev\",\n",
    "        \"Sir\",\n",
    "        \"Jonkheer\",\n",
    "        \"Dona\",\n",
    "    ],\n",
    "    \"Rare\",\n",
    ")\n",
    "valid.data[\"Title\"] = valid.data[\"Title\"].map(\n",
    "    {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    ")\n",
    "valid.data[\"Title\"] = valid.data[\"Title\"].astype(\"category\")\n",
    "\n",
    "# distill cabin feature\n",
    "valid.data[\"CabinQuarter\"] = pd.Series(\n",
    "    [i[0] if not pd.isnull(i) else \"X\" for i in valid.data[\"Cabin\"]],\n",
    "    index=valid.data.index,\n",
    "    dtype=\"category\",\n",
    ")\n",
    "\n",
    "# additional features\n",
    "valid.data[\"FamilySize\"] = valid.data[\"SibSp\"] + valid.data[\"Parch\"] + 1\n",
    "valid.data[\"FamilySize\"] = valid.data[\"FamilySize\"].astype(\"int64\")\n",
    "\n",
    "valid.featureTypeUpdate(columnsToDrop=[\"Name\",\"Cabin\"])\n",
    "\n",
    "# #################################################################################\n",
    "# # feature transformation pipeline\n",
    "# nominalColumns = [\"Embarked\",\"Sex\",\"CabinQuarter\",\"Title\"]\n",
    "\n",
    "# ordinalColumns = [\"Pclass\"]\n",
    "# ordinalEncodings = [\n",
    "#         [0, 1, 2, 3], # Pclass \n",
    "#     ]\n",
    "\n",
    "# transformPipe = PandasFeatureUnion([\n",
    "#     (\"ordinal\", pipeline.make_pipeline(\n",
    "#         DataFrameSelector(ordinalColumns),\n",
    "#         PlayWithPandas(preprocessing.OrdinalEncoder(categories=ordinalEncodings)),\n",
    "#     )),\n",
    "#     (\"nominal\", pipeline.make_pipeline(\n",
    "#         DataFrameSelector(nominalColumns),\n",
    "#         PlayWithPandas(preprocessing.OneHotEncoder(drop=\"first\")),\n",
    "#     )),\n",
    "#     (\"numeric\", pipeline.make_pipeline(\n",
    "#         DataFrameSelector(train.featureType[\"numeric\"]),\n",
    "#         PlayWithPandas(preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)),\n",
    "# #         DualTransformer(),\n",
    "#         PlayWithPandas(preprocessing.StandardScaler())\n",
    "#     )),\n",
    "#     (\"diff\", pipeline.make_pipeline(\n",
    "#         DataFrameSelector(list(set(train.data.columns).difference(nominalColumns + ordinalColumns + train.featureType[\"numeric\"]))),\n",
    "#     )),\n",
    "# ])\n",
    "\n",
    "# train.data = transformPipe.fit_transform(train.data)\n",
    "# valid.data = transformPipe.transform(valid.data)\n",
    "\n",
    "# train.featureTypeUpdate()\n",
    "# valid.featureTypeUpdate()\n",
    "\n",
    "# #################################################################################\n",
    "# # remove outliers\n",
    "# outliers = np.array([27, 88, 258, 311, 341, 438, 679, 737, 742])\n",
    "# train.data = train.data.drop(outliers)\n",
    "# train.target = train.target.drop(index=outliers)\n",
    "\n",
    "# # # accuracy >= 7\n",
    "# # bestCols = ['Age*Fare','Title_2','Fare*FamilySize','Sex_male','Fare','Pclass','CabinQuarter_X']\n",
    "# # train.data = train.data[bestCols]\n",
    "# # valid.data = valid.data[bestCols]\n",
    "\n",
    "# print('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Category encoding\n",
    "\n",
    "- to do\n",
    "    - feature engineering\n",
    "        - target encoding\n",
    "            - categorical target \n",
    "                - categorical features\n",
    "                - numeric features\n",
    "                    - bin first (ability to controls bins)\n",
    "                    - then target encode the binned numeric features against the categorical target\n",
    "            - numeric target\n",
    "                - categorical features\n",
    "                - numeric features\n",
    "            - general\n",
    "                - feature names suffixed with \"_[FEATURE NAME]\"\n",
    "                - methodologies\n",
    "                    - categorical target\n",
    "                        - categorical features\n",
    "                            - 1 - blend of posterior probability of target given a particular level over(?) the prior probability of the target over all training data.\n",
    "                                - What exactly does blending mean?\n",
    "                            - 2 - just posterior probability of target given a particular level\n",
    "                        - numeric features\n",
    "                            - binning of numeric features, plus the categorical feature methodologies described above\n",
    "                    - continuous target\n",
    "                        - categorical features\n",
    "                            - blend of mean value of target given a particular categorical level and the expected value of the target over(?) all of the training data\n",
    "                                - what exactly does blending mean?\n",
    "                        - numeric features\n",
    "                            - binning of numeric features, plus the categorical feature methodologies described above\n",
    "                        - general\n",
    "                            - can take mean, median, mode, std\n",
    "\n",
    "        - count encoding\n",
    "            - categorical features\n",
    "                - count ofthe total number of appearances of that level\n",
    "            - numeric features\n",
    "                - not sure that his is possible\n",
    "            - general\n",
    "                k fold this? probably\n",
    "        - binary encoding\n",
    "            - categorical features\n",
    "                - When the number of levels in a categorical features reaches a certain level\n",
    "            - numeric features\n",
    "                - not sure that his is possible\n",
    "        - optimal binning (the maxhalfford.github site)\n",
    "        - additive smoothing (the maxhalfford.github site)\n",
    "        - binning\n",
    "            - categorical features\n",
    "                - not necessary\n",
    "            - numeric features\n",
    "                - for each value, replace with the bin label\n",
    "        - numeric-specific transformations\n",
    "            - Percentiles\n",
    "                - for each value, replace with percentile of that value. i could see this helping to diminish the effect of outliers\n",
    "            - ratios/quotients\n",
    "            - sums\n",
    "            - products\n",
    "            - differences\n",
    "    - model evaluation\n",
    "        - incorrect predictions\n",
    "            - false positives\n",
    "            - false negatives\n",
    "   \n",
    "\n",
    "__feature selection ideas__\n",
    "\n",
    "- Single Feature performance\n",
    "    - determine performance for each feature individually\n",
    "    - by training a model like XGBoost on each feature seperately or calculating gini coefficients on binned versions of the features.\n",
    "- forward selection (check out mlxtend)\n",
    "- backwards elimination (i think i already do this with RFE)\n",
    "- noise injection to idenitfy unimportant featuers\n",
    "    - i believe the idea is that any actual features that are less important than the noise featuers can be considered worthless\n",
    "\n",
    "__cross validation__\n",
    "\n",
    "- 5 fold\n",
    "- use stratified k-fold when the test data is not in the future\n",
    "\n",
    "\n",
    "- http://blog.kaggle.com/2016/04/08/homesite-quote-conversion-winners-write-up-1st-place-kazanova-faron-clobber/\n",
    "- https://www.kaggle.com/matleonard/categorical-encodings\n",
    "- http://www.kazanovaforanalytics.com/software.html\n",
    "- https://github.com/Far0n/xgbfi\n",
    "- https://towardsdatascience.com/my-secret-sauce-to-be-in-top-2-of-a-kaggle-competition-57cff0677d3c\n",
    "- https://contrib.scikit-learn.org/categorical-encoding/\n",
    "- https://github.com/scikit-learn-contrib/categorical-encoding/tree/master/category_encoders\n",
    "- https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/\n",
    "- http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/target-encoding.html\n",
    "- https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "from collections import defaultdict\n",
    "\n",
    "X = train.data\n",
    "X_t = train.target\n",
    "Y = valid.data\n",
    "\n",
    "imputePipe = PandasFeatureUnion([\n",
    "#     (\"catcount\", pipeline.make_pipeline(\n",
    "#         DataFrameSelector([\"Embarked\",\"CabinQuarter\"]),\n",
    "#         PlayWithPandas(ce.CountEncoder(return_df=False))\n",
    "#     )),\n",
    "#     (\"binar\", pipeline.make_pipeline(\n",
    "#         DataFrameSelector([\"Embarked\",\"CabinQuarter\"]),\n",
    "#         PlayWithPandas(ce.BinaryEncoder(return_df=False))\n",
    "#     )),\n",
    "#     (\"nominal\", pipeline.make_pipeline(\n",
    "#         DataFrameSelector([\"Embarked\",\"CabinQuarter\"]),\n",
    "#         PlayWithPandas(preprocessing.OneHotEncoder(drop=\"first\")),\n",
    "#     )),\n",
    "    (\"num3\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Embarked\",\"Age\",\"Fare\"]),\n",
    "        KFoldTargetEncoderTrain(\n",
    "            target=train.target,\n",
    "            cv=model_selection.KFold(n_splits=5, shuffle=False, random_state=0),\n",
    "            nBins = {\"Age\" : 5, \"Fare\" : 10},\n",
    "            dropBinCols=False,\n",
    "            dropOriginalCols=True,\n",
    "        )\n",
    "    )),\n",
    "    (\"norm\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Age\",\"Fare\"]),\n",
    "        PlayWithPandas(preprocessing.QuantileTransformer(output_distribution=\"normal\")),\n",
    "    )),\n",
    "    (\"uni\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Age\",\"Fare\"]),\n",
    "        PlayWithPandas(preprocessing.QuantileTransformer(output_distribution=\"uniform\")),\n",
    "    )),\n",
    "#     (\"nu2\", pipeline.make_pipeline(\n",
    "#         DataFrameSelector([\"Age\",\"Fare\"]),\n",
    "#         PlayWithPandas(preprocessing.KBinsDiscretizer(n_bins=10, encode=\"ordinal\")),\n",
    "#     )),\n",
    "    (\"select2\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"Embarked\",\"Age\",\"Fare\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "X = imputePipe.fit_transform(train.data)\n",
    "y = imputePipe.transform(valid.data)\n",
    "display(X[:5])\n",
    "display(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools  \n",
    "\n",
    "df = X[[\"Age\",\"Fare\",\"Age_Quantile_normal\",\"Fare_Quantile_uniform\"]]\n",
    "\n",
    "# create list of tuples containing columns pairs\n",
    "pairs = list(itertools.product(df, df))\n",
    "\n",
    "# remove tuples if both items in pair are the same\n",
    "for x in pairs:\n",
    "    if len(set(x)) == 1:\n",
    "        pairs = [i for i in pairs if i != x]\n",
    "\n",
    "# if two tuples contain the same two items, keep only one\n",
    "for x in pairs:\n",
    "    if pairs[0][::-1] in pairs[1:]:\n",
    "        pairs = pairs[1:]\n",
    "\n",
    "#\n",
    "for x in pairs:\n",
    "    df[\"{} + {}\".format(x[0],x[1])] = df[x[0]] + df[x[1]]\n",
    "    df[\"{} / {}\".format(x[0],x[1])] = df[x[0]] / df[x[1]]\n",
    "    df[\"{} * {}\".format(x[0],x[1])] = df[x[0]] * df[x[1]]\n",
    "\n",
    "df = df.merge(train.target, left_index=True, right_index=True)\n",
    "\n",
    "XX = df.iloc[:,:-1]\n",
    "YY = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"col\",\"score\"])\n",
    "\n",
    "import xgboost\n",
    "import sklearn.linear_model as linear_model\n",
    "for x in XX.columns:\n",
    "#     obj = linear_model.LogisticRegression()\n",
    "    obj = xgboost.XGBClassifier()\n",
    "\n",
    "    obj.fit(XX[[x]], YY)\n",
    "    preds = obj.predict(XX[[x]])\n",
    "    \n",
    "    acc = sum(YY == preds) / len(YY)\n",
    "    results.loc[len(results)] = [x, acc]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values([\"score\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = d[[\"Age\"]].drop_duplicates()\n",
    "ix = np.arange(1, len(age.index)+1)\n",
    "plt.plot(ix, age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare = d[[\"Fare\"]].drop_duplicates()\n",
    "ix = np.arange(1, len(fare.index)+1)\n",
    "plt.plot(ix, fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[[\"Age\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.filter(regex=\"RAD|CHAS\")[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bayesian hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Bayesian-hyper-parameter-optimization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T23:37:19.676629Z",
     "start_time": "2019-10-29T23:37:19.639625Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parameter space\n",
    "allSpace = {\n",
    "    \"lightgbm.LGBMClassifier\": {\n",
    "        \"class_weight\": hp.choice(\"class_weight\", [None]),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"dart\"]),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.15, 0.25),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(4, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.quniform(\"min_child_samples\", 50, 150, 5),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.quniform(\"num_leaves\", 30, 70, 1),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.75, 1.25),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.quniform(\"subsample_for_bin\", 100000, 350000, 20000),\n",
    "    },\n",
    "    \"linear_model.LogisticRegression\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.04, 0.1),\n",
    "        \"penalty\": hp.choice(\"penalty\", [\"l1\"]),\n",
    "    },\n",
    "    \"xgboost.XGBClassifier\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"gamma\": hp.quniform(\"gamma\", 0.0, 10, 0.05),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.2, 0.01),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_child_weight\": hp.quniform(\"min_child_weight\", 2.5, 7.5, 1),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.4, 0.7),\n",
    "    },\n",
    "    \"ensemble.RandomForestClassifier\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 10, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 8000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(15, 25, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 11, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.09, 0.01),\n",
    "        \"loss\": hp.choice(\"loss\", [\"deviance\", \"exponential\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.AdaBoostClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.1, 0.25, 0.01),\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"SAMME\"]),\n",
    "    },\n",
    "    \"ensemble.ExtraTreesClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(4, 30, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\"]),\n",
    "        \"criterion\": hp.choice(\"criterion\", [\"entropy\"]),\n",
    "    },\n",
    "    \"svm.SVC\": {\n",
    "        \"C\": hp.uniform(\"C\", 4, 15),\n",
    "        \"decision_function_shape\": hp.choice(\"decision_function_shape\", [\"ovr\"]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.00000001, 1.5),\n",
    "    },\n",
    "    \"neighbors.KNeighborsClassifier\": {\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"ball_tree\", \"brute\"]),\n",
    "        \"n_neighbors\": hp.choice(\"n_neighbors\", np.arange(1, 15, dtype=int)),\n",
    "        \"weights\": hp.choice(\"weights\", [\"uniform\"]),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T23:39:13.469256Z",
     "start_time": "2019-10-29T23:39:13.450260Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "train.execBayesOptimSearch(\n",
    "    allSpace=allSpace,\n",
    "    data=train.data,\n",
    "    target=train.target,\n",
    "    columns=crossValFeaturesDict,\n",
    "    scoring=\"accuracy\",\n",
    "    nFolds=2,\n",
    "    nJobs=8,\n",
    "    iters=1,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Model loss by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T03:01:08.041938Z",
     "start_time": "2019-10-29T03:01:07.943932Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "bayesOptimSummary = pd.read_csv(\"bayesOptimizationSummary_accuracy_20191028_174038.csv\", na_values=\"nan\")\n",
    "bayesOptimSummary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T03:01:28.828751Z",
     "start_time": "2019-10-29T03:01:21.909180Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(bayesOptimSummary[\"estimator\"]):\n",
    "    train.modelLossPlot(bayesOptimSummary=bayesOptimSummary, estimator=estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Parameter selection by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:06:07.310087Z",
     "start_time": "2019-10-28T02:01:20.702423Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(bayesOptimSummary[\"estimator\"]):\n",
    "    train.modelParamPlot(\n",
    "        bayesOptimSummary=bayesOptimSummary,\n",
    "        estimator=estimator,\n",
    "        allSpace=allSpace,\n",
    "        nIter=100,\n",
    "#         chartProp=15,\n",
    "        titleScale=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sampleSpace = {\n",
    "                'param': hp.uniform('param', np.log(0.4), np.log(0.6))\n",
    "#     \"\": 0.000001 + hp.uniform(\"gamma\", 0.000001, 10)\n",
    "    #             'param2': hp.loguniform('param2', np.log(0.001), np.log(0.01))\n",
    "}\n",
    "\n",
    "train.samplePlot(sampleSpace, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:10:14.649664Z",
     "start_time": "2019-10-28T02:10:09.392223Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pair-wise comparison\n",
    "p = PrettierPlot(chartProp=12)\n",
    "p.prettyPairPlotCustom(\n",
    "    df=train.unpackBayesOptimSummary(bayesOptimSummary, \"lightgbm.LGBMClassifier\"),\n",
    "    cols=[\"colsample_bytree\", \"learning_rate\", \"iteration\",\"iterLoss\"],\n",
    "    gradientCol=\"iteration\",\n",
    "    color=style.styleGrey\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model performance evaluation - standard models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Model-performance-evaluation-standard-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T03:01:59.426320Z",
     "start_time": "2019-10-29T03:01:59.348313Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topModels = train.topBayesOptimModels(bayesOptimSummary=bayesOptimSummary, numModels=1)\n",
    "topModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:18:07.237546Z",
     "start_time": "2019-10-28T02:18:01.653371Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classification panel, single model\n",
    "# estimator = \"svm.SVC\"; modelIter = 1224\n",
    "# estimator = 'ensemble.GradientBoostingClassifier'; modelIter = 590\n",
    "estimator = 'xgboost.XGBClassifier'; modelIter = 1256\n",
    "\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayesOptimSummary=bayesOptimSummary, estimator=estimator, modelIter=modelIter\n",
    ")\n",
    "\n",
    "train.classificationPanel(\n",
    "    model=model,\n",
    "    XTrain=train.data,\n",
    "    yTrain=train.target,\n",
    "    cmLabels=[\"Dies\", \"Survives\"],\n",
    "    nFolds=5,\n",
    "    titleScale=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:20:39.749545Z",
     "start_time": "2019-10-28T02:19:28.468980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create classification reports for training data\n",
    "for estimator, modelIters in topModels.items():\n",
    "    for modelIter in modelIters:\n",
    "        model = train.BayesOptimModelBuilder(\n",
    "            bayesOptimSummary=bayesOptimSummary,\n",
    "            estimator=estimator,\n",
    "            modelIter=modelIter,\n",
    "        )\n",
    "        train.classificationPanel(\n",
    "            model=model, XTrain=train.data, yTrain=train.target, cmLabels=['Dies', 'Survives'], nFolds=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model explanability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:27:11.930206Z",
     "start_time": "2019-10-28T02:27:10.646175Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# estimator = \"ensemble.ExtraTreesClassifier\"; modelIter = 145\n",
    "# estimator = \"svm.SVC\"; modelIter = 135\n",
    "estimator = \"ensemble.GradientBoostingClassifier\"; modelIter = 1385\n",
    "\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayesOptimSummary=bayesOptimSummary, estimator=estimator, modelIter=modelIter\n",
    ")\n",
    "model.fit(train.data.values, train.target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# permutation importance - how much does performance decrease when shuffling a certain feature?\n",
    "perm = PermutationImportance(model.model, random_state=1).fit(train.data, train.target)\n",
    "eli5.show_weights(perm, feature_names=train.data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### SHAP values - training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Force plots - single observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:30:16.624648Z",
     "start_time": "2019-10-28T02:30:14.057459Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in train.data.index[:5]:\n",
    "    train.singleShapVizTree(obsIx=i, model=model, data=train.data, target=train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Force plots - multiple observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:30:35.980162Z",
     "start_time": "2019-10-28T02:30:32.560906Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = train.multiShapVizTree(obsIxs=train.data.index, model=model, data=train.data)\n",
    "visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:30:58.243782Z",
     "start_time": "2019-10-28T02:30:55.016538Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obsData, _, obsShapValues = train.multiShapValueTree(\n",
    "    obsIxs=train.data.index, model=model, data=train.data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:38:19.278946Z",
     "start_time": "2019-10-28T02:36:51.988249Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "# gridFeatures = [\"Pclass\", \"Age\", \"Fare\", \"SibSp\",\"Parch\"]\n",
    "gridFeatures = ['Age*Fare','Title_2','Fare*FamilySize','Sex_male','Fare','Pclass','CabinQuarter_X']\n",
    "\n",
    "train.shapDependenceGrid(\n",
    "    obsData=obsData,\n",
    "    obsShapValues=obsShapValues,\n",
    "    gridFeatures=gridFeatures,\n",
    "    allFeatures=train.data.columns,\n",
    "    dotSize=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:41:14.815016Z",
     "start_time": "2019-10-28T02:41:14.583998Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "\n",
    "train.shapDependencePlot(\n",
    "    obsData=obsData,\n",
    "    obsShapValues=obsShapValues,\n",
    "    scatterFeature=\"Fare\",\n",
    "    colorFeature=\"Sex_male\",\n",
    "    featureNames=train.data.columns,\n",
    "    dotSize=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:42:25.004355Z",
     "start_time": "2019-10-28T02:42:22.854184Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "featureNames = train.data.columns.tolist()\n",
    "topShap = np.argsort(-np.sum(np.abs(obsShapValues), 0))\n",
    "\n",
    "for topIx in topShap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.makeCanvas()\n",
    "    \n",
    "    train.shapDependencePlot(\n",
    "        obsData=obsData,\n",
    "        obsShapValues=obsShapValues,\n",
    "        scatterFeature=featureNames[topIx],\n",
    "        colorFeature=\"Fare\",\n",
    "        featureNames=featureNames,\n",
    "        dotSize=50,\n",
    "        alpha=0.5,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T02:42:41.951132Z",
     "start_time": "2019-10-28T02:42:41.645104Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "train.shapSummaryPlot(\n",
    "        obsData=obsData,\n",
    "        obsShapValues=obsShapValues,\n",
    "        featureNames=train.data.columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### SHAP values - validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Force plots - single observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in valid.data.index[:2]:\n",
    "    valid.singleShapVizTree(obsIx=i, model=model, data=valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Force plots - multiple observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = valid.multiShapVizTree(obsIxs=valid.data.index, model=model, data=valid.data)\n",
    "visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obsData, _, obsShapValues = valid.multiShapValueTree(\n",
    "    obsIxs=valid.data.index, model=model, data=valid.data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "gridFeatures = [\"Pclass\", \"Age\", \"Fare\", \"SibSp\",\"Parch\"]\n",
    "\n",
    "valid.shapDependenceGrid(\n",
    "    obsData=obsData,\n",
    "    obsShapValues=obsShapValues,\n",
    "    gridFeatures=gridFeatures,\n",
    "    allFeatures=valid.data.columns,\n",
    "    dotSize=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "\n",
    "valid.shapDependencePlot(\n",
    "    obsData=obsData,\n",
    "    obsShapValues=obsShapValues,\n",
    "    scatterFeature=\"Age\",\n",
    "    colorFeature=\"Parch\",\n",
    "    featureNames=valid.data.columns,\n",
    "    dotSize=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "featureNames = valid.data.columns.tolist()\n",
    "topShap = np.argsort(-np.sum(np.abs(obsShapValues), 0))\n",
    "\n",
    "for topIx in topShap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.makeCanvas()\n",
    "    \n",
    "    valid.shapDependencePlot(\n",
    "        obsData=obsData,\n",
    "        obsShapValues=obsShapValues,\n",
    "        scatterFeature=featureNames[topIx],\n",
    "        colorFeature=\"Age\",\n",
    "        featureNames=featureNames,\n",
    "        dotSize=50,\n",
    "        alpha=0.5,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "valid.shapSummaryPlot(\n",
    "        obsData=obsData,\n",
    "        obsShapValues=obsShapValues,\n",
    "        featureNames=valid.data.columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Submission - standard models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Submission-standard-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "{'lightgbm.LGBMClassifier': [778],\n",
    " 'linear_model.LogisticRegression': [730],\n",
    " 'xgboost.XGBClassifier': [371],\n",
    " 'ensemble.RandomForestClassifier': [712],\n",
    " 'ensemble.GradientBoostingClassifier': [965],\n",
    " 'ensemble.AdaBoostClassifier': [512],\n",
    " 'ensemble.ExtraTreesClassifier': [244],\n",
    " 'svm.SVC': [551],\n",
    " 'neighbors.KNeighborsClassifier': [576]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T03:05:31.560264Z",
     "start_time": "2019-10-29T03:05:31.373250Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## standard model fit and predict\n",
    "# select estimator and iteration\n",
    "# estimator = \"lightgbm.LGBMClassifier\"; modelIter = 778  #147 survived, 0.775\n",
    "# estimator = \"xgboost.XGBClassifier\"; modelIter = 371  #142 survived, 0.765\n",
    "# estimator = \"ensemble.RandomForestClassifier\"; modelIter = 712  #144 survived, 0.7655\n",
    "# estimator = \"ensemble.GradientBoostingClassifier\"; modelIter = 965  #144 survived, 0.7561\n",
    "estimator = \"svm.SVC\"; modelIter = 551  # survived, \n",
    "\n",
    "# extract params and instantiate model\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayesOptimSummary=bayesOptimSummary, estimator=estimator, modelIter=modelIter\n",
    ")\n",
    "model.fit(train.data.values, train.target.values)\n",
    "\n",
    "# fit model and make predictions\n",
    "yPred = model.predict(valid.data.values)\n",
    "print(sum(yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T03:05:33.123532Z",
     "start_time": "2019-10-29T03:05:33.112531Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "submit = pd.DataFrame({\"PassengerId\": dfValid.PassengerId, \"Survived\": yPred})\n",
    "submit.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Stacking'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Primary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Primary-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "{'lightgbm.LGBMClassifier': [778],\n",
    " 'linear_model.LogisticRegression': [730],\n",
    " 'xgboost.XGBClassifier': [371],\n",
    " 'ensemble.RandomForestClassifier': [712],\n",
    " 'ensemble.GradientBoostingClassifier': [965],\n",
    " 'ensemble.AdaBoostClassifier': [512],\n",
    " 'ensemble.ExtraTreesClassifier': [244],\n",
    " 'svm.SVC': [551],\n",
    " 'neighbors.KNeighborsClassifier': [576]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T03:19:09.776612Z",
     "start_time": "2019-10-29T03:19:09.746614Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lgb = train.BayesOptimModelBuilder(bayesOptimSummary=bayesOptimSummary, estimator=\"lightgbm.LGBMClassifier\", modelIter=778)\n",
    "lr = train.BayesOptimModelBuilder(bayesOptimSummary=bayesOptimSummary, estimator=\"linear_model.LogisticRegression\", modelIter=730)\n",
    "xgb = train.BayesOptimModelBuilder(bayesOptimSummary=bayesOptimSummary, estimator=\"xgboost.XGBClassifier\", modelIter=371)\n",
    "rf = train.BayesOptimModelBuilder(bayesOptimSummary=bayesOptimSummary, estimator=\"ensemble.RandomForestClassifier\", modelIter=712)\n",
    "gb = train.BayesOptimModelBuilder(bayesOptimSummary=bayesOptimSummary, estimator=\"ensemble.GradientBoostingClassifier\", modelIter=965)\n",
    "ada = train.BayesOptimModelBuilder(bayesOptimSummary=bayesOptimSummary, estimator=\"ensemble.AdaBoostClassifier\", modelIter=512)\n",
    "ext = train.BayesOptimModelBuilder(bayesOptimSummary=bayesOptimSummary, estimator=\"ensemble.ExtraTreesClassifier\", modelIter=244)\n",
    "svc = train.BayesOptimModelBuilder(bayesOptimSummary=bayesOptimSummary, estimator=\"svm.SVC\", modelIter=551)\n",
    "kn = train.BayesOptimModelBuilder(bayesOptimSummary=bayesOptimSummary, estimator=\"neighbors.KNeighborsClassifier\", modelIter=576)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T03:21:24.472308Z",
     "start_time": "2019-10-29T03:20:42.741085Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vecstack import StackingTransformer\n",
    "import sklearn.metrics as metrics\n",
    "# Get your data\n",
    "\n",
    "# Initialize 1st level estimators\n",
    "estimators = [('lgb', lgb.model),\n",
    "              ('lr',lr.model),\n",
    "              ('xgb',xgb.model),\n",
    "              ('rf',rf.model),\n",
    "              ('gb',gb.model),\n",
    "              ('ada',ada.model),\n",
    "              ('ext',ext.model),\n",
    "              ('svc',svc.model),\n",
    "              ('kn',kn.model),\n",
    "             ]\n",
    "              \n",
    "# Initialize StackingTransformer\n",
    "stack = StackingTransformer(\n",
    "    estimators,\n",
    "    regression=False,\n",
    "    metric=metrics.accuracy_score,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit\n",
    "stack = stack.fit(train.data, train.target)\n",
    "\n",
    "# Get your stacked features\n",
    "oofTrain = stack.transform(train.data)\n",
    "oofValid = stack.transform(valid.data)\n",
    "\n",
    "# Use 2nd level estimator with stacked features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T03:12:15.305201Z",
     "start_time": "2019-10-28T03:05:52.429813Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get out-of-fold predictions\n",
    "oofTrain, oofValid, columns = train.modelStacker(\n",
    "    models=topModels,\n",
    "    bayesOptimSummary=bayesOptimSummary,\n",
    "    XTrain=train.data.values,\n",
    "    yTrain=train.target.values,\n",
    "    XValid=valid.data.values,\n",
    "    nFolds=10,\n",
    "    nJobs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T03:28:58.433646Z",
     "start_time": "2019-10-28T03:28:51.398069Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view correlations of predictions\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmap(\n",
    "    df=pd.DataFrame(oofTrain, columns=columns), annot=True, ax=ax, vmin=0\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Meta-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T03:22:06.292547Z",
     "start_time": "2019-10-29T03:22:06.264544Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parameter space\n",
    "allSpace = {\n",
    "    \"lightgbm.LGBMClassifier\": {\n",
    "        \"class_weight\": hp.choice(\"class_weight\", [None]),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"dart\"]),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.15, 0.25),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(4, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.quniform(\"min_child_samples\", 50, 150, 5),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.quniform(\"num_leaves\", 30, 70, 1),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.75, 1.25),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.quniform(\"subsample_for_bin\", 100000, 350000, 20000),\n",
    "    },\n",
    "    \"xgboost.XGBClassifier\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"gamma\": hp.quniform(\"gamma\", 0.0, 10, 0.05),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.2, 0.01),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_child_weight\": hp.quniform(\"min_child_weight\", 2.5, 7.5, 1),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.4, 0.7),\n",
    "    },\n",
    "    \"ensemble.RandomForestClassifier\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 10, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 8000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(15, 25, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 11, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.09, 0.01),\n",
    "        \"loss\": hp.choice(\"loss\", [\"deviance\", \"exponential\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"svm.SVC\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.00000001, 15),\n",
    "        \"decision_function_shape\": hp.choice(\"decision_function_shape\", [\"ovr\", \"ovo\"]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.00000001, 1.5),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T03:22:27.745685Z",
     "start_time": "2019-10-29T03:22:27.730686Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "train.execBayesOptimSearch(\n",
    "    allSpace=allSpace,\n",
    "    data=oofTrain,\n",
    "    target=train.target,\n",
    "    scoring=\"accuracy\",\n",
    "    nFolds=8,\n",
    "    nJobs=8,\n",
    "    iters=1000,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T23:19:42.211680Z",
     "start_time": "2019-10-29T23:19:42.208679Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T23:19:46.259605Z",
     "start_time": "2019-10-29T23:19:46.255606Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "bayesOptimSummaryMeta = pd.read_csv(\"{}_hyperopt_meta_{}.csv\".format(rundate, analysis))\n",
    "bayesOptimSummaryMeta[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(bayesOptimSummaryMeta[\"estimator\"]):\n",
    "    train.modelLossPlot(bayesOptimSummary=bayesOptimSummaryMeta, estimator=estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(bayesOptimSummaryMeta[\"estimator\"]):\n",
    "    train.modelParamPlot(\n",
    "        bayesOptimSummary=bayesOptimSummaryMeta,\n",
    "        estimator=estimator,\n",
    "        allSpace=allSpace,\n",
    "        nIter=100,\n",
    "        chartProp=15,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model performance evaluation - stacked models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Model-performance-evaluation-stacked-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topModels = train.topBayesOptimModels(\n",
    "    bayesOptimSummary=bayesOptimSummaryMeta, numModels=1\n",
    ")\n",
    "topModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# classification panel, single model\n",
    "estimator = \"svm.SVC\"; modelIter = 135\n",
    "# estimator = 'ensemble.GradientBoostingClassifier'; modelIter = 590\n",
    "# estimator = 'xgboost.XGBClassifier'; modelIter = 380\n",
    "\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayesOptimSummary=bayesOptimSummaryMeta, estimator=estimator, modelIter=modelIter\n",
    ")\n",
    "\n",
    "train.classificationPanel(\n",
    "    model=model, XTrain=oofTrain, yTrain=train.target, labels=[0, 1], nFolds=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create classification reports for training data\n",
    "for estimator, modelIters in topModels.items():\n",
    "    for modelIter in modelIters:\n",
    "        model = train.BayesOptimModelBuilder(\n",
    "            bayesOptimSummary=bayesOptimSummaryMeta,\n",
    "            estimator=estimator,\n",
    "            modelIter=modelIter,\n",
    "        )\n",
    "        train.classificationPanel(\n",
    "            model=model, XTrain=oofTrain, yTrain=train.target, labels=[0, 1], nFolds=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Submission - stacked models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Submission-stacked-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "# estimator = \"lightgbm.LGBMClassifier\"; modelIter = 876 #0.75119\n",
    "# estimator = \"xgboost.XGBClassifier\"; modelIter = 821, #0.779\n",
    "# estimator = \"ensemble.RandomForestClassifier\"; modelIter = 82 \n",
    "# estimator = \"ensemble.GradientBoostingClassifier\"; modelIter = 673 #0.77511\n",
    "estimator = \"svm.SVC\"; modelIter = 538 # 0.77511\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayesOptimSummary=bayesOptimSummaryMeta, estimator=estimator, modelIter=modelIter\n",
    ")\n",
    "\n",
    "model.fit(oofTrain, train.target.values)\n",
    "yPred = model.predict(oofValid)\n",
    "print(sum(yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "submit = pd.DataFrame({\"PassengerId\": dfValid.PassengerId, \"Survived\": yPred})\n",
    "submit.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
