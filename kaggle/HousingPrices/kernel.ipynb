{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kaggle competition - house prices__\n",
    "\n",
    "1. [Import](#Import)\n",
    "    1. [Tools](#Tools)\n",
    "    1. [Data](#Data)    \n",
    "1. [EDA](#EDA)\n",
    "    1. [Categorical feature EDA](#Categorical-feature-EDA)\n",
    "    1. [Numeric feature EDA](#numeric-feature-EDA)\n",
    "    1. [Faceting](#Faceting)\n",
    "    1. [Target variable evaluation](#Target-variable-evaluation)    \n",
    "1. [Data preparation](#Data-preparation)\n",
    "    1. [Missing data](#Missing-data)\n",
    "    1. [Engineering](#Engineering)\n",
    "    1. [Encoding](#Encoding)\n",
    "    1. [Transformation](#Transformation)\n",
    "        1. [Polynomial features](#Polynomial-features)\n",
    "        1. [Skew](#Skew)\n",
    "        1. [Scale](#Scale)\n",
    "    1. [Outliers](#Outliers)\n",
    "1. [Feature importance](#Feature-importance)    \n",
    "1. [Modeling](#Modeling)\n",
    "    1. [Data preparation](#Data-preparation-1)\n",
    "    1. [Bayesian hyper-parameter optimization](#Bayesian-hyper-parameter-optimization)\n",
    "    1. [Model performance evaluation - standard models](#Model-performance-evaluation-standard-models)\n",
    "    1. [Model explanability](#Model-explanability)\n",
    "    1. [Submission - standard models](#Submission-standard-models)\n",
    "1. [Stacking](#Stacking)\n",
    "    1. [Primary models](#Primary-models)\n",
    "    1. [Meta model](#Meta-model)                \n",
    "    1. [Model performance evaluation - stacked models](#Model-performance-evaluation-stacked-models)\n",
    "    1. [Submission - stacked models](#Submission-stacked-models)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T02:31:52.473366Z",
     "start_time": "2019-10-21T02:31:48.691765Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from functools import reduce\n",
    "import time; rundate = time.strftime(\"%Y%m%d\")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 500); pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.impute as impute\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "\n",
    "from hyperopt import hp\n",
    "\n",
    "import lightgbm\n",
    "import xgboost\n",
    "\n",
    "import eif\n",
    "import shap\n",
    "shap.initjs()\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    #     import mlmachine as mlm\n",
    "    #     from prettierplot.plotter import PrettierPlot\n",
    "    #     import prettierplot.style as style\n",
    "    import asdfasd\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(\n",
    "        \"../../../mlmachine\"\n",
    "    ) if \"../../../../mlmachine\" not in sys.path else None\n",
    "    sys.path.append(\n",
    "        \"../../../prettierplot\"\n",
    "    ) if \"../../../../prettierplot\" not in sys.path else None\n",
    "\n",
    "    import mlmachine as mlm\n",
    "    import mlmachine.data as data\n",
    "    from mlmachine.features.preprocessing import (\n",
    "        DataFrameSelector,\n",
    "        PlayWithPandas,\n",
    "        UnprocessedColumnAdder,\n",
    "        ContextImputer,\n",
    "        PandasFeatureUnion,\n",
    "        DualTransformer,\n",
    "    )\n",
    "    from prettierplot.plotter import PrettierPlot\n",
    "    import prettierplot.style as style\n",
    "else:\n",
    "    print(\n",
    "        \"This notebook relies on the libraries mlmachine and prettierplot. Please run:\"\n",
    "    )\n",
    "    print(\"\\tpip install mlmachine\")\n",
    "    print(\"\\tpip install prettierplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T03:00:40.684019Z",
     "start_time": "2019-10-21T03:00:39.551497Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data and print dimensions\n",
    "df_train, df_valid = data.housing()\n",
    "# df_train = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-housing/train.csv\")\n",
    "# df_valid = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-housing/test.csv\")\n",
    "\n",
    "print(\"Training data dimensions: {}\".format(df_train.shape))\n",
    "print(\"Validation data dimensions: {}\".format(df_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T03:00:40.745361Z",
     "start_time": "2019-10-21T03:00:40.686677Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display info and first 5 rows\n",
    "df_train.info()\n",
    "display(df_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T03:00:40.773049Z",
     "start_time": "2019-10-21T03:00:40.748399Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# review counts of different column types\n",
    "df_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T03:00:40.852638Z",
     "start_time": "2019-10-21T03:00:40.777248Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load training data into mlmachine\n",
    "train = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"SalePrice\",\n",
    "    remove_features=[\"Id\", \"MiscVal\"],\n",
    "    force_to_categorical=[\n",
    "        \"MSSubClass\",\n",
    "        \"OverallQual\",\n",
    "        \"OverallCond\",\n",
    "        \"YearBuilt\",\n",
    "        \"YearRemodAdd\",\n",
    "        \"MoSold\",\n",
    "        \"YrSold\",\n",
    "    ],\n",
    "    target_type=\"numeric\",\n",
    ")\n",
    "print(train.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T03:00:40.917063Z",
     "start_time": "2019-10-21T03:00:40.854981Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load training data into mlmachine\n",
    "valid = mlm.Machine(\n",
    "    data=df_valid,\n",
    "    remove_features=[\"Id\", \"MiscVal\"],\n",
    "    force_to_categorical=[\n",
    "        \"MSSubClass\",\n",
    "        \"OverallQual\",\n",
    "        \"OverallCond\",\n",
    "        \"YearBuilt\",\n",
    "        \"YearRemodAdd\",\n",
    "        \"MoSold\",\n",
    "        \"YrSold\",\n",
    "    ],\n",
    ")\n",
    "print(valid.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Categorical feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Categorical-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:34:18.840997Z",
     "start_time": "2019-09-08T16:33:50.259331Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# categorical features\n",
    "for feature in train.feature_type[\"categorical\"]:\n",
    "    train.eda_num_target_cat_feat(feature=feature, level_count_cap=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Numeric feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'numeric-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:34:44.963918Z",
     "start_time": "2019-09-08T16:34:18.842647Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numeric features\n",
    "for feature in train.feature_type[\"numeric\"]:\n",
    "    train.eda_num_target_num_feat(feature=feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Correlation (all samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:34:46.271541Z",
     "start_time": "2019-09-08T16:34:44.967394Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map\n",
    "p = PrettierPlot(chart_prop=25)\n",
    "ax = p.make_canvas()\n",
    "p.pretty_corr_heatmap(df=train.data, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:34:46.627316Z",
     "start_time": "2019-09-08T16:34:46.273556Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot(plot_orientation='tall',chart_prop=15)\n",
    "ax = p.make_canvas()\n",
    "p.pretty_corr_heatmap_target(df=train.data, target=train.target, thresh=0.6, annot = True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Remarks - There are three pairs of highly correlated features:\n",
    "    - 'GarageArea' and 'GarageCars'\n",
    "    - 'TotRmsAbvGrd' and 'GrLivArea'\n",
    "    - '1stFlrSF' and 'TotalBsmtSF\n",
    "This makes sense, given what each feature represents and how each pair items relate to each other. We likely only need one feature from each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:40.370294Z",
     "start_time": "2019-09-08T16:34:46.631150Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chart_prop=10)\n",
    "p.pretty_pair_plot(\n",
    "    df=train.data,\n",
    "    cols=[\n",
    "        \"LotFrontage\",\n",
    "        \"LotArea\",\n",
    "        \"MasVnrArea\",\n",
    "        \"BsmtFinSF1\",\n",
    "        \"BsmtFinSF2\",\n",
    "        \"BsmtUnfSF\",\n",
    "        \"TotalBsmtSF\",\n",
    "        \"1stFlrSF\",\n",
    "        \"2ndFlrSF\",\n",
    "        \"GrLivArea\",\n",
    "        \"TotRmsAbvGrd\",\n",
    "        \"GarageYrBlt\",\n",
    "        \"GarageArea\",\n",
    "        \"WoodDeckSF\",\n",
    "        \"OpenPorchSF\",\n",
    "    ],\n",
    "    diag_kind=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Faceting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Faceting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Categorical by categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Categorical by numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Target-variable-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:41.007781Z",
     "start_time": "2019-09-08T16:35:40.372421Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# evaluate distribution of target variable\n",
    "train.edaTransformInitial(data=train.target, name=train.target.name)\n",
    "train.edaTransformLog1(data=train.target, name=train.target.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:41.013246Z",
     "start_time": "2019-09-08T16:35:41.009656Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# log + 1 transform target\n",
    "train.target = np.log1p(train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Data-preparation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Missing data\n",
    "\n",
    "-__MCAR__ - Completely unsystematic missingness, completely unralted to any of the other variables. simple imputation of mean, median or mode is most acceptable for this type of missingness.\n",
    "\n",
    "-__MAR__ - The nature of the missing data is related to observed data in other variables, not the missing data. The missing data is conditional on some other variable.  For example, men are more likely to tell you their weight than woemn. The missingness of weight has to do with gender.\n",
    "\n",
    "-__MNAR__ - There is a relationship between the propensity of a value to be missing and its values. For example, the wealthiest people choosing not to state their income.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Missing-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T03:00:47.217679Z",
     "start_time": "2019-10-21T03:00:46.672344Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "train.eda_missing_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:47.652557Z",
     "start_time": "2019-09-08T16:35:46.963470Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno matrix\n",
    "msno.matrix(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:48.338373Z",
     "start_time": "2019-09-08T16:35:47.654364Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno bar\n",
    "msno.bar(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:49.474364Z",
     "start_time": "2019-09-08T16:35:48.339901Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno heatmap\n",
    "msno.heatmap(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:51.551057Z",
     "start_time": "2019-09-08T16:35:49.479527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno dendrogram\n",
    "msno.dendrogram(train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T03:12:41.469538Z",
     "start_time": "2019-10-21T03:12:40.983920Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "valid.eda_missing_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:52.736712Z",
     "start_time": "2019-09-08T16:35:52.049128Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno matrix\n",
    "msno.matrix(valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:53.429001Z",
     "start_time": "2019-09-08T16:35:52.738222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno bar\n",
    "msno.bar(valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:55.831528Z",
     "start_time": "2019-09-08T16:35:53.430598Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno heatmap\n",
    "msno.heatmap(valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:57.927975Z",
     "start_time": "2019-09-08T16:35:55.833121Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingno dendrogram\n",
    "msno.dendrogram(valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training vs. validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:57.942682Z",
     "start_time": "2019-09-08T16:35:57.929770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "train.missing_col_compare(train=train.data, validation=valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Impute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# impute pipeline\n",
    "categoricalConstant = ['GarageFinish', 'Alley', 'MasVnrType', 'GarageType', 'BsmtFinType1',\n",
    "                       'BsmtCond', 'BsmtFinType2', 'BsmtQual', 'PoolQC', 'GarageCond',\n",
    "                       'FireplaceQu', 'GarageQual', 'Fence', 'BsmtExposure', 'MiscFeature']\n",
    "numericConstant = [\"GarageYrBlt\",\"MasVnrArea\",\"BsmtUnfSF\",\"GarageArea\",\"BsmtFinSF1\",\"TotalBsmtSF\",\"BsmtFinSF2\"]\n",
    "categoricalMode = [\"Electrical\",\"Functional\",\"SaleType\",\"Exterior1st\",\"MSZoning\",\"Exterior2nd\",\"KitchenQual\",\"Utilities\"]\n",
    "numericMode = [\"BsmtHalfBath\", \"GarageCars\", \"BsmtFullBath\"]\n",
    "\n",
    "impute_pipe = PandasFeatureUnion([\n",
    "    (\"catConstant\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(categoricalConstant),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"constant\", fill_value=\"Nonexistent\"))\n",
    "    )),\n",
    "    (\"numConstant\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(numericConstant),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "    )),\n",
    "    (\"catMode\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(categoricalMode),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"numMode\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(numericMode),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"LotFrontage\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"LotFrontage\",\"Neighborhood\"]),\n",
    "        ContextImputer(null_col=\"LotFrontage\", context_col=\"Neighborhood\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference([\"LotFrontage\"] + categoricalConstant + numericConstant + categoricalMode + numericMode))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = impute_pipe.fit_transform(train.data)\n",
    "valid.data = impute_pipe.transform(valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train.eda_missing_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid.eda_missing_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:58.106136Z",
     "start_time": "2019-09-08T16:35:58.099992Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# additional features\n",
    "train.data[\"BsmtFinSF\"] = train.data[\"BsmtFinSF1\"] + train.data[\"BsmtFinSF2\"]\n",
    "train.data[\"TotalSF\"] = (\n",
    "    train.data[\"TotalBsmtSF\"] + train.data[\"1stFlrSF\"] + train.data[\"2ndFlrSF\"]\n",
    ")\n",
    "train.feature_type_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:59.771172Z",
     "start_time": "2019-09-08T16:35:58.118725Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate additional features\n",
    "for feature in [\"BsmtFinSF\",\"TotalSF\"]:\n",
    "    train.eda_num_target_num_feat(feature=feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:59.778702Z",
     "start_time": "2019-09-08T16:35:59.773104Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# additional features\n",
    "valid.data[\"BsmtFinSF\"] = valid.data[\"BsmtFinSF1\"] + valid.data[\"BsmtFinSF2\"]\n",
    "valid.data[\"TotalSF\"] = (\n",
    "    valid.data[\"TotalBsmtSF\"] + valid.data[\"1stFlrSF\"] + valid.data[\"2ndFlrSF\"]\n",
    ")\n",
    "valid.feature_type_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Encoding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:59.804119Z",
     "start_time": "2019-09-08T16:35:59.780476Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in training data categorical columns\n",
    "train.data[train.feature_type[\"categorical\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:59.860009Z",
     "start_time": "2019-09-08T16:35:59.805551Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in train.data[train.feature_type[\"categorical\"]]:\n",
    "    print(col, np.unique(train.data[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:59.884900Z",
     "start_time": "2019-09-08T16:35:59.861476Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in validation data string columns\n",
    "valid.data[valid.feature_type[\"categorical\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:59.934696Z",
     "start_time": "2019-09-08T16:35:59.886571Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in valid.data[valid.feature_type[\"categorical\"]]:\n",
    "    print(col, np.unique(valid.data[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training vs. validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:59.972008Z",
     "start_time": "2019-09-08T16:35:59.936291Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify values that are present in the training data but not the validation data, and vice versa\n",
    "for col in train.feature_type[\"categorical\"]:\n",
    "    train_values = train.data[col].unique()\n",
    "    valid_values = valid.data[col].unique()\n",
    "\n",
    "    trainDiff = set(train_values) - set(valid_values)\n",
    "    valid_diff = set(valid_values) - set(train_values)\n",
    "\n",
    "    if len(trainDiff) > 0 or len(valid_diff) > 0:\n",
    "        print(\"\\n\\n*** \" + col)\n",
    "        print(\"Value present in training data, not in validation data\")\n",
    "        print(trainDiff)\n",
    "        print(\"Value present in validation data, not in training data\")\n",
    "        print(valid_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nominal columns\n",
    "nominal_columns = [\"MSSubClass\",\"MSZoning\",\"LandContour\",\"Neighborhood\",\"Condition1\",\"Condition2\",\"BldgType\",\n",
    "    \"HouseStyle\",\"RoofStyle\",\"RoofMatl\",\"Exterior1st\",\"Exterior2nd\",\"MasVnrType\",\"Foundation\",\"Heating\",\n",
    "    \"GarageType\",\"Fence\",\"SaleType\",\"SaleCondition\",\"MiscFeature\",]\n",
    "ordinal_columns = [\"Street\",\"Alley\",\"LotShape\",\"Utilities\",\"LotConfig\",\"LandSlope\",\"ExterQual\",\"ExterCond\",\"BsmtQual\",\n",
    "    \"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"HeatingQC\",\"CentralAir\",\"Electrical\",\"KitchenQual\",\n",
    "    \"Functional\",\"FireplaceQu\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\"PavedDrive\",\"PoolQC\",]\n",
    "\n",
    "ordinal_encodings = [\n",
    "    [\"Grvl\", \"Pave\"],  #  Street\n",
    "    [\"Nonexistent\", \"Grvl\", \"Pave\"],  # Alley\n",
    "    [\"IR3\", \"IR2\", \"IR1\", \"Reg\"],  # LotShape\n",
    "    [\"ELO\", \"NoSeWa\", \"NoSewr\", \"AllPub\"],  # Utilities\n",
    "    [\"FR3\", \"FR2\", \"Corner\", \"Inside\", \"CulDSac\"],  # LotConfig\n",
    "    [\"Sev\", \"Mod\", \"Gtl\"],  # LandSlope\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # ExterQual\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # ExterCond\n",
    "    [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # BsmtQual\n",
    "    [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # BsmtCond\n",
    "    [\"Nonexistent\", \"No\", \"Mn\", \"Av\", \"Gd\"],  # BsmtExposure\n",
    "    [\"Nonexistent\", \"Unf\", \"LwQ\", \"BLQ\", \"Rec\", \"ALQ\", \"GLQ\"],  # BsmtFinType1\n",
    "    [\"Nonexistent\", \"Unf\", \"LwQ\", \"BLQ\", \"Rec\", \"ALQ\", \"GLQ\"],  # BsmtFinType2\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # HeatingQC\n",
    "    [\"N\", \"Y\"],  # CentralAir\n",
    "    [\"FuseP\", \"FuseF\", \"FuseA\", \"Mix\", \"SBrkr\"],  # Electrical\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # KitchenQual\n",
    "    [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],  # Functional\n",
    "    [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # FireplaceQu\n",
    "    [\"Nonexistent\", \"Unf\", \"RFn\", \"Fin\"],  # GarageFinish\n",
    "    [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # GarageQual\n",
    "    [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # GarageCond\n",
    "    [\"N\", \"P\", \"Y\"],  # PavedDrive\n",
    "    [\"Nonexistent\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # PoolQC\n",
    "]\n",
    "\n",
    "# encode pipeline\n",
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"ordinal\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(ordinal_columns),\n",
    "        PlayWithPandas(preprocessing.OrdinalEncoder(categories=ordinal_encodings)),\n",
    "    )),\n",
    "    (\"nominal\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(nominal_columns),\n",
    "        PlayWithPandas(preprocessing.OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    )),\n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference(nominal_columns + ordinal_columns))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = encode_pipe.fit_transform(train.data)\n",
    "valid.data = encode_pipe.transform(valid.data)\n",
    "\n",
    "train.feature_type_update()\n",
    "valid.feature_type_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Transformation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Polynomial features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Polynomial-features'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Value override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T04:47:08.980776Z",
     "start_time": "2019-07-20T04:47:08.946882Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# change clearly erroneous value to what it probably was\n",
    "valid.data[\"GarageYrBlt\"].replace({2207: 2007}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# polynomial pipe\n",
    "polynomial_pipe = PandasFeatureUnion([\n",
    "    (\"polynomial\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(train.feature_type[\"numeric\"]),\n",
    "        PlayWithPandas(preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=False))\n",
    "    )),\n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference(train.feature_type[\"numeric\"]))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = polynomial_pipe.fit_transform(train.data)\n",
    "valid.data = polynomial_pipe.transform(valid.data)\n",
    "\n",
    "train.feature_type_update()\n",
    "valid.feature_type_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Skew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Skew'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:36:00.341194Z",
     "start_time": "2019-09-08T16:36:00.167826Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of numeric features - validation data\n",
    "train.skew_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:36:00.497014Z",
     "start_time": "2019-09-08T16:36:00.342968Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of numeric features - training data\n",
    "valid.skew_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T04:46:52.200949Z",
     "start_time": "2019-07-20T04:46:51.815040Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # skew pipe\n",
    "# skew_pipe = PandasFeatureUnion([\n",
    "#     (\"skew\", pipeline.make_pipeline(\n",
    "#         DataFrameSelector(train.feature_type[\"numeric\"]),\n",
    "#         DualTransformer(),\n",
    "#     )),\n",
    "#     (\"diff\", pipeline.make_pipeline(\n",
    "#         DataFrameSelector(list(set(train.data.columns).difference(train.feature_type[\"numeric\"]))),\n",
    "#     )),\n",
    "# ])\n",
    "\n",
    "# train.data = skew_pipe.fit_transform(train.data)\n",
    "# valid.data = skew_pipe.transform(valid.data)\n",
    "\n",
    "# train.feature_type_update()\n",
    "# valid.feature_type_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Scale'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# scae pipe\n",
    "scale_pipe = PandasFeatureUnion([\n",
    "    (\"scale\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(train.feature_type[\"numeric\"]),\n",
    "        PlayWithPandas(preprocessing.StandardScaler())\n",
    "    )),\n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference(train.feature_type[\"numeric\"]))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = scale_pipe.fit_transform(train.data)\n",
    "valid.data = scale_pipe.transform(valid.data)\n",
    "\n",
    "train.feature_type_update()\n",
    "valid.feature_type_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Outliers'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T04:46:52.319612Z",
     "start_time": "2019-07-20T04:46:52.203155Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    (\"outlier\",train.OutlierIQR(\n",
    "                outlier_count=20,\n",
    "                iqr_step=1.5,\n",
    "                features=train.feature_type[\"numeric\"],\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqr_outliers = np.array(sorted(trainPipe.named_steps[\"outlier\"].outliers_))\n",
    "print(iqr_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T04:46:52.763195Z",
     "start_time": "2019-07-20T04:46:52.322439Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = ensemble.IsolationForest(\n",
    "    behaviour=\"new\", max_samples=train.data.shape[0], random_state=0, contamination=0.01\n",
    ")\n",
    "clf.fit(train.data[train.data.columns])\n",
    "preds = clf.predict(train.data[train.data.columns])\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)\n",
    "if_outliers = np.array(train.data[mask].index)\n",
    "print(if_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T04:47:02.233485Z",
     "start_time": "2019-07-20T04:46:52.767126Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers using extended isolation forest\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    (\"outlier\",train.ExtendedIsoForest(\n",
    "                cols=train.feature_type[\"numeric\"],\n",
    "                n_trees=100,\n",
    "                sample_size=256,\n",
    "                ExtensionLevel=1,\n",
    "                anomalies_ratio=0.03,\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "eif_outliers = np.array(sorted(trainPipe.named_steps[\"outlier\"].outliers_))\n",
    "print(eif_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T04:47:02.248576Z",
     "start_time": "2019-07-20T04:47:02.238402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "outliers = reduce(np.intersect1d, (iqr_outliers, if_outliers, eif_outliers))\n",
    "# outliers = reduce(np.intersect1d, (if_outliers, eif_outliers))\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# review outlier identification summary\n",
    "outlier_summary = train.outlier_summary(iqr_outliers=iqr_outliers,\n",
    "                             if_outliers=if_outliers,\n",
    "                             eif_outliers=eif_outliers\n",
    "                            )\n",
    "outlier_summary[outlier_summary[\"Count\"] >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# capture index values of known outliers\n",
    "knownOutliers = (\n",
    "    train.data[train.data[\"LotArea\"] > 60000].index.values.tolist()\n",
    "    + train.data[train.data[\"LotFrontage\"] > 300].index.values.tolist()\n",
    "    + train.data[train.data[\"GrLivArea\"] > 4000].index.values.tolist()\n",
    ")\n",
    "knownOutliers = sorted(set(knownOutliers))\n",
    "print(knownOutliers)\n",
    "\n",
    "# index of known outliers and outliers identified with the known outliers removed\n",
    "outliers = [\n",
    "    53,\n",
    "    185,\n",
    "    197,\n",
    "    437,\n",
    "    492,\n",
    "    762,\n",
    "    796,\n",
    "    821,\n",
    "    847,\n",
    "    1161,\n",
    "    1221,\n",
    "    1318,\n",
    "    1376,\n",
    "    249,\n",
    "    313,\n",
    "    335,\n",
    "    451,\n",
    "    523,\n",
    "    691,\n",
    "    706,\n",
    "    934,\n",
    "    1182,\n",
    "    1298,\n",
    "]\n",
    "print(outliers)\n",
    "\n",
    "# remove outlers from predictors and response\n",
    "train.data = train.data.drop(outliers)\n",
    "train.target = train.target.drop(index=outliers)\n",
    "\n",
    "print(train.data.shape)\n",
    "print(train.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T04:47:02.414899Z",
     "start_time": "2019-07-20T04:47:02.251981Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate feature importance summary\n",
    "estimators = [\n",
    "    lightgbm.LGBMRegressor,\n",
    "    ensemble.RandomForestRegressor,\n",
    "    ensemble.GradientBoostingRegressor,\n",
    "    ensemble.ExtraTreesRegressor,\n",
    "    ensemble.AdaBoostRegressor,\n",
    "    xgboost.XGBRegressor,\n",
    "]\n",
    "\n",
    "fs = train.FeatureSelector(\n",
    "    data=train.data,\n",
    "    target=train.target,\n",
    "    estimators=estimators,\n",
    "    rank=True,\n",
    "    classification=False,\n",
    ")\n",
    "# feature_selector_summary = fs.featureSelectorSuite()\n",
    "# feature_selector_summary[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate cross-validation performance\n",
    "estimators = [\n",
    "    svm.SVR,\n",
    "    lightgbm.LGBMRegressor,\n",
    "    xgboost.XGBRegressor,\n",
    "    ensemble.RandomForestRegressor,\n",
    "    ensemble.GradientBoostingRegressor,\n",
    "    ensemble.AdaBoostRegressor,\n",
    "    ensemble.ExtraTreesRegressor,\n",
    "    neighbors.KNeighborsRegressor,\n",
    "]\n",
    "\n",
    "cvSummary = fs.feature_selector_cross_val(\n",
    "    estimators=estimators,\n",
    "    feature_selector_summary=\"featureSelectionSummary_20191027_022938.csv\",\n",
    "    scoring=[\"root_mean_squared_error\"],\n",
    "    n_folds=8,\n",
    "    step=1,\n",
    "    n_jobs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize CV performance for diminishing feature set\n",
    "fs.featureSelectorResultsPlot(\n",
    "    metric=\"root_mean_squared_error\",\n",
    "    feature_selector_summary=\"featureSelectionSummary_20191027_022938.csv\",\n",
    "    cvSummary=\"cvSummary_20191028_125617.csv\",\n",
    "    showFeatures=False,\n",
    "    markerOn=False,\n",
    "    title_scale=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "crossValFeaturesDf = fs.createCrossValFeaturesDf(\n",
    "    metric=\"root_mean_squared_error\",\n",
    "    cvSummary= pd.read_csv(\"cvSummary_20191028_125617.csv\", index_col=0),\n",
    "    feature_selector_summary=pd.read_csv(\"featureSelectionSummary_20191027_022938.csv\", index_col=0),\n",
    ")\n",
    "crossValFeaturesDf#[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_feature_dict = fs.createCrossValFeaturesDict(\n",
    "    crossValFeaturesDf=crossValFeaturesDf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T04:47:08.943153Z",
     "start_time": "2019-07-20T04:47:02.418157Z"
    }
   },
   "outputs": [],
   "source": [
    "# percent difference summary\n",
    "dfDiff = abs(\n",
    "    (\n",
    "        ((valid.data.describe() + 1) - (train.data.describe() + 1))\n",
    "        / (train.data.describe() + 1)\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "dfDiff = dfDiff[dfDiff.columns].replace({0: np.nan})\n",
    "dfDiff[dfDiff < 0] = np.nan\n",
    "dfDiff = dfDiff.fillna(\"\")\n",
    "display(dfDiff)\n",
    "display(train.data[dfDiff.columns].describe())\n",
    "display(valid.data[dfDiff.columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-preparation-1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T03:52:56.392389Z",
     "start_time": "2019-10-21T03:52:48.676243Z"
    },
    "code_folding": [
     101,
     157
    ]
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# import training data\n",
    "df_train, df_valid = data.housing()\n",
    "# df_train = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-housing/train.csv\")\n",
    "train = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=[\"SalePrice\"],\n",
    "    remove_features=[\"Id\", \"MiscVal\"],\n",
    "    force_to_categorical=[\n",
    "        \"MSSubClass\",\n",
    "        \"OverallQual\",\n",
    "        \"OverallCond\",\n",
    "        \"YearBuilt\",\n",
    "        \"YearRemodAdd\",\n",
    "        \"MoSold\",\n",
    "        \"YrSold\",\n",
    "    ],\n",
    "    target_type=\"numeric\",\n",
    ")\n",
    "\n",
    "# additional features\n",
    "train.data[\"BsmtFinSF\"] = train.data[\"BsmtFinSF1\"] + train.data[\"BsmtFinSF2\"]\n",
    "train.data[\"TotalSF\"] = (\n",
    "    train.data[\"TotalBsmtSF\"] + train.data[\"1stFlrSF\"] + train.data[\"2ndFlrSF\"]\n",
    ")\n",
    "\n",
    "#################################################################################\n",
    "# import validation data\n",
    "# df_valid = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-housing/test.csv\")\n",
    "valid = mlm.Machine(\n",
    "    data=df_valid,\n",
    "    remove_features=[\"Id\", \"MiscVal\"],\n",
    "    force_to_categorical=[\n",
    "        \"MSSubClass\",\n",
    "        \"OverallQual\",\n",
    "        \"OverallCond\",\n",
    "        \"YearBuilt\",\n",
    "        \"YearRemodAdd\",\n",
    "        \"MoSold\",\n",
    "        \"YrSold\",\n",
    "    ],\n",
    "    target_type=\"numeric\",\n",
    ")\n",
    "\n",
    "# change clearly erroneous value to what it probably was\n",
    "valid.data[\"GarageYrBlt\"].replace({2207: 2007}, inplace=True)\n",
    "\n",
    "# additional features\n",
    "valid.data[\"BsmtFinSF\"] = valid.data[\"BsmtFinSF1\"] + valid.data[\"BsmtFinSF2\"]\n",
    "valid.data[\"TotalSF\"] = (\n",
    "    valid.data[\"TotalBsmtSF\"] + valid.data[\"1stFlrSF\"] + valid.data[\"2ndFlrSF\"]\n",
    ")\n",
    "valid.data.loc[valid.data[\"TotalSF\"].isnull(), \"TotalSF\"] = (\n",
    "    valid.data[\"1stFlrSF\"] + valid.data[\"2ndFlrSF\"]\n",
    ")\n",
    "\n",
    "train.feature_type_update()\n",
    "valid.feature_type_update()\n",
    "\n",
    "#################################################################################\n",
    "# impute pipeline\n",
    "categoricalConstant = ['GarageFinish', 'Alley', 'MasVnrType', 'GarageType', 'BsmtFinType1',\n",
    "                       'BsmtCond', 'BsmtFinType2', 'BsmtQual', 'PoolQC', 'GarageCond',\n",
    "                       'FireplaceQu', 'GarageQual', 'Fence', 'BsmtExposure', 'MiscFeature']\n",
    "numericConstant = [\"GarageYrBlt\",\"MasVnrArea\",\"BsmtUnfSF\",\"GarageArea\",\"BsmtFinSF\",\"BsmtFinSF1\",\"TotalBsmtSF\",\"BsmtFinSF2\"]\n",
    "categoricalMode = [\"Electrical\",\"Functional\",\"SaleType\",\"Exterior1st\",\"MSZoning\",\"Exterior2nd\",\"KitchenQual\",\"Utilities\"]\n",
    "numericMode = [\"BsmtHalfBath\", \"GarageCars\", \"BsmtFullBath\"]\n",
    "\n",
    "impute_pipe = PandasFeatureUnion([\n",
    "    (\"catConstant\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(categoricalConstant),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"constant\", fill_value=\"Nonexistent\"))\n",
    "    )),\n",
    "    (\"numConstant\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(numericConstant),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "    )),\n",
    "    (\"catMode\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(categoricalMode),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"numMode\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(numericMode),\n",
    "        PlayWithPandas(impute.SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"LotFrontage\", pipeline.make_pipeline(\n",
    "        DataFrameSelector([\"LotFrontage\",\"Neighborhood\"]),\n",
    "        ContextImputer(null_col=\"LotFrontage\", context_col=\"Neighborhood\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference([\"LotFrontage\"] + categoricalConstant + numericConstant + categoricalMode + numericMode))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = impute_pipe.fit_transform(train.data)\n",
    "valid.data = impute_pipe.transform(valid.data)\n",
    "\n",
    "#################################################################################\n",
    "# feature transformation pipeline\n",
    "nominal_columns = [\"MSSubClass\",\"MSZoning\",\"LandContour\",\"Neighborhood\",\"Condition1\",\"Condition2\",\"BldgType\",\n",
    "    \"HouseStyle\",\"RoofStyle\",\"RoofMatl\",\"Exterior1st\",\"Exterior2nd\",\"MasVnrType\",\"Foundation\",\"Heating\",\n",
    "    \"GarageType\",\"Fence\",\"SaleType\",\"SaleCondition\",\"MiscFeature\",]\n",
    "ordinal_columns = [\"Street\",\"Alley\",\"LotShape\",\"Utilities\",\"LotConfig\",\"LandSlope\",\"ExterQual\",\"ExterCond\",\"BsmtQual\",\n",
    "    \"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"HeatingQC\",\"CentralAir\",\"Electrical\",\"KitchenQual\",\n",
    "    \"Functional\",\"FireplaceQu\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\"PavedDrive\",\"PoolQC\",]\n",
    "\n",
    "ordinal_encodings = [\n",
    "    [\"Grvl\", \"Pave\"],  #  # Street\n",
    "    [\"Nonexistent\", \"Grvl\", \"Pave\"],  # Alley\n",
    "    [\"IR3\", \"IR2\", \"IR1\", \"Reg\"],  # LotShape\n",
    "    [\"ELO\", \"NoSeWa\", \"NoSewr\", \"AllPub\"],  # Utilities\n",
    "    [\"FR3\", \"FR2\", \"Corner\", \"Inside\", \"CulDSac\"],  # LotConfig\n",
    "    [\"Sev\", \"Mod\", \"Gtl\"],  # LandSlope\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # ExterQual\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # ExterCond\n",
    "    [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # BsmtQual\n",
    "    [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # BsmtCond\n",
    "    [\"Nonexistent\", \"No\", \"Mn\", \"Av\", \"Gd\"],  # BsmtExposure\n",
    "    [\"Nonexistent\", \"Unf\", \"LwQ\", \"BLQ\", \"Rec\", \"ALQ\", \"GLQ\"],  # BsmtFinType1\n",
    "    [\"Nonexistent\", \"Unf\", \"LwQ\", \"BLQ\", \"Rec\", \"ALQ\", \"GLQ\"],  # BsmtFinType2\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # HeatingQC\n",
    "    [\"N\", \"Y\"],  # CentralAir\n",
    "    [\"FuseP\", \"FuseF\", \"FuseA\", \"Mix\", \"SBrkr\"],  # Electrical\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # KitchenQual\n",
    "    [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],  # Functional\n",
    "    [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # FireplaceQu\n",
    "    [\"Nonexistent\", \"Unf\", \"RFn\", \"Fin\"],  # GarageFinish\n",
    "    [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # GarageQual\n",
    "    [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # GarageCond\n",
    "    [\"N\", \"P\", \"Y\"],  # PavedDrive\n",
    "    [\"Nonexistent\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],  # PoolQC\n",
    "]\n",
    "\n",
    "transform_pipe = PandasFeatureUnion([\n",
    "    (\"ordinal\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(ordinal_columns),\n",
    "        PlayWithPandas(preprocessing.OrdinalEncoder(categories=ordinal_encodings)),\n",
    "    )),\n",
    "    (\"nominal\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(nominal_columns),\n",
    "        PlayWithPandas(preprocessing.OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    )),\n",
    "    (\"numeric\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(train.feature_type[\"numeric\"]),\n",
    "        PlayWithPandas(preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)),\n",
    "#         DualTransformer(),\n",
    "        PlayWithPandas(preprocessing.StandardScaler()),\n",
    "    )),\n",
    "    (\"diff\", pipeline.make_pipeline(\n",
    "        DataFrameSelector(list(set(train.data.columns).difference(nominal_columns + ordinal_columns + train.feature_type[\"numeric\"]))),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = transform_pipe.fit_transform(train.data)\n",
    "valid.data = transform_pipe.transform(valid.data)\n",
    "\n",
    "train.feature_type_update()\n",
    "valid.feature_type_update()\n",
    "\n",
    "#################################################################################\n",
    "# remove outliers\n",
    "outliers = [\n",
    "    53,\n",
    "    185,\n",
    "    197,\n",
    "    437,\n",
    "    492,\n",
    "    762,\n",
    "    796,\n",
    "    821,\n",
    "    847,\n",
    "    1161,\n",
    "    1221,\n",
    "    1318,\n",
    "    1376,\n",
    "    249,\n",
    "    313,\n",
    "    335,\n",
    "    451,\n",
    "    523,\n",
    "    691,\n",
    "    706,\n",
    "    934,\n",
    "    1182,\n",
    "    1298,\n",
    "]\n",
    "train.data = train.data.drop(outliers)\n",
    "train.target = train.target.drop(index=outliers)\n",
    "\n",
    "# log transform target\n",
    "train.target = np.log1p(train.target)\n",
    "\n",
    "# # accuracy >= 7\n",
    "# bestCols = ['Age*Fare','Title_2','Fare*FamilySize','Sex_male','Fare','Pclass','CabinQuarter_X']\n",
    "# train.data = train.data[bestCols]\n",
    "# valid.data = valid.data[bestCols]\n",
    "\n",
    "# print('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Bayesian-hyper-parameter-optimization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:13:11.439851Z",
     "start_time": "2019-07-20T20:13:11.359222Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# model/parameter space\n",
    "all_space = {\n",
    "    \"linear_model.Lasso\": {\"alpha\": hp.uniform(\"alpha\", 0.0000001, 20)},\n",
    "    \"linear_model.Ridge\": {\"alpha\": hp.uniform(\"alpha\", 0.0000001, 20)},\n",
    "    \"linear_model.ElasticNet\": {\n",
    "        \"alpha\": hp.uniform(\"alpha\", 0.0000001, 20),\n",
    "        \"l1_ratio\": hp.uniform(\"l1_ratio\", 0.0, 0.2),\n",
    "    },\n",
    "    \"kernel_ridge.KernelRidge\": {\n",
    "        \"alpha\": hp.uniform(\"alpha\", 0.000001, 15),\n",
    "        \"kernel\": hp.choice(\"kernel\", [\"linear\", \"polynomial\", \"rbf\"]),\n",
    "        \"degree\": hp.choice(\"degree\", [2, 3]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "    },\n",
    "    \"lightgbm.LGBMRegressor\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"])\n",
    "        # ,'boosting_type': hp.choice('boosting_type'\n",
    "        #                    ,[{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'goss', 'subsample': 1.0}])\n",
    "        ,\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.uniform(\"min_child_samples\", 20, 500),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.uniform(\"num_leaves\", 8, 150),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.uniform(\"subsample_for_bin\", 20000, 400000),\n",
    "    },\n",
    "    \"xgboost.XGBRegressor\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 20),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "    },\n",
    "    \"ensemble.RandomForestRegressor\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.GradientBoostingRegressor\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"loss\": hp.choice(\"loss\", [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.AdaBoostRegressor\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"loss\": hp.choice(\"loss\", [\"linear\", \"square\", \"exponential\"]),\n",
    "    },\n",
    "    \"ensemble.ExtraTreesRegressor\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "    },\n",
    "    \"svm.SVR\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.00001, 10),\n",
    "        \"kernel\": hp.choice(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        \"degree\": hp.choice(\"degree\", [2, 3]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0001, 10),\n",
    "        \"epsilon\": hp.uniform(\"epsilon\", 0.001, 5),\n",
    "    },\n",
    "    \"neighbors.KNeighborsRegressor\": {\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "        \"n_neighbors\": hp.choice(\"n_neighbors\", np.arange(1, 20, dtype=int)),\n",
    "        \"weights\": hp.choice(\"weights\", [\"distance\", \"uniform\"]),\n",
    "        \"p\": hp.choice(\"p\", [1, 2]),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:08:06.345694Z",
     "start_time": "2019-07-20T20:08:02.750572Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "train.exec_bayes_optim_search(\n",
    "    all_space=all_space,\n",
    "    data=train.data,\n",
    "    target=train.target,\n",
    "    scoring=\"root_mean_squared_error\",\n",
    "    columns=cross_val_feature_dict,\n",
    "    n_folds=5,\n",
    "    n_jobs=8,\n",
    "    iters=750,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model loss by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:13:21.836103Z",
     "start_time": "2019-07-20T20:13:20.943293Z"
    }
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "bayes_optim_summary = pd.read_csv(\"\", na_values=\"nan\")\n",
    "bayes_optim_summary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:13:32.856001Z",
     "start_time": "2019-07-20T20:13:24.355066Z"
    }
   },
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(bayes_optim_summary[\"estimator\"]):\n",
    "    train.model_loss_plot(bayes_optim_summary=bayes_optim_summary, estimator=estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter selection by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:28:02.854397Z",
     "start_time": "2019-07-20T20:15:33.532394Z"
    }
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(bayes_optim_summary['estimator']):\n",
    "    train.modelParamPlot(bayes_optim_summary = bayes_optim_summary,\n",
    "                         estimator=estimator,\n",
    "                         all_space=all_space,\n",
    "                         n_iter=100,\n",
    "                         chart_prop=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_space = {\n",
    "                'param': hp.uniform('param', np.log(0.4), np.log(0.6))\n",
    "#     \"\": 0.000001 + hp.uniform(\"gamma\", 0.000001, 10)\n",
    "    #             'param2': hp.loguniform('param2', np.log(0.001), np.log(0.01))\n",
    "}\n",
    "\n",
    "train.sample_plot(sample_space, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model performance evaluation - standard models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Model-performance-evaluation-standard-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_models = train.topBayesOptimModels(bayes_optim_summary=bayes_optim_summary, numModels=1)\n",
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## standard model fit and predict\n",
    "# select estimator and iteration\n",
    "estimator = \"lightgbm.LGBMRegressor\"; model_iter = 417\n",
    "# estimator = \"xgboost.XGBRegressor\"; model_iter = 418\n",
    "# estimator = \"ensemble.RandomForestRegressor\"; model_iter = 382\n",
    "# estimator = \"ensemble.GradientBoostingRegressor\"; model_iter = 238\n",
    "# estimator = \"svm.SVR\"; model_iter = 259\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "model.fit(train.data.values, train.target.values)\n",
    "\n",
    "X_train, X_valid, y_train, yValid = model_selection.train_test_split(train.data, train.target)\n",
    "yPred = model.predict(train.data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_selector_summary = train.regression_panel(\n",
    "    model=model,\n",
    "    X_train=train.data,\n",
    "    y_train=train.target,\n",
    "    X_valid=None,\n",
    "    yValid=None,\n",
    "#     X_train=X_train,\n",
    "#     y_train=y_train,\n",
    "#     X_valid=X_valid,\n",
    "#     yValid=yValid,\n",
    "    n_folds=4,\n",
    "    randomState=10,\n",
    "#     feature_selector_summary=None\n",
    ")\n",
    "feature_selector_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data={'prediction' : model.predict(train.data.values),\n",
    "                  'actual' : train.target.values\n",
    "                  },\n",
    "             index=train.target.index\n",
    "            )\n",
    "results['diff'] = results['prediction'] - results['actual']\n",
    "results['diffAbs'] = abs(results['prediction'] - results['actual'])\n",
    "results['diffPerc'] = ((results['prediction'] - results['actual']) / results['actual']) * 100\n",
    "\n",
    "results.sort_values(['diffAbs'], ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model explanability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "estimator = \"xgboost.XGBRegressor\"; model_iter = 418\n",
    "\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "model.fit(train.data.values, train.target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# permutation importance - how much does performance decrease when shuffling a certain feature?\n",
    "perm = PermutationImportance(model.model, random_state=1).fit(train.data, train.target)\n",
    "eli5.show_weights(perm, feature_names=train.data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### SHAP values - training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Force plots - single observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in train.data.index[:5]:\n",
    "    train.single_shap_viz_tree(obsIx=i, model=model, data=train.data, target=train.target, classification=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Force plots - multiple observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = train.multi_shap_viz_tree(obs_ixs=train.data.index, model=model, data=train.data)\n",
    "visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obs_data, _, obs_shap_values = train.multi_shap_value_tree(\n",
    "    obs_ixs=train.data.index, model=model, data=train.data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "grid_features = [\"OverallCond\",\"LotFrontage\",\"TotalSF\",\"BsmtFinSF\",\"LotConfig\"]\n",
    "\n",
    "train.shap_dependence_grid(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    grid_features=grid_features,\n",
    "    all_features=train.data.columns,\n",
    "    dot_size=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "\n",
    "train.shap_dependence_plot(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    scatter_feature=\"TotalSF\",\n",
    "    color_feature=\"LotFrontage\",\n",
    "    feature_names=train.data.columns.tolist(),\n",
    "    dot_size=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "feature_names = train.data.columns.tolist()\n",
    "top_shap = np.argsort(-np.sum(np.abs(obs_shap_values), 0))\n",
    "\n",
    "# generate force plot\n",
    "for top_ix in top_shap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.make_canvas()\n",
    "    \n",
    "    train.shap_dependence_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        scatter_feature=feature_names[top_ix],\n",
    "        color_feature=\"Age\",\n",
    "        feature_names=feature_names,\n",
    "        dot_size=35,\n",
    "        alpha=0.5,\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "feature_names = train.data.columns.tolist()\n",
    "train.shap_summary_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        feature_names=feature_names,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### SHAP values - talidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Force plots - single observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in valid.data.index[:5]:\n",
    "    valid.single_shap_viz_tree(obsIx=i, model=model, data=valid.data, classification=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Force plots - multiple observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = valid.multi_shap_viz_tree(obs_ixs=valid.data.index, model=model, data=valid.data)\n",
    "visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obs_data, _, obs_shap_values = valid.multi_shap_value_tree(\n",
    "    obs_ixs=valid.data.index, model=model, data=valid.data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "grid_features = [\"OverallCond\",\"LotFrontage\",\"TotalSF\",\"BsmtFinSF\",\"LotConfig\"]\n",
    "\n",
    "valid.shap_dependence_grid(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    grid_features=grid_features,\n",
    "    all_features=valid.data.columns,\n",
    "    dot_size=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "\n",
    "valid.shap_dependence_plot(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    scatter_feature=\"TotalSF\",\n",
    "    color_feature=\"LotFrontage\",\n",
    "    feature_names=valid.data.columns.tolist(),\n",
    "    dot_size=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "feature_names = valid.data.columns.tolist()\n",
    "top_shap = np.argsort(-np.sum(np.abs(obs_shap_values), 0))\n",
    "\n",
    "# generate force plot\n",
    "for top_ix in top_shap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.make_canvas()\n",
    "    \n",
    "    valid.shap_dependence_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        scatter_feature=feature_names[top_ix],\n",
    "        color_feature=\"Age\",\n",
    "        feature_names=feature_names,\n",
    "        dot_size=35,\n",
    "        alpha=0.5,\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "feature_names = valid.data.columns.tolist()\n",
    "valid.shap_summary_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        feature_names=feature_names,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Submission - standard models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Submission-standard-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## standard model fit and predict\n",
    "# select estimator and iteration\n",
    "estimator = \"lightgbm.LGBMClassifier\"; model_iter = 668\n",
    "# estimator = \"xgboost.XGBClassifier\"; model_iter = 380\n",
    "# estimator = \"ensemble.RandomForestClassifier\"; model_iter = 411\n",
    "# estimator = \"ensemble.GradientBoostingClassifier\"; model_iter = 590\n",
    "# estimator = \"svm.SVC\"; model_iter = 135\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "model.fit(train.data.values, train.target.values)\n",
    "\n",
    "# fit model and make predictions\n",
    "yPred = model.predict(valid.data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "submit = pd.DataFrame({\"Id\": dfTest.Id, \"SalePrice\": np.expm1(yPred)})\n",
    "submit.to_csv(\"data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Stacking'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Primary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Primary-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:05:17.965746Z",
     "start_time": "2019-03-24T14:05:17.669308Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get out-of-fold predictions\n",
    "oof_train, oof_valid, columns = train.model_stacker(\n",
    "    models=top_models,\n",
    "    bayes_optim_summary=bayes_optim_summary,\n",
    "    X_train=train.data.values,\n",
    "    y_train=train.target.values,\n",
    "    X_valid=valid.data.values,\n",
    "    n_folds=10,\n",
    "    n_jobs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:11:12.912222Z",
     "start_time": "2019-03-24T14:09:44.583800Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# view correlations of predictions\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "p.pretty_corr_heatmap(\n",
    "    df=pd.DataFrame(oof_train, columns=columns), annot=True, ax=ax, vmin=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Meta-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:16:24.776352Z",
     "start_time": "2019-03-24T14:12:03.005790Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model/parameter space\n",
    "all_space = {\n",
    "    \"kernel_ridge.KernelRidge\": {\n",
    "        \"alpha\": hp.uniform(\"alpha\", 0.000001, 15),\n",
    "        \"kernel\": hp.choice(\"kernel\", [\"linear\", \"polynomial\", \"rbf\"]),\n",
    "        \"degree\": hp.choice(\"degree\", [2, 3]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "    },\n",
    "    \"lightgbm.LGBMRegressor\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"])\n",
    "        # ,'boosting_type': hp.choice('boosting_type'\n",
    "        #                    ,[{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'goss', 'subsample': 1.0}])\n",
    "        ,\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.uniform(\"min_child_samples\", 20, 500),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.uniform(\"num_leaves\", 8, 150),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.uniform(\"subsample_for_bin\", 20000, 400000),\n",
    "    },\n",
    "    \"xgboost.XGBRegressor\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 20),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "    },\n",
    "    \"ensemble.RandomForestRegressor\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.GradientBoostingRegressor\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"loss\": hp.choice(\"loss\", [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"svm.SVR\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.00001, 10),\n",
    "        \"kernel\": hp.choice(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        \"degree\": hp.choice(\"degree\", [2, 3]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0001, 10),\n",
    "        \"epsilon\": hp.uniform(\"epsilon\", 0.001, 5),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:16:24.983826Z",
     "start_time": "2019-03-24T14:16:24.779451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "train.exec_bayes_optim_search(\n",
    "    all_space=all_space,\n",
    "    resultsDir=\"{}_hyperopt_meta_{}.csv\".format(rundate, analysis),\n",
    "    X=oof_train,\n",
    "    y=train.target,\n",
    "    scoring=\"accuracy\",\n",
    "    n_folds=8,\n",
    "    n_jobs=10,\n",
    "    iters=1000,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "analysis = \"housing\"\n",
    "rundate = \"20190807\"\n",
    "bayes_optim_summary_meta = pd.read_csv(\"{}_hyperopt_meta_{}.csv\".format(rundate, analysis))\n",
    "bayes_optim_summary_meta[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(bayes_optim_summary_meta[\"estimator\"]):\n",
    "    train.model_loss_plot(bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(bayes_optim_summary_meta[\"estimator\"]):\n",
    "    train.modelParamPlot(\n",
    "        bayes_optim_summary=bayes_optim_summary_meta,\n",
    "        estimator=estimator,\n",
    "        all_space=all_space,\n",
    "        n_iter=100,\n",
    "        chart_prop=15,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model performance evaluation - stacked models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Model-performance-evaluation-stacked-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_models = train.topBayesOptimModels(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, numModels=1\n",
    ")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "estimator = \"lightgbm.LGBMClassifier\"; model_iter = 668\n",
    "# estimator = \"xgboost.XGBClassifier\"; model_iter = 380\n",
    "# estimator = \"ensemble.RandomForestClassifier\"; model_iter = 411\n",
    "# estimator = \"ensemble.GradientBoostingClassifier\"; model_iter = 590\n",
    "# estimator = \"svm.SVC\"; model_iter = 135\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "# single model evaluation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ,multi model evaluation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Submission - stacked models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Submission-stacked-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:17:12.332889Z",
     "start_time": "2019-03-24T14:17:11.732365Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "estimator = \"lightgbm.LGBMClassifier\"; model_iter = 668\n",
    "# estimator = \"xgboost.XGBClassifier\"; model_iter = 380\n",
    "# estimator = \"ensemble.RandomForestClassifier\"; model_iter = 411\n",
    "# estimator = \"ensemble.GradientBoostingClassifier\"; model_iter = 590\n",
    "# estimator = \"svm.SVC\"; model_iter = 135\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "model.fit(oof_train, train.target.values)\n",
    "yPred = model.predict(oof_valid)\n",
    "# print(sum(yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:17:12.422440Z",
     "start_time": "2019-03-24T14:17:12.335453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "submit = pd.DataFrame({\"Id\": dfTest.Id, \"SalePrice\": np.expm1(yPred)})\n",
    "submit.to_csv(\"data/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
