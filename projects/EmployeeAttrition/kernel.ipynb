{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Project - Employee attrition__\n",
    "\n",
    "1. [Import](#Import)\n",
    "    1. [Tools](#Tools)\n",
    "    1. [Data](#Data)    \n",
    "1. [EDA](#EDA)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA)\n",
    "    1. [Count feature EDA](#Count-feature-EDA)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA)\n",
    "    1. [Faceting](#Faceting)\n",
    "    1. [Target variable evaluation](#Target-variable-evaluation)    \n",
    "1. [Data preparation](#Data-preparation)\n",
    "    1. [Missing data](#Missing-data)\n",
    "    1. [Feature engineering](#Feature-engineering)\n",
    "        1. [Handcrafted](#Handcrafted)\n",
    "        1. [Polynomial features](#Polynomial-features)\n",
    "        1. [Encoding](#Encoding)\n",
    "    1. [Feature transformation](#Feature-transformation)\n",
    "        1. [Skew correction](#Skew-correction)\n",
    "        1. [Scaling](#Scaling)     \n",
    "    1. [Outliers](#Outliers)\n",
    "    1. [Additional exploratory data analysis](#Additional-exploratory-data-analysis)\n",
    "1. [Feature importance](#Feature-importance)    \n",
    "1. [Modeling](#Modeling)\n",
    "    1. [Data preparation](#Data-preparation-1)\n",
    "    1. [Bayesian hyper-parameter optimization](#Bayesian-hyper-parameter-optimization)\n",
    "    1. [Model performance evaluation - standard models](#Model-performance-evaluation-standard-models)\n",
    "    1. [Validation set evaluation - standard models](#Validation-set-evaluation-standard-models)\n",
    "    1. [Model explanability](#Model-explanability)\n",
    "1. [Stacking](#Stacking)\n",
    "    1. [Primary models](#Primary-models)\n",
    "    1. [Meta model](#Meta-model)                \n",
    "    1. [Model performance evaluation - stacked models](#Model-performance-evaluation-stacked-models)\n",
    "    1. [Validation set evaluation - stacked models](#Validation-set-evaluation-stacked-models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:55:03.081490Z",
     "start_time": "2020-02-02T04:55:01.089626Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from functools import reduce\n",
    "import time\n",
    "\n",
    "rundate = time.strftime(\"%Y%m%d\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "# pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.datasets import (\n",
    "    load_boston,\n",
    "    load_wine,\n",
    "    load_iris,\n",
    "    load_breast_cancer,\n",
    "    make_blobs,\n",
    "    make_moons,\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    IsolationForest,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import (\n",
    "    Lasso,\n",
    "    Ridge,\n",
    "    ElasticNet,\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    SGDRegressor,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    "    PolynomialFeatures,\n",
    "    OrdinalEncoder,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    KBinsDiscretizer,\n",
    "    QuantileTransformer,\n",
    "    PowerTransformer,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "from category_encoders import (\n",
    "    WOEEncoder,\n",
    "    TargetEncoder,\n",
    "    CatBoostEncoder,\n",
    "    BinaryEncoder,\n",
    "    CountEncoder,\n",
    ")\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "from hyperopt import hp\n",
    "\n",
    "import eif\n",
    "import shap\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import squarify\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    #     import mlmachine as mlm\n",
    "    #     from prettierplot.plotter import PrettierPlot\n",
    "    #     import prettierplot.style as style\n",
    "    import asdfasd\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(\n",
    "        \"../../../mlmachine\"\n",
    "    ) if \"../../../../mlmachine\" not in sys.path else None\n",
    "    sys.path.append(\n",
    "        \"../../../prettierplot\"\n",
    "    ) if \"../../../../prettierplot\" not in sys.path else None\n",
    "\n",
    "    import mlmachine as mlm\n",
    "    import mlmachine.data as data\n",
    "    from mlmachine.features.preprocessing import (\n",
    "        DataFrameSelector,\n",
    "        PandasPipeline,\n",
    "        KFoldSelectEncoder,\n",
    "        ContextImputer,\n",
    "        PandasFeatureUnion,\n",
    "        DualTransformer,\n",
    "    )\n",
    "    from prettierplot.plotter import PrettierPlot\n",
    "    import prettierplot.style as style\n",
    "else:\n",
    "    print(\n",
    "        \"This notebook relies on the libraries mlmachine and prettierplot. Please run:\"\n",
    "    )\n",
    "    print(\"\\tpip install mlmachine\")\n",
    "    print(\"\\tpip install prettierplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:25.835895Z",
     "start_time": "2020-02-02T04:23:25.802869Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data and print dimensions\n",
    "dataset = data.attrition()\n",
    "print(\"Training data dimensions: {}\".format(dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:25.899268Z",
     "start_time": "2020-02-02T04:23:25.848861Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display info and first 5 rows\n",
    "dataset.info()\n",
    "display(dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:25.928910Z",
     "start_time": "2020-02-02T04:23:25.921403Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# review counts of different column types\n",
    "dataset.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:25.958993Z",
     "start_time": "2020-02-02T04:23:25.936695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# split dataset into train and validation datasets\n",
    "df_train, df_valid = mlm.train_test_df_compile(data=dataset, target_col='Attrition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:25.974476Z",
     "start_time": "2020-02-02T04:23:25.961977Z"
    },
    "code_folding": [
     0,
     15,
     20,
     29,
     36,
     49
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "continuous = [\n",
    "    \"Age\",\n",
    "    \"DailyRate\",\n",
    "    \"DistanceFromHome\",\n",
    "    \"HourlyRate\",\n",
    "    \"MonthlyIncome\",\n",
    "    \"MonthlyRate\",\n",
    "    \"PercentSalaryHike\",\n",
    "    \"TotalWorkingYears\",\n",
    "    \"YearsAtCompany\",\n",
    "    \"YearsInCurrentRole\",\n",
    "    \"YearsSinceLastPromotion\",\n",
    "    \"YearsWithCurrManager\",\n",
    "]\n",
    "\n",
    "count = [\n",
    "    \"NumCompaniesWorked\",\n",
    "    \"TrainingTimesLastYear\",\n",
    "]\n",
    "\n",
    "nominal = [\n",
    "    \"MaritalStatus\",\n",
    "    \"EducationField\",\n",
    "    \"Department\",\n",
    "    \"Gender\",\n",
    "    \"JobRole\",\n",
    "    \"OverTime\",\n",
    "]\n",
    "\n",
    "remove_features = [\n",
    "    \"EmployeeNumber\",\n",
    "    \"EmployeeCount\",\n",
    "    \"StandardHours\",\n",
    "    \"Over18\",\n",
    "]\n",
    "\n",
    "ordinal = [\n",
    "    \"Education\",\n",
    "    \"EnvironmentSatisfaction\",\n",
    "    \"JobInvolvement\",\n",
    "    \"JobLevel\",\n",
    "    \"JobSatisfaction\",\n",
    "    \"PerformanceRating\",\n",
    "    \"RelationshipSatisfaction\",\n",
    "    \"StockOptionLevel\",\n",
    "    \"WorkLifeBalance\",\n",
    "    \"BusinessTravel\",\n",
    "]\n",
    "\n",
    "ordinal_encodings = {\n",
    "    \"Education\": [1, 2, 3, 4, 5],\n",
    "    \"EnvironmentSatisfaction\": [1, 2, 3, 4],\n",
    "    \"JobInvolvement\": [1, 2, 3, 4],\n",
    "    \"JobLevel\": [1, 2, 3, 4, 5],\n",
    "    \"JobSatisfaction\": [1, 2, 3, 4],\n",
    "    \"PerformanceRating\": [3, 4],\n",
    "    \"RelationshipSatisfaction\": [1, 2, 3, 4],\n",
    "    \"StockOptionLevel\": [0, 1, 2, 3],\n",
    "    \"WorkLifeBalance\": [1, 2, 3, 4],\n",
    "    \"BusinessTravel\": ['Non-Travel','Travel_Rarely','Travel_Frequently'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:26.021248Z",
     "start_time": "2020-02-02T04:23:25.978208Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data into mlmachine\n",
    "train = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"Attrition\",\n",
    "    remove_features=remove_features,\n",
    "    identify_as_continuous=continuous,\n",
    "    identify_as_count=count,    \n",
    "    identify_as_nominal=nominal,\n",
    "    identify_as_ordinal=ordinal,\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    target_type=\"category\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:26.074037Z",
     "start_time": "2020-02-02T04:23:26.047394Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data into mlmachine\n",
    "valid = mlm.Machine(\n",
    "    data=df_valid,\n",
    "    target=\"Attrition\",\n",
    "    remove_features=remove_features,\n",
    "    identify_as_continuous=continuous,\n",
    "    identify_as_count=count,    \n",
    "    identify_as_nominal=nominal,\n",
    "    identify_as_ordinal=ordinal,\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Category feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Category-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:39.146090Z",
     "start_time": "2020-02-02T04:23:26.088835Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number features\n",
    "for feature in train.data.mlm_dtypes[\"category\"]:\n",
    "    train.eda_cat_target_cat_feat(\n",
    "        feature=feature,\n",
    "        level_count_cap=20,\n",
    "        legend_labels=[\"Stayed\",\"Left\"],\n",
    "        chart_scale=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Count feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Count-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:40.898827Z",
     "start_time": "2020-02-02T04:23:39.150059Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number features\n",
    "for feature in train.data.mlm_dtypes[\"count\"]:\n",
    "    train.eda_cat_target_cat_feat(\n",
    "        feature=feature,\n",
    "        level_count_cap=20,\n",
    "        legend_labels=[\"Stayed\",\"Left\"],\n",
    "        chart_scale=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Continuous-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:56.362347Z",
     "start_time": "2020-02-02T04:23:40.902833Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number features\n",
    "for feature in train.data.mlm_dtypes[\"continuous\"]:\n",
    "    train.eda_cat_target_num_feat(\n",
    "        feature=feature,\n",
    "#         outliers_out_of_scope=10,\n",
    "        legend_labels=[\"Stayed\",\"Left\"],\n",
    "        chart_scale=12\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:57.148679Z",
     "start_time": "2020-02-02T04:23:56.366282Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap(df=train.data, annot=False, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:23:57.865856Z",
     "start_time": "2020-02-02T04:23:57.153975Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot(plot_orientation='tall')\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap_target(\n",
    "    df=train.data, target=train.target, thresh=0.02, annot=True, ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:24:39.816827Z",
     "start_time": "2020-02-02T04:23:57.870812Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chart_scale=12)\n",
    "p.pair_plot(df=train.data, columns=train.data.mlm_dtypes['continuous'], diag_kind=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:17.726481Z",
     "start_time": "2020-02-02T04:24:39.835187Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chart_scale=12)\n",
    "p.pair_plot(\n",
    "    df=train.data.dropna(),\n",
    "    diag_kind=\"kde\",\n",
    "    target=train.target,\n",
    "    columns=train.data.mlm_dtypes['continuous'][:10],\n",
    "    legend_labels=[\"Stayed\",\"Left\"],\n",
    "    bbox=(2.0, 0.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Faceting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Faceting'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:18.071258Z",
     "start_time": "2020-02-02T04:25:17.728728Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# facet MaritalStatus vs. Gender\n",
    "p = PrettierPlot(chart_scale=12)\n",
    "ax = p.make_canvas(title=\"Attrition, MaritalStatus vs. Gender\", y_shift=0.7)\n",
    "p.facet_two_cat_bar(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    x=\"MaritalStatus\",\n",
    "    y=train.target.name,\n",
    "    split=\"Gender\",\n",
    "    y_units=\"fff\",\n",
    "    bbox = (1.2, 0.8),\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:18.370068Z",
     "start_time": "2020-02-02T04:25:18.073681Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# facet MaritalStatus vs. Gender\n",
    "p = PrettierPlot(chart_scale=12)\n",
    "ax = p.make_canvas(title=\"Attrition, BusinessTravel vs. Gender\", y_shift=0.7)\n",
    "p.facet_two_cat_bar(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    x=\"BusinessTravel\",\n",
    "    y=train.target.name,\n",
    "    split=\"Gender\",\n",
    "    y_units=\"fff\",\n",
    "    bbox = (1.2, 0.8),\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:18.689579Z",
     "start_time": "2020-02-02T04:25:18.376940Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# facet MaritalStatus vs. Gender\n",
    "p = PrettierPlot(chart_scale=12)\n",
    "ax = p.make_canvas(title=\"Attrition, JobSatisfaction vs. Gender\", y_shift=0.7)\n",
    "p.facet_two_cat_bar(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    x=\"JobSatisfaction\",\n",
    "    y=train.target.name,\n",
    "    split=\"Gender\",\n",
    "    y_units=\"fff\",\n",
    "    bbox = (1.2, 0.8),\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:19.237631Z",
     "start_time": "2020-02-02T04:25:18.693862Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# facet MaritalStatus vs. Gender\n",
    "p = PrettierPlot(chart_scale=12)\n",
    "ax = p.make_canvas(title=\"Attrition, JobSatisfaction by Education\", y_shift=0.7)\n",
    "p.facet_two_cat_bar(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    x=\"JobSatisfaction\",\n",
    "    y=train.target.name,\n",
    "    split=\"Education\",\n",
    "    y_units=\"fff\",\n",
    "    bbox = (1.3, 0.8),\n",
    "    ax=ax,\n",
    "    legend_labels = ['Below College','College','Bachelor','Master','Doctor']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:20.260502Z",
     "start_time": "2020-02-02T04:25:19.242621Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.facet_two_cat_point(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    x=\"Education\",\n",
    "    y=train.target.name,\n",
    "    split=\"Gender\",\n",
    "    cat_col=\"JobSatisfaction\",\n",
    "    height=5,\n",
    "    bbox=(1.3, 1.2),\n",
    "#     legend_labels=[\"1st class\", \"2nd class\", \"3rd class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:20.966031Z",
     "start_time": "2020-02-02T04:25:20.268124Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.facet_two_cat_point(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    x=\"BusinessTravel\",\n",
    "    y=train.target.name,\n",
    "    split=\"Gender\",\n",
    "    cat_col=\"MaritalStatus\",\n",
    "    aspect = 1.4,\n",
    "    height=5,\n",
    "    bbox=(1.3, 1.2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:24.314630Z",
     "start_time": "2020-02-02T04:25:20.969501Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.facet_cat_num_hist(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    split=train.target.name,\n",
    "    legend_labels=[\"Stayed\", \"Left\"],\n",
    "    cat_row=\"Gender\",\n",
    "    cat_col=\"Education\",\n",
    "    num_col=\"Age\",\n",
    "    bbox=(1.9, 1.0),\n",
    "    height=4,\n",
    "    aspect=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:26.399041Z",
     "start_time": "2020-02-02T04:25:24.334273Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.facet_cat_num_hist(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    split=train.target.name,\n",
    "    legend_labels=[\"Stayed\", \"Left\"],\n",
    "    cat_row=\"Gender\",\n",
    "    cat_col=\"MaritalStatus\",\n",
    "    num_col=\"Age\",\n",
    "    bbox=(1.9, 1.0),\n",
    "    height=4,\n",
    "    aspect=1.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Target-variable-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:26.420126Z",
     "start_time": "2020-02-02T04:25:26.409914Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# null score\n",
    "pd.Series(train.target).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Data-preparation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Missing-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:26.442204Z",
     "start_time": "2020-02-02T04:25:26.427724Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "train.eda_missing_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:26.459253Z",
     "start_time": "2020-02-02T04:25:26.444635Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "valid.eda_missing_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:26.477130Z",
     "start_time": "2020-02-02T04:25:26.461756Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "train.missing_col_compare(train=train.data, validation=valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Impute'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Feature-engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Handcrafted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Handcrafted'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-19T02:56:52.163234Z",
     "start_time": "2020-01-19T02:56:41.576496Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Polynomial-features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:27.201655Z",
     "start_time": "2020-02-02T04:25:26.479309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transform pipe\n",
    "polynomial_pipe = PandasFeatureUnion([\n",
    "    (\"polynomial\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "        PandasPipeline(PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"continuous\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = polynomial_pipe.fit_transform(train.data)\n",
    "valid.data = polynomial_pipe.transform(valid.data)\n",
    "\n",
    "train.update_dtypes()\n",
    "valid.update_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Encoding'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:27.229693Z",
     "start_time": "2020-02-02T04:25:27.208167Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in training data string columns\n",
    "train.data[train.data.mlm_dtypes[\"category\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:27.260529Z",
     "start_time": "2020-02-02T04:25:27.231901Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each object columns\n",
    "for col in train.data[train.data.mlm_dtypes[\"category\"]]:\n",
    "    print(col, np.unique(train.data[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:27.286053Z",
     "start_time": "2020-02-02T04:25:27.263527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in validation data string columns\n",
    "valid.data[valid.data.mlm_dtypes[\"category\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:27.317172Z",
     "start_time": "2020-02-02T04:25:27.293566Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each object columns\n",
    "for col in valid.data[valid.data.mlm_dtypes[\"category\"]]:\n",
    "    print(col, np.unique(valid.data[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:27.353120Z",
     "start_time": "2020-02-02T04:25:27.319489Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify values that are present in the training data but not the validation data, and vice versa\n",
    "for col in train.data.mlm_dtypes[\"category\"]:\n",
    "    train_values = train.data[col].unique()\n",
    "    valid_values = valid.data[col].unique()\n",
    "\n",
    "    train_diff = set(train_values) - set(valid_values)\n",
    "    valid_diff = set(valid_values) - set(train_values)\n",
    "\n",
    "    if len(train_diff) > 0 or len(valid_diff) > 0:\n",
    "        print(\"\\n\\n*** \" + col)\n",
    "        print(\"Value present in training data, not in validation data\")\n",
    "        print(train_diff)\n",
    "        print(\"Value present in validation data, not in training data\")\n",
    "        print(valid_diff)\n",
    "    else:\n",
    "        print(' {} = fully represented'.format(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:28.969445Z",
     "start_time": "2020-02-02T04:25:27.357818Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encode pipeline\n",
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"nominal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=nominal),\n",
    "        PandasPipeline(OneHotEncoder(drop=\"first\")),\n",
    "    )),\n",
    "    (\"ordinal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=list(ordinal_encodings.keys())),\n",
    "        PandasPipeline(OrdinalEncoder(categories=list(ordinal_encodings.values()))),\n",
    "    )),\n",
    "    (\"bin\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=train.data.mlm_dtypes[\"continuous\"]),\n",
    "        PandasPipeline(KBinsDiscretizer(encode=\"ordinal\")),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=nominal + list(ordinal_encodings.keys())),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = encode_pipe.fit_transform(train.data)\n",
    "valid.data = encode_pipe.transform(valid.data)\n",
    "\n",
    "train.update_dtypes()\n",
    "valid.update_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:28.987553Z",
     "start_time": "2020-02-02T04:25:28.980525Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# encode_pipe = Pipeline([\n",
    "#     ('nominal', OneHotEncoder()),\n",
    "# ])\n",
    "\n",
    "# encode_pipe.fit_transform(train.data[[\"MaritalStatus\",\"Gender\"]]).todense()[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:29.010572Z",
     "start_time": "2020-02-02T04:25:29.003317Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train.data[[\"MaritalStatus\",\"Gender\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:29.030023Z",
     "start_time": "2020-02-02T04:25:29.025123Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encode_pipe = PandasFeatureUnion([\n",
    "#     (\"nominal\", make_pipeline(\n",
    "#         PandasPipeline(OneHotEncoder()),\n",
    "#     )),\n",
    "# ])\n",
    "# encode_pipe.fit_transform(train.data[[\"MaritalStatus\",\"Gender\"]]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:29.048080Z",
     "start_time": "2020-02-02T04:25:29.041453Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encode_pipe = FeatureUnion([\n",
    "#     (\"nominal\", make_pipeline(\n",
    "#         OneHotEncoder(),\n",
    "#     )),\n",
    "# ])\n",
    "# encode_pipe.fit_transform(train.data[[\"MaritalStatus\",\"Gender\"]]).todense()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:39.212888Z",
     "start_time": "2020-02-02T04:25:29.058820Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "target_encode_pipe = PandasFeatureUnion([\n",
    "    (\"target\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldSelectEncoder(\n",
    "            target=train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False, random_state=0),\n",
    "            encoder=TargetEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"woe\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldSelectEncoder(\n",
    "            target=train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False, random_state=0),\n",
    "            encoder=WOEEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"catboost\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldSelectEncoder(\n",
    "            target=train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False, random_state=0),\n",
    "            encoder=CatBoostEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"category\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = target_encode_pipe.fit_transform(train.data)\n",
    "valid.data = target_encode_pipe.transform(valid.data)\n",
    "\n",
    "train.update_dtypes()\n",
    "valid.update_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Feature-transformation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Skew correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Skew-correction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:39.229740Z",
     "start_time": "2020-02-02T04:26:39.215188Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# skew correction pipeline\n",
    "skew_pipe = PandasFeatureUnion([\n",
    "    (\"skew\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "        DualTransformer(),\n",
    "    )),    \n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"continuous\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# train.data = skew_pipe.fit_transform(train.data)\n",
    "# valid.data = skew_pipe.transform(valid.data)\n",
    "\n",
    "# train.update_dtypes()\n",
    "# valid.update_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:47.829205Z",
     "start_time": "2020-02-02T04:26:39.285580Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of number features - training data\n",
    "train.skew_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:49.795538Z",
     "start_time": "2020-02-02T04:26:47.831904Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of number features - validation data\n",
    "valid.skew_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Scaling'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:50.260208Z",
     "start_time": "2020-02-02T04:26:49.797713Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# scale pipeline\n",
    "scale_pipe = PandasFeatureUnion([\n",
    "    (\"scale\", make_pipeline(\n",
    "        DataFrameSelector(),\n",
    "        PandasPipeline(RobustScaler())\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = scale_pipe.fit_transform(train.data)\n",
    "valid.data = scale_pipe.transform(valid.data)\n",
    "\n",
    "train.update_dtypes()\n",
    "valid.update_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Outliers'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:51.010863Z",
     "start_time": "2020-02-02T04:26:50.262708Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "train_pipe = Pipeline([\n",
    "    (\"outlier\",train.OutlierIQR(\n",
    "                outlier_count=5,\n",
    "                iqr_step=1.5,\n",
    "                features=train.data.mlm_dtypes[\"continuous\"],\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "train.data = train_pipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqr_outliers = np.array(sorted(train_pipe.named_steps[\"outlier\"].outliers))\n",
    "print(iqr_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:52.112359Z",
     "start_time": "2020-02-02T04:26:51.013044Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = IsolationForest(\n",
    "    behaviour=\"new\", max_samples=train.data.shape[0], random_state=0, contamination=0.01\n",
    ")\n",
    "clf.fit(train.data[train.data.columns])\n",
    "preds = clf.predict(train.data[train.data.columns])\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)\n",
    "if_outliers = np.array(train.data[mask].index)\n",
    "print(if_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:56.139503Z",
     "start_time": "2020-02-02T04:26:52.114728Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers using extended isolation forest\n",
    "train_pipe = Pipeline([\n",
    "    (\"outlier\",train.ExtendedIsoForest(\n",
    "                columns=train.data.mlm_dtypes[\"continuous\"],\n",
    "                n_trees=100,\n",
    "                sample_size=256,\n",
    "                extension_level=1,\n",
    "                anomalies_ratio=0.03,\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "train.data = train_pipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "eif_outliers = np.array(sorted(train_pipe.named_steps[\"outlier\"].outliers))\n",
    "print(eif_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:56.148172Z",
     "start_time": "2020-02-02T04:26:56.141847Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "outliers = reduce(np.intersect1d, (iqr_outliers, if_outliers, eif_outliers))\n",
    "# outliers = reduce(np.intersect1d, (if_outliers, eif_outliers))\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:56.170374Z",
     "start_time": "2020-02-02T04:26:56.151645Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# review outlier identification summary\n",
    "outlier_summary = train.outlier_summary(iqr_outliers=iqr_outliers,\n",
    "                             if_outliers=if_outliers,\n",
    "                             eif_outliers=eif_outliers\n",
    "                            )\n",
    "outlier_summary[outlier_summary[\"count\"] >= 3].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:56.253852Z",
     "start_time": "2020-02-02T04:26:56.177483Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove outlers from predictors and response\n",
    "outliers = np.array([123, 63, 976, 237, 126, 914, 473, 187, 270, 875, 1116, 427])\n",
    "train.data = train.data.drop(outliers)\n",
    "train.target = train.target.drop(index=outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Additional exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Additional-exploratory-data-analysis'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:26:58.668760Z",
     "start_time": "2020-02-02T04:26:56.276478Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot(plot_orientation='tall',chart_scale=15)\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap_target(\n",
    "    df=train.data,\n",
    "    target=train.target,\n",
    "    thresh=0.2,\n",
    "    annot=True,\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:35:20.279205Z",
     "start_time": "2020-02-02T04:26:58.671554Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate feature importance summary\n",
    "estimators = [\n",
    "    LGBMClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    XGBClassifier,\n",
    "]\n",
    "\n",
    "fs = train.FeatureSelector(\n",
    "    data=train.data,\n",
    "    target=train.target,\n",
    "    estimators=estimators,\n",
    ")\n",
    "feature_selector_summary = fs.feature_selector_suite(\n",
    "    save_to_csv=True,\n",
    "    n_jobs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T03:54:18.033990Z",
     "start_time": "2020-02-01T03:54:17.965870Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# calculate cross-validation performance\n",
    "estimators = [\n",
    "    SVC,\n",
    "    LGBMClassifier,\n",
    "    LogisticRegression,\n",
    "    XGBClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    #AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    KNeighborsClassifier,\n",
    "]\n",
    "\n",
    "cv_summary = fs.feature_selector_cross_val(\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    "#     feature_selector_summary=pd.read_csv(\"feature_selection_summary_2001291515.csv\", index_col=0),\n",
    "    estimators=estimators,\n",
    "    scoring=[\"accuracy\",\"roc_auc\"],\n",
    "    n_folds=5,\n",
    "    step=1,\n",
    "    n_jobs=4,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# visualize CV performance for diminishing feature set\n",
    "fs.feature_selector_results_plot(\n",
    "#     cv_summary=cv_summary,\n",
    "#     feature_selector_summary=feature_selector_summary,\n",
    "    cv_summary= pd.read_csv(\".csv\", index_col=0),\n",
    "    feature_selector_summary=pd.read_csv(\"feature_selection_summary_2001291515.csv\", index_col=0),\n",
    "    scoring=\"accuracy\",\n",
    "    title_scale=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_val_features_df = fs.create_cross_val_features_df(\n",
    "    scoring=\"accuracy\",\n",
    "    cv_summary=cv_summary,\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    "#     cv_summary= pd.read_csv(\"cv_summary_2001291517.csv\", index_col=0),\n",
    "#     feature_selector_summary=pd.read_csv(\"feature_selection_summary_2001291333.csv\", index_col=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cross_val_feature_dict = fs.create_cross_val_features_dict(\n",
    "    cross_val_features_df=cross_val_features_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# visualize CV performance for diminishing feature set\n",
    "fs.feature_selector_results_plot(\n",
    "    metric=\"roc_auc\",\n",
    "    title_scale=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_val_features_df = fs.create_cross_val_features_df(\n",
    "    scoring=\"roc_auc\",\n",
    "    cv_summary=cv_summary,\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    "#     cv_summary= pd.read_csv(\"cv_summary_2001291517.csv\", index_col=0),\n",
    "#     feature_selector_summary=pd.read_csv(\"feature_selection_summary_2001291333.csv\", index_col=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cross_val_feature_dict = fs.create_cross_val_features_dict(\n",
    "    cross_val_features_df=cross_val_features_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-preparation-1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T04:55:09.831Z"
    },
    "code_folding": [
     7,
     22,
     27,
     36,
     43,
     56
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# import data\n",
    "dataset = data.attrition()\n",
    "\n",
    "# split dataset into train and validation datasets\n",
    "df_train, df_valid = mlm.train_test_df_compile(data=dataset, target_col='Attrition')\n",
    "\n",
    "continuous = [\n",
    "    \"Age\",\n",
    "    \"DailyRate\",\n",
    "    \"DistanceFromHome\",\n",
    "    \"HourlyRate\",\n",
    "    \"MonthlyIncome\",\n",
    "    \"MonthlyRate\",\n",
    "    \"PercentSalaryHike\",\n",
    "    \"TotalWorkingYears\",\n",
    "    \"YearsAtCompany\",\n",
    "    \"YearsInCurrentRole\",\n",
    "    \"YearsSinceLastPromotion\",\n",
    "    \"YearsWithCurrManager\",\n",
    "]\n",
    "\n",
    "count = [\n",
    "    \"NumCompaniesWorked\",\n",
    "    \"TrainingTimesLastYear\",\n",
    "]\n",
    "\n",
    "nominal = [\n",
    "    \"MaritalStatus\",\n",
    "    \"EducationField\",\n",
    "    \"Department\",\n",
    "    \"Gender\",\n",
    "    \"JobRole\",\n",
    "    \"OverTime\",\n",
    "]\n",
    "\n",
    "remove_features = [\n",
    "    \"EmployeeNumber\",\n",
    "    \"EmployeeCount\",\n",
    "    \"StandardHours\",\n",
    "    \"Over18\",\n",
    "]\n",
    "\n",
    "ordinal = [\n",
    "    \"Education\",\n",
    "    \"EnvironmentSatisfaction\",\n",
    "    \"JobInvolvement\",\n",
    "    \"JobLevel\",\n",
    "    \"JobSatisfaction\",\n",
    "    \"PerformanceRating\",\n",
    "    \"RelationshipSatisfaction\",\n",
    "    \"StockOptionLevel\",\n",
    "    \"WorkLifeBalance\",\n",
    "    \"BusinessTravel\",\n",
    "]\n",
    "\n",
    "ordinal_encodings = {\n",
    "    \"Education\": [1, 2, 3, 4, 5],\n",
    "    \"EnvironmentSatisfaction\": [1, 2, 3, 4],\n",
    "    \"JobInvolvement\": [1, 2, 3, 4],\n",
    "    \"JobLevel\": [1, 2, 3, 4, 5],\n",
    "    \"JobSatisfaction\": [1, 2, 3, 4],\n",
    "    \"PerformanceRating\": [3, 4],\n",
    "    \"RelationshipSatisfaction\": [1, 2, 3, 4],\n",
    "    \"StockOptionLevel\": [0, 1, 2, 3],\n",
    "    \"WorkLifeBalance\": [1, 2, 3, 4],\n",
    "    \"BusinessTravel\": ['Non-Travel','Travel_Rarely','Travel_Frequently'],\n",
    "}\n",
    "\n",
    "# import training data\n",
    "# Load training data into mlmachine\n",
    "train = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"Attrition\",\n",
    "    remove_features=remove_features,\n",
    "    identify_as_continuous=continuous,\n",
    "    identify_as_count=count,    \n",
    "    identify_as_nominal=nominal,\n",
    "    identify_as_ordinal=ordinal,\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    target_type=\"category\",\n",
    ")\n",
    "\n",
    "# import valid data\n",
    "# Load training data into mlmachine\n",
    "valid = mlm.Machine(\n",
    "    data=df_valid,\n",
    "    target=\"Attrition\",\n",
    "    remove_features=remove_features,\n",
    "    identify_as_continuous=continuous,\n",
    "    identify_as_count=count,    \n",
    "    identify_as_nominal=nominal,\n",
    "    identify_as_ordinal=ordinal,\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    ")\n",
    "\n",
    "#################################################################################\n",
    "### feature transformation pipeline\n",
    "\n",
    "# polynomial feature pipe\n",
    "polynomial_pipe = PandasFeatureUnion([\n",
    "    (\"polynomial\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "        PandasPipeline(PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"continuous\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = polynomial_pipe.fit_transform(train.data)\n",
    "valid.data = polynomial_pipe.transform(valid.data)\n",
    "\n",
    "train.update_dtypes()\n",
    "valid.update_dtypes()\n",
    "\n",
    "# encode feature pipeline\n",
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"nominal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=nominal),\n",
    "        PandasPipeline(OneHotEncoder(drop=\"first\")),\n",
    "    )),\n",
    "    (\"ordinal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=list(ordinal_encodings.keys())),\n",
    "        PandasPipeline(OrdinalEncoder(categories=list(ordinal_encodings.values()))),\n",
    "    )),\n",
    "    (\"bin\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=train.data.mlm_dtypes[\"continuous\"]),\n",
    "        PandasPipeline(KBinsDiscretizer(encode=\"ordinal\")),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=nominal + list(ordinal_encodings.keys())),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = encode_pipe.fit_transform(train.data)\n",
    "valid.data = encode_pipe.transform(valid.data)\n",
    "\n",
    "train.update_dtypes()\n",
    "valid.update_dtypes()\n",
    "\n",
    "# target encoded feature pipeline\n",
    "#\n",
    "target_encode_pipe = PandasFeatureUnion([\n",
    "    (\"target\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldSelectEncoder(\n",
    "            target=train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False, random_state=0),\n",
    "            encoder=TargetEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"woe\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldSelectEncoder(\n",
    "            target=train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False, random_state=0),\n",
    "            encoder=WOEEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"catboost\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldSelectEncoder(\n",
    "            target=train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False, random_state=0),\n",
    "            encoder=CatBoostEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"category\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = target_encode_pipe.fit_transform(train.data)\n",
    "valid.data = target_encode_pipe.transform(valid.data)\n",
    "\n",
    "train.update_dtypes()\n",
    "valid.update_dtypes()\n",
    "\n",
    "# skew correction pipeline\n",
    "skew_pipe = PandasFeatureUnion([\n",
    "    (\"skew\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "        DualTransformer(),\n",
    "    )),    \n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"continuous\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# train.data = skew_pipe.fit_transform(train.data)\n",
    "# valid.data = skew_pipe.transform(valid.data)\n",
    "\n",
    "# train.update_dtypes()\n",
    "# valid.update_dtypes()\n",
    "\n",
    "# scale pipeline\n",
    "scale_pipe = PandasFeatureUnion([\n",
    "    (\"scale\", make_pipeline(\n",
    "        DataFrameSelector(),\n",
    "        PandasPipeline(RobustScaler())\n",
    "    )),\n",
    "])\n",
    "\n",
    "train.data = scale_pipe.fit_transform(train.data)\n",
    "valid.data = scale_pipe.transform(valid.data)\n",
    "\n",
    "train.update_dtypes()\n",
    "valid.update_dtypes()\n",
    "\n",
    "#################################################################################\n",
    "# remove outliers\n",
    "outliers = np.array([123, 63, 976, 237, 126, 914, 473, 187, 270, 875, 1116, 427])\n",
    "train.data = train.data.drop(outliers)\n",
    "train.target = train.target.drop(index=outliers)\n",
    "\n",
    "# accuracy >= 7\n",
    "# train.data = train.data[best_columns]\n",
    "# valid.data = valid.data[best_columns]\n",
    "\n",
    "print('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Bayesian-hyper-parameter-optimization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T04:55:13.863Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# model/parameter space\n",
    "all_space = {\n",
    "    \"LGBMClassifier\": {\n",
    "        \"class_weight\": hp.choice(\"class_weight\", [None, \"balanced\"]),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"])\n",
    "        # ,'boosting_type': hp.choice('boosting_type'\n",
    "        #                    ,[{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'goss', 'subsample': 1.0}])\n",
    "        ,\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.uniform(\"min_child_samples\", 20, 500),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"num_leaves\": hp.uniform(\"num_leaves\", 8, 150),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.5),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.uniform(\"subsample_for_bin\", 20000, 400000),\n",
    "    },\n",
    "#     \"LogisticRegression\": {\n",
    "#         \"C\": hp.loguniform(\"C\", np.log(0.001), np.log(0.2)),\n",
    "#         \"penalty\": hp.choice(\"penalty\", [\"l1\", \"l2\"]),\n",
    "#     },\n",
    "    \"XGBClassifier\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 20),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.3, 1),\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"loss\": hp.choice(\"loss\", [\"deviance\", \"exponential\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"AdaBoostClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"SAMME\", \"SAMME.R\"]),\n",
    "    },\n",
    "    \"ExtraTreesClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.001, 15),\n",
    "        \"decision_function_shape\": hp.choice(\"decision_function_shape\", [\"ovo\", \"ovr\"]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.000000001, 5),\n",
    "    },\n",
    "    \"KNeighborsClassifier\": {\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "        \"n_neighbors\": hp.choice(\"n_neighbors\", np.arange(1, 20, dtype=int)),\n",
    "        \"weights\": hp.choice(\"weights\", [\"distance\", \"uniform\"]),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T06:10:48.964508Z",
     "start_time": "2019-07-20T04:43:54.172908Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "train.exec_bayes_optim_search(\n",
    "    all_space=all_space,\n",
    "    data=train.data,\n",
    "    target=train.target,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_folds=5,\n",
    "    n_jobs=8,\n",
    "    iters=2000,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model loss by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:54.668306Z",
     "start_time": "2019-07-20T21:36:54.259967Z"
    }
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "bayes_optim_summary = pd.read_csv(\"\", na_values=\"nan\")\n",
    "bayes_optim_summary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:30:59.839867Z",
     "start_time": "2019-07-20T21:30:54.281216Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(bayes_optim_summary[\"estimator\"]):\n",
    "    train.model_loss_plot(bayes_optim_summary=bayes_optim_summary, estimator=estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter selection by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T01:47:36.876677Z",
     "start_time": "2019-07-19T01:47:23.026068Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(bayes_optim_summary[\"estimator\"]):\n",
    "    train.modelParamPlot(\n",
    "        bayes_optim_summary=bayes_optim_summary,\n",
    "        estimator=estimator,\n",
    "        all_space=all_space,\n",
    "        n_iter=100,\n",
    "        chart_scale=15,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T04:23:45.411641Z",
     "start_time": "2019-07-18T04:23:44.926648Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_space = {\n",
    "                'param': hp.uniform('param', np.log(0.4), np.log(0.6))\n",
    "#     \"\": 0.000001 + hp.uniform(\"gamma\", 0.000001, 10)\n",
    "    #             'param2': hp.loguniform('param2', np.log(0.001), np.log(0.01))\n",
    "}\n",
    "\n",
    "train.sample_plot(sample_space, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance evaluation - standard models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Model-performance-evaluation-standard-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:59.047576Z",
     "start_time": "2019-07-20T21:36:59.013711Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_models = train.top_bayes_optim_models(bayes_optim_summary=bayes_optim_summary, num_models=1)\n",
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:37:19.341306Z",
     "start_time": "2019-07-20T21:37:02.406940Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classification panel, single model\n",
    "estimator = \"XGBClassifier\"; model_iter = 218\n",
    "# estimator = 'GradientBoostingClassifier'; model_iter = 590\n",
    "# estimator = 'XGBClassifier'; model_iter = 380\n",
    "\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "train.classification_panel(\n",
    "    model=model, X_train=train.data, y_train=train.target, cm_labels=['Stays', 'Quits']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classification reports\n",
    "for estimator, model_iters in top_models.items():\n",
    "    for model_iter in model_iters:\n",
    "        model = train.BayesOptimModelBuilder(\n",
    "            bayes_optim_summary=bayes_optim_summary,\n",
    "            estimator=estimator,\n",
    "            model_iter=model_iter,\n",
    "        )\n",
    "        train.classification_panel(\n",
    "            model=model, X_train=train.data, y_train=train.target, cm_labels=['Stays', 'Quits']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set evaluation - standard models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation-set-evaluation-standard-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard model fit and predict\n",
    "# select estimator and iteration\n",
    "# estimator = \"LGBMClassifier\"; model_iter = 476\n",
    "estimator = \"XGBClassifier\"; model_iter = 418\n",
    "# estimator = \"RandomForestClassifier\"; model_iter = 382\n",
    "# estimator = \"GradientBoostingClassifier\"; model_iter = 238\n",
    "# estimator = \"SVC\"; model_iter = 135\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "# classification panel for validation data\n",
    "train.classification_panel(\n",
    "    model=model,\n",
    "    X_train=train.data,\n",
    "    y_train=train.target,\n",
    "    X_valid=valid.data,\n",
    "    y_valid=valid.target,\n",
    "    cm_labels=['Stays', 'Quits'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classification reports\n",
    "for estimator, model_iters in top_models.items():\n",
    "    for model_iter in model_iters:\n",
    "        model = train.BayesOptimModelBuilder(\n",
    "            bayes_optim_summary=bayes_optim_summary,\n",
    "            estimator=estimator,\n",
    "            model_iter=model_iter,\n",
    "        )\n",
    "        train.classification_panel(\n",
    "            model=model,\n",
    "            X_train=train.data,\n",
    "            y_train=train.target,\n",
    "            X_valid=valid.data,\n",
    "            y_valid=valid.target,\n",
    "            labels=[0, 1],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model explanability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "estimator = \"ExtraTreesClassifier\"; model_iter = 145\n",
    "estimator = \"XGBClassifier\"; model_iter = 218\n",
    "\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "model.fit(train.data.values, train.target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.726938Z",
     "start_time": "2019-07-01T02:02:15.740Z"
    }
   },
   "outputs": [],
   "source": [
    "# permutation importance - how much does performance decrease when shuffling a certain feature?\n",
    "perm = PermutationImportance(model.model, random_state=1).fit(train.data, train.target)\n",
    "eli5.show_weights(perm, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SHAP values - training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Force plots - single observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in train.data.index[:5]:\n",
    "    train.single_shap_viz_tree(obsIx=i, model=model, data=train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Force plots -multiple observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = train.multi_shap_viz_tree(obs_ixs=train.data.index, model=model, data=train.data)\n",
    "visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obs_data, _, obs_shap_values = train.multi_shap_value_tree(\n",
    "    obs_ixs=train.data.index, model=model, data=train.data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "grid_features = [\n",
    "    \"BusinessTravel\",\n",
    "    \"Age\",\n",
    "    \"WorkLifeBalance\",\n",
    "    \"Education\",\n",
    "    \"DistanceFromHome\",\n",
    "    \"MonthlyIncome\",\n",
    "    \"Gender_Male\",\n",
    "]\n",
    "\n",
    "train.shap_dependence_grid(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    grid_features=grid_features,\n",
    "    all_features=train.data.columns,\n",
    "    dot_size=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "\n",
    "train.shap_dependence_plot(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    scatter_feature=\"Age\",\n",
    "    color_feature=\"BusinessTravel\",\n",
    "    feature_names=train.data.columns,\n",
    "    dot_size=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "feature_names = train.data.columns.tolist()\n",
    "top_shap = np.argsort(-np.sum(np.abs(obs_shap_values), 0))\n",
    "\n",
    "for top_ix in top_shap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.make_canvas()\n",
    "\n",
    "    train.shap_dependence_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        scatter_feature=feature_names[top_ix],\n",
    "        color_feature=\"Age\",\n",
    "        feature_names=feature_names,\n",
    "        dot_size=50,\n",
    "        alpha=0.5,\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "train.shap_summary_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        feature_names=train.data.columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SHAP values - validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Force plots - single observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in valid.data.index[:5]:\n",
    "    valid.single_shap_viz_tree(obsIx=i, model=model, data=valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Force plots -multiple observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = valid.multi_shap_viz_tree(obs_ixs=valid.data.index, model=model, data=valid.data)\n",
    "visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obs_data, _, obs_shap_values = valid.multi_shap_value_tree(\n",
    "    obs_ixs=valid.data.index, model=model, data=valid.data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "grid_features = [\n",
    "    \"BusinessTravel\",\n",
    "    \"Age\",\n",
    "    \"WorkLifeBalance\",\n",
    "    \"Education\",\n",
    "    \"DistanceFromHome\",\n",
    "    \"MonthlyIncome\",\n",
    "    \"Gender_Male\",\n",
    "]\n",
    "\n",
    "valid.shap_dependence_grid(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    grid_features=grid_features,\n",
    "    all_features=valid.data.columns,\n",
    "    dot_size=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "\n",
    "valid.shap_dependence_plot(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    scatter_feature=\"Age\",\n",
    "    color_feature=\"BusinessTravel\",\n",
    "    feature_names=valid.data.columns,\n",
    "    dot_size=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "feature_names = valid.data.columns.tolist()\n",
    "top_shap = np.argsort(-np.sum(np.abs(obs_shap_values), 0))\n",
    "\n",
    "for top_ix in top_shap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.make_canvas()\n",
    "\n",
    "    valid.shap_dependence_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        scatter_feature=feature_names[top_ix],\n",
    "        color_feature=\"Age\",\n",
    "        feature_names=feature_names,\n",
    "        dot_size=50,\n",
    "        alpha=0.5,\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "valid.shap_summary_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        feature_names=valid.data.columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Stacking'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Primary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Primary-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T04:08:08.084300Z",
     "start_time": "2019-07-18T04:08:08.069443Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get out-of-fold predictions\n",
    "oof_train, oof_valid, columns = train.model_stacker(\n",
    "    models=top_models,\n",
    "    bayes_optim_summary=bayes_optim_summary,\n",
    "    X_train=train.data.values,\n",
    "    y_train=train.target.values,\n",
    "    X_valid=valid.data.values,\n",
    "    n_folds=10,\n",
    "    n_jobs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T04:08:42.303668Z",
     "start_time": "2019-07-18T04:08:42.292269Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# view correlations of predictions\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap(\n",
    "    df=pd.DataFrame(oof_train, columns=columns), annot=True, ax=ax, vmin=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Meta-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.777169Z",
     "start_time": "2019-07-01T02:02:15.841Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parameter space\n",
    "all_space = {\n",
    "    \"LGBMClassifier\": {\n",
    "        \"class_weight\": hp.choice(\"class_weight\", [None]),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"dart\"]),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.15, 0.25),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(4, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.quniform(\"min_child_samples\", 50, 150, 5),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.quniform(\"num_leaves\", 30, 70, 1),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.75, 1.25),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.quniform(\"subsample_for_bin\", 100000, 350000, 20000),\n",
    "    },\n",
    "    \"XGBClassifier\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"gamma\": hp.quniform(\"gamma\", 0.0, 10, 0.05),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.2, 0.01),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_child_weight\": hp.quniform(\"min_child_weight\", 2.5, 7.5, 1),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.4, 0.7),\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 10, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 8000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(15, 25, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "    },\n",
    "    \"GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 11, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.09, 0.01),\n",
    "        \"loss\": hp.choice(\"loss\", [\"deviance\", \"exponential\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.00000001, 15),\n",
    "        \"decision_function_shape\": hp.choice(\"decision_function_shape\", [\"ovr\", \"ovo\"]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.00000001, 1.5),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.779160Z",
     "start_time": "2019-07-01T02:02:15.847Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "train.exec_bayes_optim_search(\n",
    "    all_space=all_space,\n",
    "    results_dir=\"{}_hyperopt_meta_{}.csv\".format(rundate, analysis),\n",
    "    X=oof_train,\n",
    "    y=train.target,\n",
    "    scoring=\"f1_micro\",\n",
    "    n_folds=8,\n",
    "    n_jobs=10,\n",
    "    iters=1000,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "analysis = \"attrition\"\n",
    "rundate = \"20190807\"\n",
    "bayes_optim_summary_meta = pd.read_csv(\"{}_hyperopt_meta_{}.csv\".format(rundate, analysis))\n",
    "bayes_optim_summary_meta[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(bayes_optim_summary_meta[\"estimator\"]):\n",
    "    train.model_loss_plot(bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.781225Z",
     "start_time": "2019-07-01T02:02:15.853Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(bayes_optim_summary_meta['estimator']):\n",
    "    train.modelParamPlot(bayes_optim_summary = bayes_optim_summary_meta,\n",
    "                         estimator=estimator,\n",
    "                         all_space=all_space,\n",
    "                         n_iter=100,\n",
    "                         chart_scale=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model performance evaluation - stacked models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Model-performance-evaluation-stacked-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_models = train.top_bayes_optim_models(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, num_models=1\n",
    ")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "estimator = \"LGBMClassifier\"; model_iter = 668\n",
    "# estimator = \"XGBClassifier\"; model_iter = 380\n",
    "# estimator = \"RandomForestClassifier\"; model_iter = 411\n",
    "# estimator = \"GradientBoostingClassifier\"; model_iter = 590\n",
    "# estimator = \"SVC\"; model_iter = 135\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "train.classification_panel(\n",
    "    model=model, X_train=oof_train, y_train=train.target, labels=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create classification reports\n",
    "for estimator, model_iters in top_models.items():\n",
    "    for model_iter in model_iters:\n",
    "        model = train.BayesOptimModelBuilder(\n",
    "            bayes_optim_summary=bayes_optim_summary_meta,\n",
    "            estimator=estimator,\n",
    "            model_iter=model_iter,\n",
    "        )\n",
    "        train.classification_panel(\n",
    "            model=model, X_train=oof_train, y_train=train.target, labels=[0, 1], n_folds=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Validation set evaluation - stacked models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "<a id = 'Validation-set-evaluation-stacked-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.787282Z",
     "start_time": "2019-07-01T02:02:15.874Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## standard model fit and predict\n",
    "# select estimator and iteration\n",
    "estimator = \"LGBMClassifier\"; model_iter = 668\n",
    "# estimator = \"XGBClassifier\"; model_iter = 380\n",
    "# estimator = \"RandomForestClassifier\"; model_iter = 411\n",
    "# estimator = \"GradientBoostingClassifier\"; model_iter = 590\n",
    "# estimator = \"SVC\"; model_iter = 135\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "model.fit(oof_train, train.target.values)\n",
    "\n",
    "# fit model and make predictions\n",
    "y_pred = model.predict(oof_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train.classification_panel(\n",
    "    model=model,\n",
    "    X_train=oof_train,\n",
    "    y_train=train.target,\n",
    "    X_valid=oof_valid,\n",
    "    y_valid=valid.target,\n",
    "    labels=[0, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create classification reports\n",
    "for estimator, model_iters in top_models.items():\n",
    "    for model_iter in model_iters:\n",
    "        model = train.BayesOptimModelBuilder(\n",
    "            bayes_optim_summary=bayes_optim_summary_meta,\n",
    "            estimator=estimator,\n",
    "            model_iter=model_iter,\n",
    "        )\n",
    "        train.classification_panel(\n",
    "            model=model,\n",
    "            X_train=oof_train,\n",
    "            y_train=train.target,\n",
    "            X_valid=oof_valid,\n",
    "            y_valid=valid.target,\n",
    "            labels=[0, 1],\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
