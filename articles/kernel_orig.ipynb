{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kaggle competition - Titanic__\n",
    "\n",
    "1. [Import](#Import)\n",
    "    1. [Tools](#Tools)\n",
    "    1. [Data](#Data)    \n",
    "1. [Initial EDA](#Initial-EDA)\n",
    "    1. [object feature EDA](#object-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target)\n",
    "    1. [number feature EDA](#number-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target2)\n",
    "        1. [Correlation](#Correlation)\n",
    "        1. [Pair plot](#Pair-plot)\n",
    "    1. [Faceting](#Faceting)\n",
    "    1. [Target variable evaluation](#Target-variable-evaluation)    \n",
    "1. [Data preparation](#Data-preparation)\n",
    "    1. [Outliers (preliminary)](#Outliers-preliminary)\n",
    "        1. [Evaluate](#Evaluate)\n",
    "        1. [Remove](#remove)\n",
    "    1. [Missing data](#Missing-data)\n",
    "        1. [Evaluate](#Evaluate1)\n",
    "        1. [Impute](#Impute)\n",
    "    1. [Engineering](#Engineering)\n",
    "        1. [Evaluate](#Evaluate3)\n",
    "        1. [Engineer](#Engineer)\n",
    "    1. [Encoding](#Encoding)\n",
    "        1. [Evaluate](#Evaluate2)\n",
    "        1. [Encode](#Encode)\n",
    "    1. [Transformation](#Transformation)\n",
    "        1. [Evaluate](#Evaluate4)\n",
    "        1. [Transform](#Transform)\n",
    "    1. [Outliers (final)](#Outliers-final)\n",
    "        1. [Evaluate](#Evaluate5)\n",
    "        1. [Remove](#remove1)\n",
    "1. [Data evaluation](#Data-evaluation)\n",
    "    1. [Feature importance](#Feature-importance)    \n",
    "    1. [Rationality](#Rationality)\n",
    "    1. [Value override](#Value-override)\n",
    "    1. [number feature EDA](#number-feature-EDA3)\n",
    "    1. [Correlation](#Correlation3)\n",
    "1. [Modeling](#Modeling)\n",
    "    1. [Data preparation](#Data-preparation-1)\n",
    "    1. [Bayesian hyper-parameter optimization](#Bayesian-hyper-parameter-optimization)\n",
    "        1. [Model loss by iteration](#Model-loss-by-iteration)\n",
    "        1. [Parameter selection by iteration](#Parameter-selection-by-iteration)\n",
    "    1. [Model performance evaluation - standard models](#Model-performance-evaluation-standard-models)\n",
    "    1. [Model explanability](#Model-explanability)\n",
    "        1. [Permutation importance](#Permutation-importance)\n",
    "        1. [SHAP values](#SHAP-values)\n",
    "    1. [Submission - standard models](#Submission-standard-models)\n",
    "1. [Stacking](#Stacking)\n",
    "    1. [Primary models](#Primary-models)\n",
    "    1. [Meta model](#Meta-model)                \n",
    "    1. [Model performance evaluation - stacked models](#Model-performance-evaluation-stacked-models)\n",
    "    1. [Submission - stacked models](#Submission-stacked-models)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import csv\n",
    "import ast\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "global ITERATION\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "rundate = time.strftime(\"%Y%m%d\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.gaussian_process as gaussian_process\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.kernel_ridge as kernel_ridge\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.naive_bayes as naive_bayes\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.utils as utils\n",
    "\n",
    "import eif\n",
    "import shap; shap.initjs()\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "from scipy import stats, special\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import catboost\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "#     import mlmachine as mlm\n",
    "#     from prettierplot.plotter import PrettierPlot\n",
    "#     import prettierplot.style as style\n",
    "    import asdfasd\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(\"../../../mlmachine\") if \"../../../../mlmachine\" not in sys.path else None\n",
    "    sys.path.append(\"../../../prettierplot\") if \"../../../../prettierplot\" not in sys.path else None\n",
    "    \n",
    "    import mlmachine as mlm\n",
    "    from prettierplot.plotter import PrettierPlot\n",
    "    import prettierplot.style as style\n",
    "else:\n",
    "    print('This notebook relies on the libraries mlmachine and prettierplot. Please run:')\n",
    "    print('\\tpip install mlmachine')\n",
    "    print('\\tpip install prettierplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and print dimensions\n",
    "df_train = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-titanic//train.csv\")\n",
    "df_valid = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-titanic//test.csv\")\n",
    "\n",
    "print(\"Training data dimensions: {}\".format(df_train.shape))\n",
    "print(\"Validation data dimensions: {}\".format(df_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display info and first 5 rows\n",
    "df_train.info()\n",
    "display(df_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review counts of different column types\n",
    "df_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data into mlmachine\n",
    "train = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"Survived\",\n",
    "    remove_features=[\"PassengerId\", \"Ticket\"],\n",
    "    force_to_object=[\"Pclass\", \"SibSp\", \"Parch\"],\n",
    "    target_type=\"object\",\n",
    ")\n",
    "print(train.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data into mlmachine\n",
    "valid = mlm.Machine(\n",
    "    data=df_valid,\n",
    "    remove_features=[\"PassengerId\", \"Ticket\"],\n",
    "    force_to_object=[\"Pclass\", \"SibSp\", \"Parch\"],\n",
    ")\n",
    "print(valid.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Initial-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## object feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'object-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object features\n",
    "for feature in train.feature_type[\"object\"]:\n",
    "    train.eda_cat_target_cat_feat(feature=feature, level_count_cap=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## number feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'number-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number features\n",
    "for feature in train.feature_type[\"number\"]:\n",
    "    train.eda_cat_target_num_feat(feature=feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation (all samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heat map\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "p.pretty_corr_heatmap(df=train.data, annot=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot(plot_orientation='tall',chart_prop=10)\n",
    "ax = p.make_canvas()\n",
    "p.pretty_corr_heatmap_target(\n",
    "    df=train.data, target=train.target, thresh=0.01, annot=True, ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - There are three pairs of highly correlated features:\n",
    "    - 'GarageArea' and 'GarageCars'\n",
    "    - 'TotRmsAbvGrd' and 'GrLivArea'\n",
    "    - '1stFlrSF' and 'TotalBsmtSF\n",
    "This makes sense, given what each feature represents and how each pair items relate to each other. We likely only need one feature from each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Pair-plot'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chart_prop=12)\n",
    "p.pretty_pair_plot(df=train.data, diag_kind=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chart_prop=12)\n",
    "p.pretty_pair_plot(\n",
    "    df=train.data.dropna(),\n",
    "    diag_kind=\"kde\",\n",
    "    target=train.target,\n",
    "    columns=[\"Age\", \"Fare\", \"Pclass\", \"Parch\", \"SibSp\"],\n",
    "    legend_labels=[\"Died\", \"Survived\"],\n",
    "    bbox=(2.0, 0.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faceting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Faceting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### object by object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chart_prop=12)\n",
    "ax = p.make_canvas(title=\"Survivorship, embark location by passenger class\", y_shift=0.7)\n",
    "p.pretty_facet_two_cat_bar(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    x=\"Embarked\",\n",
    "    y=train.target.name,\n",
    "    split=\"Pclass\",\n",
    "    y_units=\"ff\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chart_prop=12)\n",
    "ax = p.make_canvas(title=\"Survivorship, passenger class by gender\", y_shift=0.7)\n",
    "p.pretty_facet_two_cat_bar(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    x=\"Pclass\",\n",
    "    y=train.target.name,\n",
    "    split=\"Sex\",\n",
    "    y_units=\"ff\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chart_prop=12)\n",
    "ax = p.make_canvas(title=\"Survivorship,embark location by gender\", y_shift=0.7)\n",
    "p.pretty_facet_two_cat_bar(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    x=\"Embarked\",\n",
    "    y=train.target.name,\n",
    "    split=\"Sex\",\n",
    "    y_units=\"ff\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.pretty_facet_two_cat_point(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    x=\"Sex\",\n",
    "    y=train.target.name,\n",
    "    split=\"Pclass\",\n",
    "    cat_row=\"Embarked\",\n",
    "    aspect=1.0,\n",
    "    height=5,\n",
    "    bbox=(1.3, 1.2),\n",
    "    legend_labels=[\"1st class\", \"2nd class\", \"3rd class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.pretty_facet_two_cat_point(\n",
    "    df=train.recombine_data(train.data, train.target).dropna(subset=[\"Embarked\"]),\n",
    "    x=\"Embarked\",\n",
    "    y=train.target.name,\n",
    "    split=\"Pclass\",\n",
    "    cat_row=\"Sex\",\n",
    "    aspect=1.0,\n",
    "    height=5,\n",
    "    bbox=(1.5, 0.8),\n",
    "    legend_labels=[\"1st class\", \"2nd class\", \"3rd class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### object by number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.pretty_facet_cat_num_hist(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    split=train.target.name,\n",
    "    legend_labels=[\"Died\", \"Lived\"],\n",
    "    cat_row=\"Sex\",\n",
    "    cat_col=\"Embarked\",\n",
    "    num_col=\"Age\",\n",
    "    bbox=(1.9, 1.0),\n",
    "    height=4,\n",
    "    aspect=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot(chart_prop=15)\n",
    "p.pretty_facet_cat_num_scatter(\n",
    "    df=train.recombine_data(train.data, train.target),\n",
    "    split=train.target.name,\n",
    "    legend_labels=[\"Died\", \"Lived\"],\n",
    "    cat_row=\"Sex\",\n",
    "    cat_col=\"Embarked\",\n",
    "    xNum=\"Fare\",\n",
    "    yNum=\"Age\",\n",
    "    bbox=(1.9, 1.0),\n",
    "    height=4,\n",
    "    aspect=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Target-variable-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null score\n",
    "pd.Series(train.target).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-preparation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (preliminary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-preliminary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify columns that have zero missing values\n",
    "nonNull = train.data.columns[train.data.isnull().sum() == 0].values.tolist()\n",
    "\n",
    "# identify intersection between non-null columns and number columns\n",
    "nonNullnum_col = list(set(nonNull).intersection(train.feature_type[\"number\"]))\n",
    "print(nonNullnum_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "train_pipe = pipeline.Pipeline([\n",
    "    (\"outlier\",train.OutlierIQR(\n",
    "                outlier_count=2,\n",
    "                iqr_step=1.5,\n",
    "                features=[\"Age\", \"SibSp\", \"Parch\", \"Fare\"],\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "train.data = train_pipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqr_outliers = np.array(sorted(train_pipe.named_steps[\"outlier\"].outliers_))\n",
    "print(iqr_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = ensemble.IsolationForest(\n",
    "    behaviour=\"new\", max_samples=train.data.shape[0], random_state=0, contamination=0.02\n",
    ")\n",
    "clf.fit(train.data[[\"SibSp\", \"Parch\", \"Fare\"]])\n",
    "preds = clf.predict(train.data[[\"SibSp\", \"Parch\", \"Fare\"]])\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)\n",
    "if_outliers = np.array(train.data[mask].index)\n",
    "print(if_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers using extended isolation forest\n",
    "train_pipe = pipeline.Pipeline([\n",
    "    (\"outlier\",train.ExtendedIsoForest(\n",
    "                columns=[\"SibSp\", \"Parch\", \"Fare\"],\n",
    "                n_trees=100,\n",
    "                sample_size=256,\n",
    "                ExtensionLevel=1,\n",
    "                anomalies_ratio=0.03,\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "train.data = train_pipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "eif_outliers = np.array(sorted(train_pipe.named_steps[\"outlier\"].outliers_))\n",
    "print(eif_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "# reduce(np.intersect1d, (iqr_outliers, if_outliers, eif_outliers))\n",
    "outliers = reduce(np.intersect1d, (if_outliers, eif_outliers))\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review outlier identification summary\n",
    "outlier_summary = train.outlier_summary(iqr_outliers=iqr_outliers,\n",
    "                             if_outliers=if_outliers,\n",
    "                             eif_outliers=eif_outliers\n",
    "                            )\n",
    "outlier_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'remove'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outlers from predictors and response\n",
    "outliers = np.array([27, 88, 258, 311, 341, 438, 679, 737, 742])\n",
    "train.data = train.data.drop(outliers)\n",
    "train.target = train.target.drop(index=outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Missing-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "train.eda_missing_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingno matrix\n",
    "msno.matrix(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingno bar\n",
    "msno.bar(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingno heatmap\n",
    "msno.heatmap(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingno dendrogram\n",
    "msno.dendrogram(train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "valid.eda_missing_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingno matrix\n",
    "msno.matrix(valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingno bar\n",
    "msno.bar(valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingno heatmap\n",
    "msno.heatmap(valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingno dendrogram\n",
    "msno.dendrogram(valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Training vs. validation missingness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "train.missing_col_compare(train=train.data, validation=valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Impute'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply imputations to missing data in training dataset\n",
    "train_pipe = pipeline.Pipeline([\n",
    "        (\"imputeMedian\",train.ContextImputer(null_col=\"Age\", context_col=\"Parch\", strategy=\"median\")),\n",
    "        (\"imputeMode\", train.ModeImputer(columns=[\"Embarked\"])),\n",
    "    ])\n",
    "train.data = train_pipe.transform(train.data)\n",
    "train.eda_missing_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impute validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply imputations to missing data in validation dataset\n",
    "validPipe = pipeline.Pipeline([\n",
    "        (\"imputeMedian\",valid.ContextImputer(null_col=\"Age\",context_col=\"Parch\",train=False,trainValue=train_pipe.named_steps[\"imputeMedian\"].trainValue_,)),\n",
    "        (\"imputeMedian2\",valid.numberalImputer(columns=[\"Fare\", \"Age\"], strategy=\"median\")),\n",
    "    ])\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "valid.eda_missing_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Engineer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Engineer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train.data[\"Name\"]]\n",
    "train.data[\"Title\"] = pd.Series(title)\n",
    "train.data[\"Title\"] = train.data[\"Title\"].replace(\n",
    "    [\n",
    "        \"Lady\",\n",
    "        \"the Countess\",\n",
    "        \"Countess\",\n",
    "        \"Capt\",\n",
    "        \"Col\",\n",
    "        \"Don\",\n",
    "        \"Dr\",\n",
    "        \"Major\",\n",
    "        \"Rev\",\n",
    "        \"Sir\",\n",
    "        \"Jonkheer\",\n",
    "        \"Dona\",\n",
    "    ],\n",
    "    \"Rare\",\n",
    ")\n",
    "train.data[\"Title\"] = train.data[\"Title\"].map(\n",
    "    {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    ")\n",
    "\n",
    "# distill cabin feature\n",
    "train.data[\"CabinQuarter\"] = pd.Series(\n",
    "    [i[0] if not pd.isnull(i) else \"X\" for i in train.data[\"Cabin\"]]\n",
    ")\n",
    "\n",
    "# family size features and binning\n",
    "train.data[\"FamilySize\"] = train.data[\"SibSp\"] + train.data[\"Parch\"] + 1\n",
    "\n",
    "customBinDict = {\"Age\": [16, 32, 48, 64], \"FamilySize\": [1, 2, 4]}\n",
    "\n",
    "train_pipe = pipeline.Pipeline([\n",
    "        (\"customBin\", train.CustomBinner(customBinDict=customBinDict)),\n",
    "        (\"percentileBin\",train.PercentileBinner(columns=[\"Age\", \"Fare\"], percs=[25, 50, 75])),\n",
    "    ])\n",
    "train.data = train_pipe.transform(train.data)\n",
    "\n",
    "# drop features\n",
    "train.data, train.feature_type = train.featureDropper(\n",
    "    columns=[\"Name\", \"Cabin\"], data=train.data, feature_type=train.feature_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print new columns\n",
    "for col in train.data.columns:\n",
    "    if (\n",
    "        col not in train.feature_type[\"object\"]\n",
    "        and col not in train.feature_type[\"number\"]\n",
    "    ):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append new number features\n",
    "for col in [\"FamilySize\"]:\n",
    "    train.feature_type[\"number\"].append(col)\n",
    "\n",
    "# append new object features\n",
    "for col in [\n",
    "    \"AgeCustomBin\",\n",
    "    \"AgePercBin\",\n",
    "    \"FarePercBin\",\n",
    "    \"FamilySize\",\n",
    "    \"FamilySizeCustomBin\",\n",
    "    \"Title\",\n",
    "    \"CabinQuarter\",\n",
    "]:\n",
    "    train.feature_type[\"object\"].append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate additional features\n",
    "for feature in train.feature_type['object']:\n",
    "    train.eda_cat_target_cat_feat(feature=feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Engineer validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(\",\")[1].split(\".\")[0].strip() for i in valid.data[\"Name\"]]\n",
    "valid.data[\"Title\"] = pd.Series(title)\n",
    "valid.data[\"Title\"] = valid.data[\"Title\"].replace(\n",
    "    [\n",
    "        \"Lady\",\n",
    "        \"the Countess\",\n",
    "        \"Countess\",\n",
    "        \"Capt\",\n",
    "        \"Col\",\n",
    "        \"Don\",\n",
    "        \"Dr\",\n",
    "        \"Major\",\n",
    "        \"Rev\",\n",
    "        \"Sir\",\n",
    "        \"Jonkheer\",\n",
    "        \"Dona\",\n",
    "    ],\n",
    "    \"Rare\",\n",
    ")\n",
    "valid.data[\"Title\"] = valid.data[\"Title\"].map(\n",
    "    {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    ")\n",
    "\n",
    "# distill cabin feature\n",
    "valid.data[\"CabinQuarter\"] = pd.Series(\n",
    "    [i[0] if not pd.isnull(i) else \"X\" for i in valid.data[\"Cabin\"]]\n",
    ")\n",
    "\n",
    "# additional features\n",
    "valid.data[\"FamilySize\"] = valid.data[\"SibSp\"] + valid.data[\"Parch\"] + 1\n",
    "\n",
    "validPipe = pipeline.Pipeline([\n",
    "        (\"customBin\", valid.CustomBinner(customBinDict=customBinDict)),\n",
    "        (\"percentileBin\",valid.PercentileBinner(train=False, trainValue=train_pipe.named_steps[\"percentileBin\"].trainValue_)),\n",
    "    ])\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "\n",
    "# drop features\n",
    "valid.data, valid.feature_type = valid.featureDropper(\n",
    "    columns=[\"Name\", \"Cabin\"], data=valid.data, feature_type=valid.feature_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print new columns\n",
    "for col in valid.data.columns:\n",
    "    if (\n",
    "        col not in valid.feature_type[\"object\"]\n",
    "        and col not in valid.feature_type[\"number\"]\n",
    "    ):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append new number features\n",
    "for col in [\"FamilySize\"]:\n",
    "    valid.feature_type[\"number\"].append(col)\n",
    "\n",
    "# append new object features\n",
    "for col in [\n",
    "    \"AgeCustomBin\",\n",
    "    \"AgePercBin\",\n",
    "    \"FarePercBin\",\n",
    "    \"FamilySize\",\n",
    "    \"FamilySizeCustomBin\",\n",
    "    \"Title\",\n",
    "    \"CabinQuarter\",\n",
    "]:\n",
    "    valid.feature_type[\"object\"].append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Encoding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training feature evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts of unique values in training data string columns\n",
    "train.data[train.feature_type[\"object\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print unique values in each object columns\n",
    "for col in train.data[train.feature_type[\"object\"]]:\n",
    "    try:\n",
    "        print(col, np.unique(train.data[col]))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation feature evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts of unique values in validation data string columns\n",
    "valid.data[valid.feature_type[\"object\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print unique values in each object columns\n",
    "for col in valid.data[valid.feature_type[\"object\"]]:\n",
    "    if col not in [\"Name\", \"Cabin\"]:\n",
    "        print(col, np.unique(valid.data[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training vs. validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify values that are present in the training data but not the validation data, and vice versa\n",
    "for col in train.feature_type[\"object\"]:\n",
    "    if col not in [\"Name\", \"Cabin\"]:\n",
    "        train_values = train.data[col].unique()\n",
    "        valid_values = valid.data[col].unique()\n",
    "\n",
    "        train_diff = set(train_values) - set(valid_values)\n",
    "        valid_diff = set(valid_values) - set(train_values)\n",
    "\n",
    "        if len(train_diff) > 0 or len(valid_diff) > 0:\n",
    "            print(\"\\n\\n*** \" + col)\n",
    "            print(\"Value present in training data, not in validation data\")\n",
    "            print(train_diff)\n",
    "            print(\"Value present in validation data, not in training data\")\n",
    "            print(valid_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Encode'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encode training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal column encoding instructions\n",
    "ordobject_columns = {\"Pclass\": {1: 1, 2: 2, 3: 3}}\n",
    "\n",
    "# nominal columns\n",
    "nomobject_columns = [\"Embarked\", \"Sex\", \"CabinQuarter\", \"Title\"]\n",
    "\n",
    "# apply encodings to training data\n",
    "train_pipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"encodeOrdinal\", train.CustomOrdinalEncoder(encodings=ordobject_columns)),\n",
    "        (\"dummyNominal\", train.Dummies(columns=nomobject_columns, dropFirst=True)),\n",
    "    ]\n",
    ")\n",
    "train.data = train_pipe.transform(train.data)\n",
    "train.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encode validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply encodings to validation data\n",
    "validPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"encodeOrdinal\", valid.CustomOrdinalEncoder(encodings=ordobject_columns)),\n",
    "        (\"dummyNominal\", valid.Dummies(columns=nomobject_columns, dropFirst=False)),\n",
    "        (\"sync\", valid.FeatureSync(trainCols=train.data.columns)),\n",
    "    ]\n",
    ")\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "valid.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Transformation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate skew of number features - training data\n",
    "train.skew_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate skew of number features - validation data\n",
    "valid.skew_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Transform'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew correct in training dataset, which also learns te best lambda value for each columns\n",
    "train_pipe = pipeline.Pipeline([\n",
    "        (\"skew\",train.SkewTransform(columns=train.feature_type[\"number\"], skewMin=0.75, pctZeroMax=1.0, verbose = True))\n",
    "    ])\n",
    "train.data = train_pipe.transform(train.data)\n",
    "train.skew_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew correction in validation dataset using lambdas learned on training data\n",
    "validPipe = pipeline.Pipeline([\n",
    "        (\"skew\",valid.SkewTransform(train=False, trainValue=train_pipe.named_steps[\"skew\"].trainValue_))\n",
    "    ])\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "valid.skew_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-final'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "train_pipe = pipeline.Pipeline([\n",
    "    (\"outlier\",train.OutlierIQR(\n",
    "                outlier_count=5,\n",
    "                iqr_step=1.5,\n",
    "                features=train.data.columns,\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "train.data = train_pipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqr_outliers = np.array(sorted(train_pipe.named_steps[\"outlier\"].outliers_))\n",
    "print(iqr_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = ensemble.IsolationForest(\n",
    "    behaviour=\"new\", max_samples=train.data.shape[0], random_state=0, contamination=0.01\n",
    ")\n",
    "clf.fit(train.data[train.data.columns])\n",
    "preds = clf.predict(train.data[train.data.columns])\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)\n",
    "if_outliers = np.array(train.data[mask].index)\n",
    "print(if_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers using extended isolation forest\n",
    "train_pipe = pipeline.Pipeline([\n",
    "    (\"outlier\",train.ExtendedIsoForest(\n",
    "                columns=train.data.columns,\n",
    "                n_trees=100,\n",
    "                sample_size=256,\n",
    "                ExtensionLevel=1,\n",
    "                anomalies_ratio=0.03,\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "train.data = train_pipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "eif_outliers = np.array(sorted(train_pipe.named_steps[\"outlier\"].outliers_))\n",
    "print(eif_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "outliers = reduce(np.intersect1d, (iqr_outliers, if_outliers, eif_outliers))\n",
    "# outliers = reduce(np.intersect1d, (if_outliers, eif_outliers))\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review outlier identification summary\n",
    "outlier_summary = train.outlier_summary(iqr_outliers=iqr_outliers,\n",
    "                             if_outliers=if_outliers,\n",
    "                             eif_outliers=eif_outliers\n",
    "                            )\n",
    "outlier_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'remove1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove outlers from predictors and response\n",
    "# outliers = np.array([59,121])\n",
    "# train.data = train.data.drop(outliers)\n",
    "# train.target = train.target.drop(index=outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate feature importance summary\n",
    "estimators = [\n",
    "    \"lightgbm.LGBMClassifier\",\n",
    "    \"ensemble.RandomForestClassifier\",\n",
    "    \"ensemble.GradientBoostingClassifier\",\n",
    "    \"ensemble.ExtraTreesClassifier\",\n",
    "    \"ensemble.AdaBoostClassifier\",\n",
    "    \"xgboost.XGBClassifier\",\n",
    "]\n",
    "\n",
    "featureSummary = train.feature_selector_suite(estimators=estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cross-validation performance\n",
    "estimators = [\n",
    "    \"svm.SVC\",\n",
    "    \"lightgbm.LGBMClassifier\",\n",
    "    \"linear_model.LogisticRegression\",\n",
    "    \"xgboost.XGBClassifier\",\n",
    "    \"ensemble.RandomForestClassifier\",\n",
    "    \"ensemble.GradientBoostingClassifier\",\n",
    "    \"ensemble.AdaBoostClassifier\",\n",
    "    \"ensemble.ExtraTreesClassifier\",\n",
    "    \"neighbors.KNeighborsClassifier\",\n",
    "]\n",
    "\n",
    "cv_summary = train.feature_selector_cross_val(\n",
    "    estimators=estimators,\n",
    "    featureSummary=featureSummary,\n",
    "    metrics=[\"accuracy\",\"f1_macro\",\"roc_auc\"],\n",
    "    n_folds=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize CV performance for diminishing feature set\n",
    "train.feature_selector_results_plot(\n",
    "    cv_summary=cv_summary,\n",
    "    featureSummary=featureSummary,\n",
    "    metric=\"accuracy\",\n",
    "    show_features=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.features_used_summary(\n",
    "    cv_summary=cv_summary, metric=\"accuracy\", featureSummary=featureSummary\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Rationality'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# percent difference summary\n",
    "df_diff = abs(\n",
    "    (\n",
    "        ((valid.data.describe() + 1) - (train.data.describe() + 1))\n",
    "        / (train.data.describe() + 1)\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "df_diff = df_diff[df_diff.columns].replace({0: np.nan})\n",
    "df_diff[df_diff < 0] = np.nan\n",
    "df_diff = df_diff.fillna(\"\")\n",
    "display(df_diff)\n",
    "display(train.data[df_diff.columns].describe())\n",
    "display(valid.data[df_diff.columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value override"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Value override'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change clearly erroneous value to what it probably was\n",
    "# exploreValid.data['GarageYrBlt'].replace({2207 : 2007}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## number feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'number-feature-EDA3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot(chart_prop=15)\n",
    "ax = p.make_canvas()\n",
    "p.pretty_corr_heatmap_target(df=train.data, target=train.target, thresh=0.2, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-preparation-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     15
    ]
   },
   "outputs": [],
   "source": [
    "# import training data\n",
    "df_train = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-titanic//train.csv\")\n",
    "train = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"Survived\",\n",
    "    remove_features=[\"PassengerId\", \"Ticket\"],\n",
    "    force_to_object=[\"Pclass\", \"SibSp\", \"Parch\"],\n",
    "    target_type=\"object\",\n",
    ")\n",
    "\n",
    "### feature engineering\n",
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train.data[\"Name\"]]\n",
    "train.data[\"Title\"] = pd.Series(title)\n",
    "train.data[\"Title\"] = train.data[\"Title\"].replace(\n",
    "    [\n",
    "        \"Lady\",\n",
    "        \"the Countess\",\n",
    "        \"Countess\",\n",
    "        \"Capt\",\n",
    "        \"Col\",\n",
    "        \"Don\",\n",
    "        \"Dr\",\n",
    "        \"Major\",\n",
    "        \"Rev\",\n",
    "        \"Sir\",\n",
    "        \"Jonkheer\",\n",
    "        \"Dona\",\n",
    "    ],\n",
    "    \"Rare\",\n",
    ")\n",
    "train.data[\"Title\"] = train.data[\"Title\"].map(\n",
    "    {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    ")\n",
    "\n",
    "# distill cabin feature\n",
    "train.data[\"CabinQuarter\"] = pd.Series(\n",
    "    [i[0] if not pd.isnull(i) else \"X\" for i in train.data[\"Cabin\"]]\n",
    ")\n",
    "\n",
    "# family size features\n",
    "train.data[\"FamilySize\"] = train.data[\"SibSp\"] + train.data[\"Parch\"] + 1\n",
    "\n",
    "# custom bin specifications\n",
    "customBinDict = {\"Age\": [16, 32, 48, 64], \"FamilySize\": [1, 2, 4]}\n",
    "# object column specifications\n",
    "ordobject_columns = {\"Pclass\": {1: 1, 2: 2, 3: 3}}\n",
    "nomobject_columns = [\"Embarked\", \"Sex\", \"CabinQuarter\", \"Title\"]\n",
    "\n",
    "# remove outliers\n",
    "outliers = np.array([27, 88, 258, 311, 341, 438, 679, 737, 742])\n",
    "train.data = train.data.drop(outliers)\n",
    "train.target = train.target.drop(index=outliers)\n",
    "\n",
    "### pipeline\n",
    "train_pipe = pipeline.Pipeline([\n",
    "        ('imputeMedian', train.ContextImputer(null_col = 'Age', context_col = 'Parch', strategy = 'median')),\n",
    "        ('imputeMode', train.ModeImputer(columns = ['Embarked'])),\n",
    "        ('customBin', train.CustomBinner(customBinDict = customBinDict)),\n",
    "        ('percentileBin', train.PercentileBinner(columns = ['Age','Fare'], percs = [10, 25, 50, 75, 90])),\n",
    "        ('encodeOrdinal', train.CustomOrdinalEncoder(encodings = ordobject_columns)),\n",
    "        ('dummyNominal', train.Dummies(columns = nomobject_columns, dropFirst = True)),\n",
    "        ('skew', train.SkewTransform(columns = train.feature_type['number'], skewMin = 0.75, pctZeroMax = 1.0)),\n",
    "    ])\n",
    "train.data = train_pipe.transform(train.data)\n",
    "\n",
    "# drop features\n",
    "train.data, train.feature_type = train.featureDropper(\n",
    "    columns=[\"Name\", \"Cabin\"], data=train.data, feature_type=train.feature_type\n",
    ")\n",
    "print('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "### import valid data\n",
    "df_valid = pd.read_csv(\"s3://tdp-ml-datasets/kaggle-titanic//test.csv\")\n",
    "valid = mlm.Machine(\n",
    "    data=df_valid,\n",
    "    remove_features=[\"PassengerId\", \"Ticket\"],\n",
    "    force_to_object=[\"Pclass\", \"SibSp\", \"Parch\"],\n",
    ")\n",
    "\n",
    "### feature engineering\n",
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(\",\")[1].split(\".\")[0].strip() for i in valid.data[\"Name\"]]\n",
    "valid.data[\"Title\"] = pd.Series(title)\n",
    "valid.data[\"Title\"] = valid.data[\"Title\"].replace(\n",
    "    [\n",
    "        \"Lady\",\n",
    "        \"the Countess\",\n",
    "        \"Countess\",\n",
    "        \"Capt\",\n",
    "        \"Col\",\n",
    "        \"Don\",\n",
    "        \"Dr\",\n",
    "        \"Major\",\n",
    "        \"Rev\",\n",
    "        \"Sir\",\n",
    "        \"Jonkheer\",\n",
    "        \"Dona\",\n",
    "    ],\n",
    "    \"Rare\",\n",
    ")\n",
    "valid.data[\"Title\"] = valid.data[\"Title\"].map(\n",
    "    {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    ")\n",
    "\n",
    "# distill cabin feature\n",
    "valid.data[\"CabinQuarter\"] = pd.Series(\n",
    "    [i[0] if not pd.isnull(i) else \"X\" for i in valid.data[\"Cabin\"]]\n",
    ")\n",
    "\n",
    "# additional features\n",
    "valid.data[\"FamilySize\"] = valid.data[\"SibSp\"] + valid.data[\"Parch\"] + 1\n",
    "\n",
    "### pipeline\n",
    "validPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"imputeMedian\",valid.ContextImputer(null_col=\"Age\",context_col=\"Parch\",train=False,trainValue=train_pipe.named_steps[\"imputeMedian\"].trainValue_)),\n",
    "        (\"imputeMedian2\",valid.numberalImputer(columns=[\"Fare\", \"Age\"], strategy=\"median\",train=False,trainValue=train.data)),\n",
    "        (\"customBin\", valid.CustomBinner(customBinDict=customBinDict)),\n",
    "        (\"percentileBin\",valid.PercentileBinner(train=False, trainValue=train_pipe.named_steps[\"percentileBin\"].trainValue_)),\n",
    "        (\"encodeOrdinal\", valid.CustomOrdinalEncoder(encodings=ordobject_columns)),\n",
    "        (\"dummyNominal\", valid.Dummies(columns=nomobject_columns, dropFirst=False)),\n",
    "        (\"sync\", valid.FeatureSync(trainCols=train.data.columns)),\n",
    "        (\"skew\",valid.SkewTransform(train=False, trainValue=train_pipe.named_steps[\"skew\"].trainValue_)),\n",
    "    ]\n",
    ")\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "print('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Bayesian-hyper-parameter-optimization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# parameter space\n",
    "all_space = {\n",
    "    \"lightgbm.LGBMClassifier\": {\n",
    "        \"class_weight\": hp.choice(\"class_weight\", [None]),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"dart\"]),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.15, 0.25),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(4, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.quniform(\"min_child_samples\", 50, 150, 5),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.quniform(\"num_leaves\", 30, 70, 1),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.75, 1.25),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.quniform(\"subsample_for_bin\", 100000, 350000, 20000),\n",
    "    },\n",
    "    \"linear_model.LogisticRegression\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.04, 0.1),\n",
    "        \"penalty\": hp.choice(\"penalty\", [\"l1\"]),\n",
    "    },\n",
    "    \"xgboost.XGBClassifier\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"gamma\": hp.quniform(\"gamma\", 0.0, 10, 0.05),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.2, 0.01),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_child_weight\": hp.quniform(\"min_child_weight\", 2.5, 7.5, 1),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.4, 0.7),\n",
    "    },\n",
    "    \"ensemble.RandomForestClassifier\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 10, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 8000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(15, 25, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 11, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.09, 0.01),\n",
    "        \"loss\": hp.choice(\"loss\", [\"deviance\", \"exponential\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.AdaBoostClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.1, 0.25, 0.01),\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"SAMME\"]),\n",
    "    },\n",
    "    \"ensemble.ExtraTreesClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(4, 30, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\"]),\n",
    "        \"criterion\": hp.choice(\"criterion\", [\"entropy\"]),\n",
    "    },\n",
    "    \"svm.SVC\": {\n",
    "        \"C\": hp.uniform(\"C\", 4, 15),\n",
    "        \"decision_function_shape\": hp.choice(\"decision_function_shape\", [\"ovr\"]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.00000001, 1.5),\n",
    "    },\n",
    "    \"neighbors.KNeighborsClassifier\": {\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"ball_tree\", \"brute\"]),\n",
    "        \"n_neighbors\": hp.choice(\"n_neighbors\", np.arange(1, 15, dtype=int)),\n",
    "        \"weights\": hp.choice(\"weights\", [\"uniform\"]),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "analysis = \"titanic\"\n",
    "train.exec_bayes_optim_search(\n",
    "    all_space=all_space,\n",
    "    results_dir=\"{}_hyperopt_{}.csv\".format(rundate, analysis),\n",
    "    X=train.data,\n",
    "    y=train.target,\n",
    "    scoring=\"accuracy\",\n",
    "    n_folds=2,\n",
    "    n_jobs=4,\n",
    "    iters=8,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loss by iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Model-loss-by-iteration'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "analysis = \"titanic\"\n",
    "rundate = \"20190807\"\n",
    "bayes_optim_summary = pd.read_csv(\n",
    "    \"{}_hyperopt_{}.csv\".format(rundate, analysis), na_values=\"nan\"\n",
    ")\n",
    "bayes_optim_summary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(bayes_optim_summary[\"estimator\"]):\n",
    "    train.model_loss_plot(bayes_optim_summary=bayes_optim_summary, estimator=estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter selection by iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Parameter-selection-by-iteration'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(bayes_optim_summary[\"estimator\"]):\n",
    "    train.modelParamPlot(\n",
    "        bayes_optim_summary=bayes_optim_summary,\n",
    "        estimator=estimator,\n",
    "        all_space=all_space,\n",
    "        n_iter=100,\n",
    "        chart_prop=15,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_space = {\n",
    "                'param': hp.uniform('param', np.log(0.4), np.log(0.6))\n",
    "#     \"\": 0.000001 + hp.uniform(\"gamma\", 0.000001, 10)\n",
    "    #             'param2': hp.loguniform('param2', np.log(0.001), np.log(0.01))\n",
    "}\n",
    "\n",
    "train.sample_plot(sample_space, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair-wise comparison\n",
    "p = PrettierPlot(chart_prop=12)\n",
    "p.pretty_pair_plot_custom(\n",
    "    df=df,\n",
    "    columns=[\"colsample_bytree\", \"learning_rate\", \"iteration\",\"iterLoss\"],\n",
    "    gradient_col=\"iteration\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance evaluation - standard models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Model-performance-evaluation-standard-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = train.top_bayes_optim_models(bayes_optim_summary=bayes_optim_summary, num_models=1)\n",
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification panel, single model\n",
    "estimator = \"svm.SVC\"\n",
    "model_iter = 135\n",
    "# estimator = 'ensemble.GradientBoostingClassifier'; model_iter = 590\n",
    "# estimator = 'xgboost.XGBClassifier'; model_iter = 380\n",
    "\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "train.classification_panel(\n",
    "    model=model,\n",
    "    X_train=train.data,\n",
    "    y_train=train.target,\n",
    "    cm_labels=[\"Dies\", \"Survives\"],\n",
    "    n_folds=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classification reports for training data\n",
    "for estimator, model_iters in top_models.items():\n",
    "    for model_iter in model_iters:\n",
    "        model = train.BayesOptimModelBuilder(\n",
    "            bayes_optim_summary=bayes_optim_summary,\n",
    "            estimator=estimator,\n",
    "            model_iter=model_iter,\n",
    "        )\n",
    "        train.classification_panel(\n",
    "            model=model, X_train=train.data, y_train=train.target, cm_labels=['Dies', 'Survives'], n_folds=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model explanability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# estimator = \"ensemble.ExtraTreesClassifier\"; model_iter = 145\n",
    "# estimator = \"svm.SVC\"; model_iter = 135\n",
    "estimator = \"ensemble.GradientBoostingClassifier\"; model_iter = 490\n",
    "\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "model.fit(train.data.values, train.target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Permutation-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation importance - how much does performance decrease when shuffling a certain feature?\n",
    "perm = PermutationImportance(model.model, random_state=1).fit(train.data, train.target)\n",
    "eli5.show_weights(perm, feature_names=train.data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'SHAP-values'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Force plots - single observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in train.data.index[:2]:\n",
    "    train.single_shap_viz_tree(obsIx=i, model=model, data=train.data, target=train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Force plots - multiple observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = train.multi_shap_viz_tree(obs_ixs=train.data.index, model=model, data=train.data)\n",
    "visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obs_data, _, obs_shap_values = train.multi_shap_value_tree(\n",
    "    obs_ixs=train.data.index, model=model, data=train.data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "grid_features = [\"Pclass\", \"Age\", \"Fare\", \"SibSp\",\"Parch\"]\n",
    "\n",
    "train.shap_dependence_grid(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    grid_features=grid_features,\n",
    "    all_features=train.data.columns,\n",
    "    dot_size=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "\n",
    "train.shap_dependence_plot(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    scatter_feature=\"Age\",\n",
    "    color_feature=\"Parch\",\n",
    "    feature_names=train.data.columns,\n",
    "    dot_size=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "feature_names = train.data.columns.tolist()\n",
    "top_shap = np.argsort(-np.sum(np.abs(obs_shap_values), 0))\n",
    "\n",
    "for top_ix in top_shap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.make_canvas()\n",
    "    \n",
    "    train.shap_dependence_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        scatter_feature=feature_names[top_ix],\n",
    "        color_feature=\"Age\",\n",
    "        feature_names=feature_names,\n",
    "        dot_size=50,\n",
    "        alpha=0.5,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "train.shap_summary_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        feature_names=train.data.columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Force plots - single observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in valid.data.index[:2]:\n",
    "    valid.single_shap_viz_tree(obsIx=i, model=model, data=valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Force plots - multiple observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = valid.multi_shap_viz_tree(obs_ixs=valid.data.index, model=model, data=valid.data)\n",
    "visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obs_data, _, obs_shap_values = valid.multi_shap_value_tree(\n",
    "    obs_ixs=valid.data.index, model=model, data=valid.data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "grid_features = [\"Pclass\", \"Age\", \"Fare\", \"SibSp\",\"Parch\"]\n",
    "\n",
    "valid.shap_dependence_grid(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    grid_features=grid_features,\n",
    "    all_features=valid.data.columns,\n",
    "    dot_size=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "\n",
    "valid.shap_dependence_plot(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    scatter_feature=\"Age\",\n",
    "    color_feature=\"Parch\",\n",
    "    feature_names=valid.data.columns,\n",
    "    dot_size=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "feature_names = valid.data.columns.tolist()\n",
    "top_shap = np.argsort(-np.sum(np.abs(obs_shap_values), 0))\n",
    "\n",
    "for top_ix in top_shap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.make_canvas()\n",
    "    \n",
    "    valid.shap_dependence_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        scatter_feature=feature_names[top_ix],\n",
    "        color_feature=\"Age\",\n",
    "        feature_names=feature_names,\n",
    "        dot_size=50,\n",
    "        alpha=0.5,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "valid.shap_summary_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        feature_names=valid.data.columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission - standard models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Submission-standard-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard model fit and predict\n",
    "# select estimator and iteration\n",
    "# estimator = \"lightgbm.LGBMClassifier\"; model_iter = 668  #142 survived, 0.77511\n",
    "# estimator = \"xgboost.XGBClassifier\"; model_iter = 380  #151 survived, 0.7655\n",
    "# estimator = \"ensemble.RandomForestClassifier\"; model_iter = 405  #148 survived, 0.79425\n",
    "# estimator = \"ensemble.GradientBoostingClassifier\"; model_iter = 590  #142 survived, 0.7655\n",
    "estimator = \"svm.SVC\"; model_iter = 135  #154 survived, 0.755\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "model.fit(train.data.values, train.target.values)\n",
    "\n",
    "# fit model and make predictions\n",
    "y_pred = model.predict(valid.data.values)\n",
    "print(sum(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "submit = pd.DataFrame({\"PassengerId\": df_valid.PassengerId, \"Survived\": y_pred})\n",
    "submit.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Stacking'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Primary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Primary-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get out-of-fold predictions\n",
    "oof_train, oof_valid, columns = train.model_stacker(\n",
    "    models=top_models,\n",
    "    bayes_optim_summary=bayes_optim_summary,\n",
    "    X_train=train.data.values,\n",
    "    y_train=train.target.values,\n",
    "    X_valid=valid.data.values,\n",
    "    n_folds=10,\n",
    "    n_jobs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view correlations of predictions\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "p.pretty_corr_heatmap(\n",
    "    df=pd.DataFrame(oof_train, columns=columns), annot=True, ax=ax, vmin=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Meta-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parameter space\n",
    "all_space = {\n",
    "    \"lightgbm.LGBMClassifier\": {\n",
    "        \"class_weight\": hp.choice(\"class_weight\", [None]),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"dart\"]),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.15, 0.25),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(4, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.quniform(\"min_child_samples\", 50, 150, 5),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.quniform(\"num_leaves\", 30, 70, 1),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.75, 1.25),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.quniform(\"subsample_for_bin\", 100000, 350000, 20000),\n",
    "    },\n",
    "    \"xgboost.XGBClassifier\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"gamma\": hp.quniform(\"gamma\", 0.0, 10, 0.05),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.2, 0.01),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_child_weight\": hp.quniform(\"min_child_weight\", 2.5, 7.5, 1),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.4, 0.7),\n",
    "    },\n",
    "    \"ensemble.RandomForestClassifier\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 10, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 8000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(15, 25, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 11, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.09, 0.01),\n",
    "        \"loss\": hp.choice(\"loss\", [\"deviance\", \"exponential\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"svm.SVC\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.00000001, 15),\n",
    "        \"decision_function_shape\": hp.choice(\"decision_function_shape\", [\"ovr\", \"ovo\"]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.00000001, 1.5),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "train.exec_bayes_optim_search(\n",
    "    all_space=all_space,\n",
    "    results_dir=\"{}_hyperopt_meta_{}.csv\".format(rundate, analysis),\n",
    "    X=oof_train,\n",
    "    y=train.target,\n",
    "    scoring=\"accuracy\",\n",
    "    n_folds=8,\n",
    "    n_jobs=10,\n",
    "    iters=1000,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "analysis = \"Titanic\"\n",
    "rundate = \"20190807\"\n",
    "bayes_optim_summary_meta = pd.read_csv(\"{}_hyperopt_meta_{}.csv\".format(rundate, analysis))\n",
    "bayes_optim_summary_meta[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(bayes_optim_summary_meta[\"estimator\"]):\n",
    "    train.model_loss_plot(bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(bayes_optim_summary_meta[\"estimator\"]):\n",
    "    train.modelParamPlot(\n",
    "        bayes_optim_summary=bayes_optim_summary_meta,\n",
    "        estimator=estimator,\n",
    "        all_space=all_space,\n",
    "        n_iter=100,\n",
    "        chart_prop=15,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Model performance evaluation - stacked models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Model-performance-evaluation-stacked-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_models = train.top_bayes_optim_models(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, num_models=1\n",
    ")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# classification panel, single model\n",
    "estimator = \"svm.SVC\"; model_iter = 135\n",
    "# estimator = 'ensemble.GradientBoostingClassifier'; model_iter = 590\n",
    "# estimator = 'xgboost.XGBClassifier'; model_iter = 380\n",
    "\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "train.classification_panel(\n",
    "    model=model, X_train=oof_train, y_train=train.target, labels=[0, 1], n_folds=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create classification reports for training data\n",
    "for estimator, model_iters in top_models.items():\n",
    "    for model_iter in model_iters:\n",
    "        model = train.BayesOptimModelBuilder(\n",
    "            bayes_optim_summary=bayes_optim_summary_meta,\n",
    "            estimator=estimator,\n",
    "            model_iter=model_iter,\n",
    "        )\n",
    "        train.classification_panel(\n",
    "            model=model, X_train=oof_train, y_train=train.target, labels=[0, 1], n_folds=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Submission - stacked models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Submission-stacked-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "# estimator = \"lightgbm.LGBMClassifier\"; model_iter = 876 #0.75119\n",
    "# estimator = \"xgboost.XGBClassifier\"; model_iter = 821, #0.779\n",
    "# estimator = \"ensemble.RandomForestClassifier\"; model_iter = 82 \n",
    "# estimator = \"ensemble.GradientBoostingClassifier\"; model_iter = 673 #0.77511\n",
    "estimator = \"svm.SVC\"; model_iter = 538 # 0.77511\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = train.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "model.fit(oof_train, train.target.values)\n",
    "y_pred = model.predict(oof_valid)\n",
    "print(sum(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "submit = pd.DataFrame({\"PassengerId\": df_valid.PassengerId, \"Survived\": y_pred})\n",
    "submit.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
